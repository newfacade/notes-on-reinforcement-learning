{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1b862a-401f-4ffa-a0da-1fbf1489ad42",
   "metadata": {},
   "source": [
    "# Basic Concepts\n",
    "\n",
    "In supervised learning, we saw algorithms that tried to make their outputs mimic the labels $y$ given in the training set. In that setting, the labels gave\n",
    "an unambiguous \"right answer\" for each of the inputs $x$. In contrast, for many sequential decision making and control problems, it is very difficult to provide this type of explicit supervision to a learning algorithm. For example, if we have just built a four-legged robot and are trying to program it to walk, then initially we have no idea what the \"correct\" actions to take are to make it walk.\n",
    "\n",
    "In the reinforcement learning framework, we will instead provide our algorithms only a reward function, which indicates to the learning agent when it is doing well, and when it is doing poorly. In the four-legged walking example, the reward function might give the robot positive rewards for moving forwards, and negative rewards for either moving backwards or falling over. It will then be the learning algorithm's job to â€€gure out how to choose actions over time so as to obtain large rewards.\n",
    "\n",
    "<center><img src=\"images/reinforce.png\" width=\"500px\"></center>\n",
    "\n",
    "That is:\n",
    "\n",
    "$$s_{t} \\xrightarrow[]{Policy} a_{t} \\xrightarrow[]{Enviroment} r_{t+1},s_{t+1}$$\n",
    "\n",
    "Policy: $\\pi(a|s)$\n",
    "\n",
    "Enviroment has the Markov property: state transition and reward process $p(r|s, a)$\n",
    "\n",
    "$$p(s_{t+1}|s_{t},a_{t},\\dots,s_{0},a_{0})=p(s_{t+1}|s_{t},a_{t})$$\n",
    "$$p(r_{t+1}|s_{t},a_{t},\\dots,s_{0},a_{0})=p(r_{t+1}|s_{t},a_{t})$$\n",
    "\n",
    "Obtain trajectory $(s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},s_{2},a_{2},\\dots)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea12ec8-e828-431e-bc58-53471665a1ec",
   "metadata": {},
   "source": [
    "## Bellman equation\n",
    "\n",
    "Discounted return:\n",
    "\n",
    "$$G_{t} = R_{t+1} + \\gamma R_{t+2} + \\gamma^{2}R_{t+3} + \\dots$$\n",
    "\n",
    "State value is the expected value of discounted return:\n",
    "\n",
    "$$v_{\\pi}(s) := \\mathbb{E}[G_{t}|S_{t}=s]$$\n",
    "\n",
    "Bellman equation move one-step forward:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &= \\mathbb{E}[G_{t}|S_{t}=s]\\\\\n",
    "&=\\mathbb{E}[R_{t+1} + \\gamma G_{t+1}|S_{t}=s]\\\\\n",
    "&=\\mathbb{E}[R_{t+1}|S_{t}=s] + \\gamma\\mathbb{E}[G_{t+1}|S_{t}=s]\\\\\n",
    "&=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\sum_{r\\in\\mathcal{R}}p(r|s,a)r + \\gamma\\sum_{s'\\in\\mathcal{S}}v_{\\pi}(s')\\sum_{a\\in\\mathcal{A}}p'(s'|s,a)\\pi(a|s)\\\\\n",
    "&=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\left[\\sum_{r\\in\\mathcal{R}}p(r|s,a)r + \\gamma\\sum_{s'\\in\\mathcal{S}}p'(s'|s,a)v_{\\pi}(s')\\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef438e0d-13a2-42c4-b464-37fd1f98a624",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
