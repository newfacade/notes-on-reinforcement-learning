

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Policy Gradient &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '11-policy';</script>
    <link rel="shortcut icon" href="_static/github.ico"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Actor-Critic Methods" href="12-actor-critic.html" />
    <link rel="prev" title="Deep Q-Learning with Pytorch" href="10-dqn-torch.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="01-intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/shuishen-min.jpeg" class="logo__image only-light" alt="My sample book - Home"/>
    <script>document.write(`<img src="_static/shuishen-min.jpeg" class="logo__image only-dark" alt="My sample book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="01-intro.html">
                    What is Reinforcement Learning?
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="02-frame.html">The Reinforcement Learning Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="03-approach.html">Two main approaches for solving RL problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="04-bellman.html">The Bellman Equation</a></li>
<li class="toctree-l1"><a class="reference internal" href="05-bellman-optimal.html">The Bellman Optimality Equation</a></li>
<li class="toctree-l1"><a class="reference internal" href="06-dynamic.html">Dynamic Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="07-monte-carlo-td.html">Monte Carlo vs Temporal Difference Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="08-q-learning.html">Q-learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="09-deep-q-learning.html">Deep Q-Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="10-dqn-torch.html">Deep Q-Learning with Pytorch</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Policy Gradient</a></li>
<li class="toctree-l1"><a class="reference internal" href="12-actor-critic.html">Actor-Critic Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="13-trpo.html">TRPO</a></li>
<li class="toctree-l1"><a class="reference internal" href="14-gae.html">Generalized Advantage Estimation</a></li>
<li class="toctree-l1"><a class="reference internal" href="15-ppo.html">Proximal Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="17-ppo-trl.html">PPO with TRL</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F11-policy.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/11-policy.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Policy Gradient</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-policy-gradient-methods">The policy-gradient methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-advantages-and-disadvantages-of-policy-gradient-methods">The advantages and disadvantages of policy-gradient methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-policy-gradient-theorem">The Policy Gradient Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-reinforce-algorithm-monte-carlo-reinforce">The Reinforce algorithm (Monte Carlo Reinforce)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dont-let-the-past-distract-you">Don’t Let the Past Distract You</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#baselines-in-policy-gradients">Baselines in Policy Gradients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-forms-of-the-policy-gradient">Other Forms of the Policy Gradient</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-example">Pytorch example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-network">Policy network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#env">Env</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-episode">One episode</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-loop">Main loop</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="policy-gradient">
<h1>Policy Gradient<a class="headerlink" href="#policy-gradient" title="Permalink to this heading">#</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Since the beginning of the course, we have only studied value-based methods, where we estimate a value function as an intermediate step towards finding an optimal policy. Finding an optimal value function leads to having an optimal policy:</p>
<div class="math notranslate nohighlight">
\[\pi^{\ast}(s) = \arg\max_{a}Q^{\ast}(s, a)\]</div>
<p>With policy-based methods, we want to optimize the policy directly without having an intermediate step of learning a value function.</p>
</div>
<section id="the-policy-gradient-methods">
<h2>The policy-gradient methods<a class="headerlink" href="#the-policy-gradient-methods" title="Permalink to this heading">#</a></h2>
<p>In policy-based methods, we directly learn to approximate <span class="math notranslate nohighlight">\(\pi^{\ast}\)</span>.</p>
<ul class="simple">
<li><p>The idea is to parameterize the policy. For instance, using a neural network <span class="math notranslate nohighlight">\(\pi_{\theta}\)</span>, this policy will output a probability distribution over actions (stochastic policy).</p></li>
<li><p>Our objective then is to maximize the performance of the parameterized policy using gradient ascent. To do that, we define an objective function <span class="math notranslate nohighlight">\(J(\theta)\)</span>, that is, the expected cumulative reward, and we want to find the value <span class="math notranslate nohighlight">\(\theta\)</span> that maximizes this objective function.</p></li>
</ul>
<p><img alt="" src="_images/policy2.png" /></p>
</section>
<section id="the-advantages-and-disadvantages-of-policy-gradient-methods">
<h2>The advantages and disadvantages of policy-gradient methods<a class="headerlink" href="#the-advantages-and-disadvantages-of-policy-gradient-methods" title="Permalink to this heading">#</a></h2>
<p>There are multiple advantages over value-based methods. Let’s see some of them:</p>
<ul class="simple">
<li><p>Policy-gradient methods can learn a stochastic policy while value functions can’t.</p></li>
<li><p>Policy-gradient methods are more effective in high-dimensional action spaces and continuous actions spaces. The problem with Deep Q-learning is that their predictions assign a score for each possible action, at each time step, given the current state. Instead, with policy-gradient methods, we output a probability distribution over actions.</p></li>
<li><p>Policy-gradient methods have better convergence properties. In value-based methods, we use an aggressive operator to change the value function: we take the maximum over Q-estimates. Consequently, the action probabilities may change dramatically for an arbitrarily small change in the estimated action values if that change results in a different action having the maximal value.</p></li>
</ul>
<p>Naturally, policy-gradient methods also have some disadvantages:</p>
<ul class="simple">
<li><p>Frequently, policy-gradient methods converges to a local maximum instead of a global optimum.</p></li>
<li><p>Policy-gradient goes slower, step by step: it can take longer to train.</p></li>
<li><p>Policy-gradient can have high variance.</p></li>
</ul>
</section>
<section id="the-policy-gradient-theorem">
<h2>The Policy Gradient Theorem<a class="headerlink" href="#the-policy-gradient-theorem" title="Permalink to this heading">#</a></h2>
<p>The objective function outputs the expected cumulative reward:</p>
<div class="math notranslate nohighlight">
\[J(\theta) = \mathbb{E}_{\tau\sim\pi}[R(\tau)]\]</div>
<p>It can be formulated as:</p>
<p><img alt="" src="_images/policy3.png" /></p>
<p>Policy-gradient is an optimization problem: we want to find the values of <span class="math notranslate nohighlight">\(\theta\)</span> that maximize our objective function <span class="math notranslate nohighlight">\(J(\theta)\)</span>,  so we need to use gradient-ascent:</p>
<div class="math notranslate nohighlight">
\[\theta\leftarrow\theta + \alpha * \nabla J(\theta)\]</div>
<p>However, there are two problems with computing the derivative of <span class="math notranslate nohighlight">\(J(\theta)\)</span>:</p>
<ol class="arabic simple">
<li><p>We can’t calculate the true gradient of the objective function since it requires calculating the probability of each possible trajectory, which is computationally super expensive. So we want to calculate a gradient estimation with a sample-based estimate.</p></li>
<li><p>To differentiate this objective function, we need to differentiate the state distribution, this is attached to the environment. The problem is that we can’t differentiate it because we might not know about it.</p></li>
</ol>
<p>Fortunately we’re going to use a solution called the Policy Gradient Theorem that will help us to reformulate the objective function into a differentiable function that does not involve the differentiation of the state distribution.</p>
<p><img alt="" src="_images/policy4.png" /></p>
<div class="proof admonition" id="proof">
<p>Proof. We have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
\nabla J(\theta) &amp;= \nabla_{\theta}\sum_{\tau}P(\tau;\theta)R(\tau)\\
&amp;=\sum_{\tau}\nabla_{\theta}P(\tau;\theta)R(\tau)\\
&amp;=\sum_{\tau}P(\tau;\theta)\frac{\nabla_{\theta}P(\tau;\theta)}{P(\tau;\theta)}R(\tau)\\
&amp;=\sum_{\tau}P(\tau;\theta)\nabla{\log P(\tau;\theta)}R(\tau)\\
&amp;=\sum_{\tau}P(\tau;\theta)\nabla\left[\log \mu(s_{0})\Pi_{t=0}^{H}P(s_{t+1}|s_{t}, a_{t})\pi_{\theta}(a_{t}|s_{t})\right]R(\tau)\\
&amp;=\sum_{\tau}P(\tau;\theta)\sum_{t=0}^{H}\nabla \pi_{\theta}(a_{t}|s_{t})R(\tau)\\
&amp;=\mathbb{E}_{\pi_{\theta}}\left[\nabla_{\theta}\log \pi_{\theta}(a_{t}|s_{t})R(\tau)\right]
\end{aligned}\end{split}\]</div>
</div>
</section>
<section id="the-reinforce-algorithm-monte-carlo-reinforce">
<h2>The Reinforce algorithm (Monte Carlo Reinforce)<a class="headerlink" href="#the-reinforce-algorithm-monte-carlo-reinforce" title="Permalink to this heading">#</a></h2>
<p>In a loop:</p>
<ul class="simple">
<li><p>Use the policy <span class="math notranslate nohighlight">\(\pi_{\theta}\)</span> to collect some episodes</p></li>
<li><p>Use these episodes to estimate the gradient.</p></li>
</ul>
<p><img alt="" src="_images/policy5.png" /></p>
<p>We can interpret this update as follows: <span class="math notranslate nohighlight">\(-\nabla_{\theta}\log\pi_{\theta}(a_{t}|s_{t})\)</span> is the direction of steepest increase of the (log) probability of selecting action at from state <span class="math notranslate nohighlight">\(s_{t}\)</span>. This tells us:</p>
<ul class="simple">
<li><p>If the return <span class="math notranslate nohighlight">\(R(\tau)\)</span> is high, it will push up the probabilities of the (state, action) combinations.</p></li>
<li><p>Otherwise, it will push down the probabilities of the (state, action) combinations.</p></li>
</ul>
</section>
<section id="dont-let-the-past-distract-you">
<h2>Don’t Let the Past Distract You<a class="headerlink" href="#dont-let-the-past-distract-you" title="Permalink to this heading">#</a></h2>
<p>Examine our most recent expression for the policy gradient:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau\sim\theta}\left[\sum_{t=0}^{T}\nabla_{\theta}\log\pi_{\theta}(a_{t}|s_{t})R(\tau)\right]\]</div>
<p>Taking a step with this gradient pushes up the log-probabilities of each action in proportion to <span class="math notranslate nohighlight">\(R(\tau)\)</span>, the sum of all rewards ever obtained. But this doesn’t make much sense.</p>
<p>Agents should really only reinforce actions on the basis of their consequences. Rewards obtained before taking an action have no bearing on how good that action was: only rewards that come after.</p>
<p>It turns out that this intuition shows up in the math, and we can show that the policy gradient can also be expressed by</p>
<div class="math notranslate nohighlight">
\[\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau\sim\theta}\left[\sum_{t=0}^{T}\nabla_{\theta}\log\pi_{\theta}(a_{t}|s_{t})\sum_{t'=t}^{T}R(s_{t'},a_{t'},s_{t'+1})\right]\]</div>
<p>In this form, actions are only reinforced based on rewards obtained after they are taken. We’ll call this form the “reward-to-go policy gradient”, it is a lower-variance estimate than the simplest policy gradient.</p>
<p>The proof of this claim depends on the <strong>EGLP lemma</strong>: Suppose that <span class="math notranslate nohighlight">\(P_{\theta}\)</span> is a parameterized probability distribution over a random variable, <span class="math notranslate nohighlight">\(x\)</span>. Then:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_{x\sim P_{\theta}}\left[\nabla_{\theta}\log P_{\theta}(x)\right] = 0\]</div>
<div class="proof admonition" id="proof">
<p>Proof. Recall that all probability distributions are normalized:</p>
<div class="math notranslate nohighlight">
\[\int_{x}P_{\theta}(x) = 1\]</div>
<p>Take the gradient of both sides of the normalization condition:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\theta}\int_{x}P_{\theta}(x) = \nabla_{\theta}1 = 0\]</div>
<p>Use the log derivative trick to get:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
0 &amp;= \nabla_{\theta}\int_{x}P_{\theta}(x) \\
 &amp;= \int_{x}\nabla_{\theta}P_{\theta}(x) \\
 &amp;= \int_{x}P_{\theta}(x)\nabla_{\theta}\log P_{\theta}(x)
\end{aligned}\end{split}\]</div>
</div>
</section>
<section id="baselines-in-policy-gradients">
<h2>Baselines in Policy Gradients<a class="headerlink" href="#baselines-in-policy-gradients" title="Permalink to this heading">#</a></h2>
<p>An immediate consequence of the EGLP lemma is that for any function <span class="math notranslate nohighlight">\(b\)</span> which only depends on state,</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_{a_{t}\sim\pi_{\theta}}\left[\nabla_{\theta}\log\pi(a_t|s_t)b(s_t)\right]\]</div>
<p>This allows us to add or subtract any number of terms like this from our expression for the policy gradient, without changing it in expectation:</p>
<div class="math notranslate nohighlight">
\[\nabla_{\theta}J(\theta) = \mathbb{E}_{\tau\sim\theta}\left[\sum_{t=0}^{T}\nabla_{\theta}\log\pi_{\theta}(a_{t}|s_{t})\left(\sum_{t'=t}^{T}R(s_{t'},a_{t'},s_{t'+1}) - b(s_{t})\right)\right]\]</div>
<p>Any function <span class="math notranslate nohighlight">\(b\)</span> used in this way is called a baseline.</p>
<p>The most common choice of baseline is the on-policy value function <span class="math notranslate nohighlight">\(V^{\pi}(s_t)\)</span>. Recall that this is the average return an agent gets if it starts in state <span class="math notranslate nohighlight">\(s_t\)</span> and then acts according to policy <span class="math notranslate nohighlight">\(\pi\)</span> for the rest of its life. Empirically, the choice <span class="math notranslate nohighlight">\(b(s_t) = V^{\pi}(s_t)\)</span> has the desirable effect of reducing variance in the sample estimate for the policy gradient. This results in faster and more stable policy learning.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In practice, <span class="math notranslate nohighlight">\(V^{\pi}(s_t)\)</span> cannot be computed exactly, so it has to be approximated. This is usually done with a neural network, <span class="math notranslate nohighlight">\(V_{\phi}(s_t)\)</span>, which is updated concurrently with the policy (so that the value network always approximates the value function of the most recent policy).</p>
<p>The simplest method for learning <span class="math notranslate nohighlight">\(V_{\phi}\)</span>, used in most implementations of policy optimization algorithms,  is to minimize a mean-squared-error objective:</p>
<div class="math notranslate nohighlight">
\[\phi_{k} = \arg\underset{\phi}{\min}\underset{s_t, \hat{R}_{t}\sim\pi_{k}}{\mathbb{E}}\left[\left(V_{\phi}(s_t) - \hat{R}_{t}\right)^{2}\right]\]</div>
<p>where <span class="math notranslate nohighlight">\(\pi_k\)</span> is the policy at epoch <span class="math notranslate nohighlight">\(k\)</span>. This is done with one or more steps of gradient descent, starting from the previous value parameters <span class="math notranslate nohighlight">\(\phi_{k-1}\)</span>.</p>
</div>
</section>
<section id="other-forms-of-the-policy-gradient">
<h2>Other Forms of the Policy Gradient<a class="headerlink" href="#other-forms-of-the-policy-gradient" title="Permalink to this heading">#</a></h2>
<p>What we have seen so far is that the policy gradient has the general form</p>
<div class="math notranslate nohighlight">
\[\nabla_{\theta} J(\pi_{\theta}) = \underset{{\tau \sim \pi_{\theta}}}{\mathbb{E}}\left[{\sum_{t=0}^{T} \nabla_{\theta} \log \pi_{\theta}(a_t |s_t) \Phi_t}\right],\]</div>
<p>where <span class="math notranslate nohighlight">\(\Phi_t\)</span> could be any of</p>
<div class="math notranslate nohighlight">
\[\Phi_t = R(\tau),\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[\Phi_t = \sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1}),\]</div>
<p>or</p>
<div class="math notranslate nohighlight">
\[\Phi_t = \sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1}) - b(s_t).\]</div>
<p>All of these choices lead to the same expected value for the policy gradient, despite having different variances. It turns out that there are two more valid choices of weights <span class="math notranslate nohighlight">\(\Phi_t\)</span> which are important to know.</p>
<ol class="arabic">
<li><p>On-Policy Action-Value Function. The choice</p>
<div class="math notranslate nohighlight">
\[\Phi_t = Q^{\pi_{\theta}}(s_t, a_t)\]</div>
<p>is also valid.</p>
</li>
<li><p>The Advantage Function defined by <span class="math notranslate nohighlight">\(A^{\pi}(s_t,a_t) = Q^{\pi}(s_t,a_t) - V^{\pi}(s_t)\)</span>, describes how much better or worse it is than other actions on average (relative to the current policy). This choice,</p>
<div class="math notranslate nohighlight">
\[\Phi_t = A^{\pi_{\theta}}(s_t, a_t)\]</div>
<p>is also valid.</p>
</li>
</ol>
</section>
<section id="pytorch-example">
<h2>Pytorch example<a class="headerlink" href="#pytorch-example" title="Permalink to this heading">#</a></h2>
<section id="policy-network">
<h3>Policy network<a class="headerlink" href="#policy-network" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>

<span class="k">class</span> <span class="nc">Policy</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;MLP&quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Policy</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">affine1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">affine2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">saved_log_probs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">affine1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">action_scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">affine2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">action_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">policy</span> <span class="o">=</span> <span class="n">Policy</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>*<code class="docutils literal notranslate"><span class="pre">self.saved_log_probs</span></code> saves <span class="math notranslate nohighlight">\(\left[\pi_{\theta}(a_{0}|s_{0}), \pi_{\theta}(a_{1}|s_{1}),\dots,\pi_{\theta}(a_{T}|s_{T})\right]\)</span></p>
<p>*<code class="docutils literal notranslate"><span class="pre">self.rewards</span></code> saves <span class="math notranslate nohighlight">\(\left[r_{0},r_{1},\dots,r_{T}\right]\)</span></p>
</section>
<section id="env">
<h3>Env<a class="headerlink" href="#env" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gym</span>

<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CartPole-v1&#39;</span><span class="p">)</span>
<span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(array([ 0.00118216,  0.04504637, -0.03558404,  0.04486495], dtype=float32),
 {})
</pre></div>
</div>
</div>
</div>
</section>
<section id="one-episode">
<h3>One episode<a class="headerlink" href="#one-episode" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Categorical</span>

<span class="k">def</span> <span class="nf">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">):</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
    <span class="n">policy</span><span class="o">.</span><span class="n">saved_log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">action</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>

<span class="k">def</span> <span class="nf">finish_episode</span><span class="p">():</span>
    <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">eps</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">finfo</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span><span class="o">.</span><span class="n">eps</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
    <span class="n">policy_loss</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="n">deque</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="n">policy</span><span class="o">.</span><span class="n">rewards</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">r</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">R</span>
        <span class="n">returns</span><span class="o">.</span><span class="n">appendleft</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">returns</span><span class="p">)</span>
    <span class="n">returns</span> <span class="o">=</span> <span class="p">(</span><span class="n">returns</span> <span class="o">-</span> <span class="n">returns</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">returns</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">+</span> <span class="n">eps</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">R</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">policy</span><span class="o">.</span><span class="n">saved_log_probs</span><span class="p">,</span> <span class="n">returns</span><span class="p">):</span>
        <span class="n">policy_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="o">-</span><span class="n">log_prob</span> <span class="o">*</span> <span class="n">R</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">policy_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">policy_loss</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">policy_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">del</span> <span class="n">policy</span><span class="o">.</span><span class="n">rewards</span><span class="p">[:]</span>
    <span class="k">del</span> <span class="n">policy</span><span class="o">.</span><span class="n">saved_log_probs</span><span class="p">[:]</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">returns</span></code> = <span class="math notranslate nohighlight">\(\left[R_{0}, R_{1}, \dots, R_{T}\right]\)</span> where <span class="math notranslate nohighlight">\(R_{t} = r_{t} + \gamma r_{t+1} + \gamma^{2}r_{t+2} + \dots\)</span></p>
<p><code class="docutils literal notranslate"><span class="pre">policy_loss</span></code> = <span class="math notranslate nohighlight">\(\sum_{i=0}^{T}-\log\pi_{\theta}(a_{t}|s_{t})R_{t}\)</span></p>
</section>
<section id="main-loop">
<h3>Main loop<a class="headerlink" href="#main-loop" title="Permalink to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">count</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">running_reward</span> <span class="o">=</span> <span class="mi">10</span>
    <span class="k">for</span> <span class="n">i_episode</span> <span class="ow">in</span> <span class="n">count</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">ep_reward</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10000</span><span class="p">):</span>  <span class="c1"># Don&#39;t infinite loop while learning</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">select_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">policy</span><span class="o">.</span><span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
            <span class="n">ep_reward</span> <span class="o">+=</span> <span class="n">reward</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="k">break</span>

        <span class="n">running_reward</span> <span class="o">=</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="n">ep_reward</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="mf">0.05</span><span class="p">)</span> <span class="o">*</span> <span class="n">running_reward</span>
        <span class="n">finish_episode</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">i_episode</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Episode </span><span class="si">{}</span><span class="se">\t</span><span class="s1">Last reward: </span><span class="si">{:.2f}</span><span class="se">\t</span><span class="s1">Average reward: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                  <span class="n">i_episode</span><span class="p">,</span> <span class="n">ep_reward</span><span class="p">,</span> <span class="n">running_reward</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">running_reward</span> <span class="o">&gt;</span> <span class="n">env</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">reward_threshold</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Solved! Running reward is now </span><span class="si">{}</span><span class="s2"> and &quot;</span>
                  <span class="s2">&quot;the last episode runs to </span><span class="si">{}</span><span class="s2"> time steps!&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">running_reward</span><span class="p">,</span> <span class="n">t</span><span class="p">))</span>
            <span class="k">break</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/xiayunhui/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)
  if not isinstance(terminated, (bool, np.bool8)):
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Episode 10	Last reward: 45.00	Average reward: 15.06
Episode 20	Last reward: 35.00	Average reward: 27.11
Episode 30	Last reward: 98.00	Average reward: 40.73
Episode 40	Last reward: 30.00	Average reward: 42.89
Episode 50	Last reward: 39.00	Average reward: 50.75
Episode 60	Last reward: 92.00	Average reward: 66.16
Episode 70	Last reward: 331.00	Average reward: 107.18
Episode 80	Last reward: 103.00	Average reward: 154.36
Episode 90	Last reward: 48.00	Average reward: 113.81
Episode 100	Last reward: 96.00	Average reward: 99.90
Episode 110	Last reward: 172.00	Average reward: 155.10
Episode 120	Last reward: 239.00	Average reward: 206.40
Episode 130	Last reward: 196.00	Average reward: 224.19
Episode 140	Last reward: 194.00	Average reward: 251.26
Episode 150	Last reward: 372.00	Average reward: 239.45
Episode 160	Last reward: 305.00	Average reward: 362.52
Solved! Running reward is now 649.4581415449471 and the last episode runs to 5964 time steps!
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="10-dqn-torch.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Deep Q-Learning with Pytorch</p>
      </div>
    </a>
    <a class="right-next"
       href="12-actor-critic.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Actor-Critic Methods</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-policy-gradient-methods">The policy-gradient methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-advantages-and-disadvantages-of-policy-gradient-methods">The advantages and disadvantages of policy-gradient methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-policy-gradient-theorem">The Policy Gradient Theorem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-reinforce-algorithm-monte-carlo-reinforce">The Reinforce algorithm (Monte Carlo Reinforce)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dont-let-the-past-distract-you">Don’t Let the Past Distract You</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#baselines-in-policy-gradients">Baselines in Policy Gradients</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-forms-of-the-policy-gradient">Other Forms of the Policy Gradient</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pytorch-example">Pytorch example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-network">Policy network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#env">Env</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-episode">One episode</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#main-loop">Main loop</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>