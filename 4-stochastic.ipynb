{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08ce6270-5975-4fb6-ba6d-54095ac051e1",
   "metadata": {},
   "source": [
    "# Stochastic approximation\n",
    "\n",
    "## Mean estimation\n",
    "\n",
    "The expected value of $X$ can be approximated by\n",
    "\n",
    "$$\\mathbb{E}(X)\\approx\\bar{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}$$\n",
    "\n",
    "We can rewrite it in an incremental manner, denote\n",
    "\n",
    "$$w_{k+1} = \\frac{1}{k}\\sum_{i=1}^{k}x_{i}$$\n",
    "\n",
    "Then $w_{k+1}$ can be expressed in terms of $w_{k}$:\n",
    "\n",
    "$$w_{k+1} = w_{k} - \\frac{1}{k}(w_{k} - x_{k})$$\n",
    "\n",
    "Furthermore, consider an algorithm with a more general expression (moving average):\n",
    "\n",
    "$$w_{k+1} = w_{k} - \\alpha_{k}(w_{k} - x_{k}) = (1 - \\alpha_{k})w_{k} + \\alpha_{k}x_{k}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2cfcb4-9ba1-41e8-abbb-f56ea39cfd19",
   "metadata": {},
   "source": [
    "## Robbins-Monro algorithm\n",
    "\n",
    "Suppose that we would like to find the root of the equation\n",
    "\n",
    "$$g(w) = 0$$\n",
    "\n",
    "where $w\\in\\mathbb{R}$ is the unknown variable and $g:\\mathbb{R}\\to\\mathbb{R}$ is a function. Many problems can be formulated as root-finding problems, e.g. optimizing objective function $J(w)$ can be converted to solving $g(w) = \\nabla_{w}J(w) = 0$.\n",
    "\n",
    "If the expression of $g$ or its derivative is known, there are many numerical algorithms that can be used. However, the problem we are facing is that the expression of the function $g$ is unkown, e.g. the function may be represented by an artificial neural network whose structure and parameters are unknown. Moreover, we can only obtain a noisy observation of $g(w)$:\n",
    "\n",
    "$$\\tilde{g}(w, \\eta) = g(w) + \\eta$$\n",
    "\n",
    "where $\\eta\\in\\mathbb{R}$ is the observation error. It is a black-box where only the input $w$ and the noisy output $\\tilde{g}(w, \\eta)$ are known. The RM algorithm that can solve $g(w)=0$ is\n",
    "\n",
    "$$w_{k+1} = w_{k} - a_{k}\\tilde{g}(w_{k}, \\eta_{k})$$\n",
    "\n",
    "(Robbins-Monre theorem) If\n",
    "\n",
    "1. $0<c_{1}\\le\\nabla_{w}g(w)<c_{2}$ for all w;\n",
    "2. $\\sum_{k=1}^{\\infty}a_{k}=\\infty$ and $\\sum_{k=1}^{\\infty}a_{k}^{2}<\\infty$\n",
    "3. $\\mathbb{E}[\\eta_{k}|\\mathcal{H}_{k}]=0$ and $\\mathbb{E}[\\eta_{k}^{2}|\\mathcal{H}_{k}]<\\infty$\n",
    "\n",
    "where $\\mathcal{H}_{k}=\\{w_{k},w_{k-1},\\dots\\}$, then $w_{k}$ almost surely converges to the root $w^{\\ast}$ satisfying $g(w^{\\ast})=0$.\n",
    "\n",
    "Let $g(w) = w - \\mathbb{E}(X) = 0$, this algorithm can be apply to mean estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3c1b4a-f50e-4fff-8b5b-551035c2e4a6",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent\n",
    "\n",
    "Consider the following optimization problem:\n",
    "\n",
    "$$\\underset{w}{\\min}J(w) = \\mathbb{E}[f(w, X)]$$\n",
    "\n",
    "where $w$ is the parameter to be optimized, and $X$ is a random variable, the expectation is calculated with respect to $X$.\n",
    "\n",
    "A straightforward method for solving this problem is gradient descent:\n",
    "\n",
    "$$w_{k+1} = w_{k} - \\alpha_{k}\\nabla_{w}J(w_{k}) = w_{k} - \\alpha_{k}\\mathbb{E}[\\nabla_{w}f(w, X)]$$\n",
    "\n",
    "The gradient descent algorithm requires the expected value $\\mathbb{E}[\\nabla_{w}f(w, X)]$, one way to obatin this is the Monte-Carlo method:\n",
    "\n",
    "$$\\mathbb{E}[\\nabla_{w}f(w, X)] \\approx \\frac{1}{n}\\sum_{i=1}^{n}\\nabla_{w}f(w_{k},x_{i})$$\n",
    "\n",
    "This leads to batch gradient-descent:\n",
    "\n",
    "$$w_{k+1} = w_{k} - \\frac{\\alpha_{k}}{n}\\sum_{i=1}^{n}\\nabla_{w}f(w_{k},x_{i})$$\n",
    "\n",
    "Similarily, stochastic gradient descent:\n",
    "\n",
    "$$w_{k+1} = w_{k} - \\alpha_{k}\\nabla_{w}f(w_{k},x_{k})$$\n",
    "\n",
    "Let $g(w) = \\mathbb{E}[\\nabla_{w}f(w, X)] = 0$ be the equation we want to solve, then $\\nabla_{w}f(w_{k},x_{k})$ is an noisy output to approximate $\\mathbb{E}[\\nabla_{w}f(w, X)]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7df23d-c445-4b97-b884-6217dea2b6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
