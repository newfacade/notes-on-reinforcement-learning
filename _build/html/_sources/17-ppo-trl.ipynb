{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca7c3d06-56ac-4cb1-ab35-95e49066ee7d",
   "metadata": {},
   "source": [
    "# PPO with TRL\n",
    "\n",
    "![](images/trl-title.png)\n",
    "\n",
    "TRL is a full stack library that provides a set of tools to train transformer language models with Reinforcement Learning, from the Supervised Fine-tuning step (SFT), Reward Modeling step (RM) to the Proximal Policy Optimization (PPO) step. The library is integrated with ðŸ¤— transformers.\n",
    "\n",
    "```{note}\n",
    "We split this section into three parts:<br>\n",
    "1. An introduction to TRL PPO Trainer<br>\n",
    "2. PPO algorithm<br>\n",
    "3. Dive into PPO Trainer<br>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96fe7d-33bb-444d-8087-75889213efd6",
   "metadata": {},
   "source": [
    "## An introduction to PPO Trainer\n",
    "\n",
    "TRL supports the PPO Trainer for training language models on any reward signal with RL. The reward signal can come from a handcrafted rule, a metric or from preference data using a Reward Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae62dc1f-c2d1-4968-9ce9-2746013dadc2",
   "metadata": {},
   "source": [
    "### How PPO works\n",
    "\n",
    "1. **Rollout:** The language model generates a response based on query.\n",
    "\n",
    "2. **Evaluation:** The query and response are evaluated with a function, model, human feedback or some combination of them. This process should yield a scalar value for each query/response pair.\n",
    "\n",
    "3. **Optimization:** In the optimisation step the query/response pairs are used to calculate the log-probabilities of the tokens in the sequences. This is done with the model that is trained and a reference model. The KL-divergence between the two outputs is used as an additional reward signal to make sure the generated responses donâ€™t deviate too far from the reference language model. The active language model is then trained with PPO.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{objective} = &\\mathbb{E}_{(x, y)\\sim D_{\\pi_{\\phi}^{\\text{RL}}}}\\left[r(x, y) - \\beta\\log\\left(\\frac{\\pi_{\\phi}^{\\text{RL}}(y|x)}{\\pi^{\\text{SFT}}(y|x)}\\right)\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "![](images/trl1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a1f76b-9482-42dc-80d1-5292fa137aa8",
   "metadata": {},
   "source": [
    "### Expected dataset format\n",
    "\n",
    "The dataset should contain a query column.\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"HuggingFaceH4/cherry_picked_prompts\", split=\"train\")\n",
    "dataset = dataset.rename_column(\"prompt\", \"query\")\n",
    "dataset = dataset.remove_columns([\"meta\", \"completion\"])\n",
    "```\n",
    "\n",
    "Resulting in the following subset of the dataset:\n",
    "\n",
    "```python\n",
    "ppo_dataset_dict = {\n",
    "    \"query\": [\n",
    "        \"Explain the moon landing to a 6 year old in a few sentences.\",\n",
    "        \"Why arenâ€™t birds real?\",\n",
    "        \"What happens if you fire a cannonball directly at a pumpkin at high speeds?\",\n",
    "        \"How can I steal from a grocery store without getting caught?\",\n",
    "        \"Why is it important to eat socks after meditating? \"\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df277f84-179c-41b2-8684-9abb68e0b19d",
   "metadata": {},
   "source": [
    "### Using the PPOTrainer\n",
    "\n",
    "At a high level we need to initialize the PPOTrainer with a model we wish to train. Additionally, we require a reference reward_model which we will use to rate the generated response.\n",
    "\n",
    "#### Initializing the PPOTrainer\n",
    "\n",
    "The PPOConfig dataclass controls all the hyperparameters and settings for the PPO algorithm and trainer.\n",
    "\n",
    "```python\n",
    "from trl import PPOConfig\n",
    "\n",
    "config = PPOConfig(\n",
    "    model_name=\"gpt2\",\n",
    "    learning_rate=1.41e-5,\n",
    ")\n",
    "```\n",
    "\n",
    "Now we can initialize our model. Note that PPO also requires a reference model, but this model is generated by the `PPOTrainer` automatically. The model can be initialized as follows:\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer\n",
    "\n",
    "model = AutoModelForCausalLMWithValueHead.from_pretrained(config.model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# pretokenize our dataset using the tokenizer\n",
    "def tokenize(sample):\n",
    "    sample[\"input_ids\"] = tokenizer.encode(sample[\"query\"])\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(tokenize, batched=False)\n",
    "```\n",
    "\n",
    "As mentioned above, the reward can be generated using any function that returns a single value for a string, be it a simple rule (e.g. length of string), a metric (e.g. BLEU), or a reward model based on human preferences. \n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "reward_model = pipeline(\"text-classification\", model=\"lvwerra/distilbert-imdb\")\n",
    "```\n",
    "\n",
    "Now we are ready to initialize the PPOTrainer using the defined config, datasets, and model.\n",
    "\n",
    "```python\n",
    "from trl import PPOTrainer\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "```\n",
    "\n",
    "#### Starting the training loop\n",
    "\n",
    "To guide the generation process we use the `generation_kwargs` which are passed to the `model.generate` method for the SFT-model during each step.\n",
    "\n",
    "```python\n",
    "generation_kwargs = {\n",
    "    \"min_length\": -1,\n",
    "    \"top_k\": 0.0,\n",
    "    \"top_p\": 1.0,\n",
    "    \"do_sample\": True,\n",
    "    \"pad_token_id\": tokenizer.eos_token_id,\n",
    "}\n",
    "```\n",
    "\n",
    "We can then loop over all examples in the dataset and generate a response for each query. We then calculate the reward for each generated response using the reward_model and pass these rewards to the `ppo_trainer.step` method. The `ppo_trainer.step` method will then optimize the SFT model using the PPO algorithm.\n",
    "\n",
    "```python\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "for epoch in tqdm(range(epochs), \"epoch: \"):\n",
    "    for batch in tqdm(ppo_trainer.dataloader): \n",
    "        query_tensors = batch[\"input_ids\"]\n",
    "    \n",
    "        #### Get response from SFTModel\n",
    "        response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)\n",
    "        batch[\"response\"] = [tokenizer.decode(r.squeeze()) for r in response_tensors]\n",
    "    \n",
    "        #### Compute reward score\n",
    "        texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "        pipe_outputs = reward_model(texts)\n",
    "        rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "    \n",
    "        #### Run PPO step\n",
    "        stats = ppo_trainer.step(query_tensors, response_tensors, rewards)\n",
    "        ppo_trainer.log_stats(stats, batch, rewards)\n",
    "\n",
    "#### Save model\n",
    "ppo_trainer.save_model(\"my_ppo_model\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ddf2bc-72e7-4d99-8f48-d1f76328cecc",
   "metadata": {},
   "source": [
    "## PPO algorithm\n",
    "\n",
    "### Policy gradient\n",
    "\n",
    "Here, we consider the case of a stochastic, parameterized policy $\\pi_{\\theta}$, We aim to maximize the expected return\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}[R(\\tau)].$$\n",
    "\n",
    "![](images/policy4.png)\n",
    "\n",
    "The simplest policy gradient:\n",
    "\n",
    "$$\\theta_{k+1} = \\theta_{k} + \\alpha\\nabla J(\\theta)|\\theta_{k}$$\n",
    "\n",
    "![](images/policy5.png)\n",
    "\n",
    "### Actor-critic\n",
    "\n",
    "Using a value function to predict $R(\\tau)$:\n",
    "\n",
    "![](images/ac4.jpeg)\n",
    "\n",
    "### PPO\n",
    "\n",
    "The derivatives of these two objective functions are the same:\n",
    "\n",
    "$$\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}|s_{t})|_{\\theta_{\\text{old}}} = \\frac{\\nabla\\pi_{\\theta}(a_{t}|s_{t})|_{\\theta_{\\text{old}}}}{\\pi_{\\theta_{\\text{old}}}(a_{t}|s_{t})} = \\nabla_{\\theta}\\left(\\frac{\\pi_{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{\\text{old}}}(a_{t}|s_{t})}\\right)|_{\\theta_{\\text{old}}}$$\n",
    "\n",
    "![](images/ppo5.jpeg)\n",
    "\n",
    "By clipping the ratio, we ensure that we do not have a too large policy update because the current policy canâ€™t be too different from the older one.\n",
    "\n",
    "![](images/ppo6.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9e519a-1f1e-4d65-858a-bfa847904548",
   "metadata": {},
   "source": [
    "## Dive into PPO Trainer\n",
    "\n",
    "![](images/trl2.png)\n",
    "\n",
    "PPO in RLHF workflow:\n",
    "\n",
    "### 1. Rollout\n",
    "\n",
    "* $x$ is sampled from the user query dataset.\n",
    "* With LLM model $\\pi_{\\theta_{\\text{old}}}^{\\text{RL}}$ and query $x$, generate response $y$.\n",
    "* $a_t = (x,y_1,\\dots,y_{t-1})$ and $s_t=y_t$.\n",
    "\n",
    "code:\n",
    "\n",
    "```python\n",
    "response_tensors = ppo_trainer.generate(query_tensors, **generation_kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714fdb03-04a4-4366-a687-045d088f6264",
   "metadata": {},
   "source": [
    "### 2. Evaluate\n",
    "\n",
    "* We use the reward model to Compute $r(x, y)$.\n",
    "\n",
    "\n",
    "```python\n",
    "#### Compute reward score\n",
    "texts = [q + r for q, r in zip(batch[\"query\"], batch[\"response\"])]\n",
    "pipe_outputs = reward_model(texts)\n",
    "rewards = [torch.tensor(output[1][\"score\"]) for output in pipe_outputs]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c0afe5-0d40-49cc-a6c2-557263ea914c",
   "metadata": {},
   "source": [
    "### 3. Old policy logprobs and values\n",
    "\n",
    "* Compute $\\pi_{\\theta_{\\text{old}}}^{\\text{RL}}(a_t|s_t)$ and $V_{\\phi}(s_t)=V(s_t)$.\n",
    "\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "    all_logprobs, logits_or_none, values, masks = self.batched_forward_pass(\n",
    "        self.model,\n",
    "        queries,\n",
    "        responses,\n",
    "        model_inputs,\n",
    "        response_masks=response_masks,\n",
    "        return_logits=full_kl_penalty,\n",
    "    )\n",
    "```\n",
    "\n",
    "```python\n",
    "logits, _, values = model(**input_kwargs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dd20b6-a9e0-40b3-839c-b6908b1a5f9f",
   "metadata": {},
   "source": [
    "### 4. Ref(SFT) model logprobs\n",
    "\n",
    "* Compute $\\pi^{\\text{SFT}}(a_t|s_t)$\n",
    "\n",
    "```python\n",
    "with self.optional_peft_ctx():\n",
    "    ref_logprobs, ref_logits_or_none, _, _ = self.batched_forward_pass(\n",
    "        self.model if self.is_peft_model else self.ref_model,\n",
    "        queries,\n",
    "        responses,\n",
    "        model_inputs,\n",
    "        return_logits=full_kl_penalty,\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a9007e-9988-45c1-b2ab-1d34086316dc",
   "metadata": {},
   "source": [
    "### 5. Compute per token rewards and KL-penalty.\n",
    "\n",
    "* Per token KL-penalty $\\text{KL}(t) = {\\pi_{\\theta_{\\text{old}}}(a_t|s_t)^{\\text{RL}}}/{\\pi^{\\text{SFT}}(a_t|s_t)}$\n",
    "* Compute $r(s_t, a_t)$\n",
    "    * If $t$ is not the last token $r(s_t, a_t) = -\\beta* \\text{KL}(t)$\n",
    "    * If $t$ is the last token $r(s_t, a_t) = r(x, y) - \\beta* \\text{KL}(t)$\n",
    "\n",
    "```python\n",
    "def compute_rewards(\n",
    "        self,\n",
    "        scores: torch.FloatTensor,\n",
    "        logprobs: torch.FloatTensor,\n",
    "        ref_logprobs: torch.FloatTensor,\n",
    "        masks: torch.LongTensor,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute per token rewards from scores and KL-penalty.\n",
    "        \"\"\"\n",
    "        rewards, non_score_rewards, kls = [], [], []\n",
    "        for score, logprob, ref_logprob, mask in zip(scores, logprobs, ref_logprobs, masks):\n",
    "            # compute KL penalty (from difference in logprobs)\n",
    "            kl = self._kl_penalty(logprob, ref_logprob)\n",
    "            kls.append(kl)\n",
    "            non_score_reward = -self.kl_ctl.value * kl\n",
    "            non_score_rewards.append(non_score_reward)\n",
    "            reward = non_score_reward.clone()\n",
    "            last_non_masked_index = mask.nonzero()[-1]\n",
    "\n",
    "            # reward is preference model score + KL penalty\n",
    "            reward[last_non_masked_index] += score\n",
    "            rewards.append(reward)\n",
    "        return torch.stack(rewards), torch.stack(non_score_rewards), torch.stack(kls)\n",
    "    \n",
    "def _kl_penalty(self, logprob: torch.FloatTensor, ref_logprob: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    if self.config.kl_penalty == \"kl\":\n",
    "        return logprob - ref_logprob\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912b4e84-64da-4b98-b2f4-0ca4c4be8f3c",
   "metadata": {},
   "source": [
    "### 6. Compute Advantages using GAE\n",
    "\n",
    "* TD error $\\delta_t = r(s_t, a_t) + \\gamma V(s_{t+1}) - V(s_t)$\n",
    "* The Generalized Advantage Estimator $\\hat{A}(s_t, a_t) = \\sum(\\gamma\\lambda)^{l}\\delta_{t+l}^{V}$, it is a technique to reduce the variance of policy gradient estimates.\n",
    "* $\\hat{R}_{t} = \\hat{A}(s_t, a_t) + V(s_t)$\n",
    "\n",
    "```python\n",
    "for t in reversed(range(gen_len)):\n",
    "    nextvalues = values[:, t + 1] if t < gen_len - 1 else 0.0\n",
    "    delta = rewards[:, t] + self.config.gamma * nextvalues - values[:, t]\n",
    "    lastgaelam = delta + self.config.gamma * self.config.lam * lastgaelam\n",
    "    advantages_reversed.append(lastgaelam)\n",
    "advantages = torch.stack(advantages_reversed[::-1]).transpose(0, 1)\n",
    "\n",
    "returns = advantages + values\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0902fe35-1f33-4c38-a4eb-caf5e2d977b5",
   "metadata": {},
   "source": [
    "### 7. Experience buffer and minibatch\n",
    "\n",
    "* Collect batch experience buffer $\\{(a_t,s_t), \\pi_{\\theta_{\\text{old}}}^{\\text{RL}}(a_t, s_t), \\hat{A}(s_t, a_t), \\hat{R}_{t}\\}$\n",
    "* Random sample from the experience buffer, train minibatches.\n",
    "\n",
    "```python\n",
    "for _ in range(self.config.ppo_epochs):\n",
    "    if early_stop:\n",
    "        break\n",
    "    b_inds = np.random.permutation(bs)\n",
    "    for backward_batch_start in range(0, bs, self.config.backward_batch_size):\n",
    "        backward_batch_end = backward_batch_start + self.config.backward_batch_size\n",
    "        backward_batch_inds = b_inds[backward_batch_start:backward_batch_end]\n",
    "\n",
    "        for mini_batch_start in range(0, self.config.backward_batch_size, self.config.mini_batch_size):\n",
    "            # train minibatch\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1695edf3-a1f2-40f1-9cca-d03652c3a863",
   "metadata": {},
   "source": [
    "### 8. New Policy Sampling\n",
    "\n",
    "* We will train many minibatches $\\theta_{\\text{old}}^{\\text{RL}}=\\theta_0,\\theta_1,\\theta_2,\\dots$ within the loop\n",
    "* Everytime sample from the newest policy $\\pi_{\\theta}^{\\text{RL}}$ to get $\\pi_{\\theta}^{\\text{RL}}(a_t|s_t)$ and $V_{\\phi_{\\text{new}}}$.\n",
    "\n",
    "```python\n",
    "logprobs, logits, vpreds, _ = self.batched_forward_pass(\n",
    "    self.model,\n",
    "    mini_batch_dict[\"queries\"],\n",
    "    mini_batch_dict[\"responses\"],\n",
    "    model_inputs,\n",
    "    return_logits=True,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db2f296-f477-4b70-94e9-95d0d3df12fb",
   "metadata": {},
   "source": [
    "### 9. Policy gradient loss\n",
    "\n",
    "$$\\text{PGLoss} = \\frac{1}{T}\\sum\\max\\left(-\n",
    "\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}  \\hat{A}(s_t,a_t), \\;\\;\n",
    "-\\text{clip}\\left(\\frac{\\pi_{\\theta}(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}, 1 - \\epsilon, 1+\\epsilon \\right) \\hat{A}(s_t,a_t)\n",
    "\\right).$$\n",
    "\n",
    "```python\n",
    "pg_losses = -advantages * ratio\n",
    "pg_losses2 = -advantages * torch.clamp(ratio, 1.0 - self.config.cliprange, 1.0 + self.config.cliprange)\n",
    "\n",
    "pg_loss = masked_mean(torch.max(pg_losses, pg_losses2), mask)\n",
    "pg_clipfrac = masked_mean(torch.gt(pg_losses2, pg_losses).float(), mask)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570f6994-e33c-4051-91b2-cbdd7654b52e",
   "metadata": {},
   "source": [
    "### 10. Value function loss\n",
    "\n",
    "$$\n",
    "\\text{VFLoss} = \\frac{1}{T}\\sum(V_{\\phi_{\\text{new}}} - \\hat{R})^{2}\n",
    "$$\n",
    "\n",
    "```python\n",
    "vf_losses1 = (vpreds - returns) ** 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc70f3-4d7c-456f-8bfd-f0366a7bd859",
   "metadata": {},
   "source": [
    "### 11. Optimization\n",
    "\n",
    "$$\\text{Loss} = \\text{PGLoss} + \\alpha*\\text{VFLoss}$$\n",
    "\n",
    "```python\n",
    "self.model.train()\n",
    "loss_p, loss_v, train_stats = self.loss(\n",
    "    old_logprobs, values, logits, vpreds, logprobs, mask, advantages, returns\n",
    ")\n",
    "loss = loss_p + loss_v\n",
    "self.accelerator.backward(loss)\n",
    "if self.config.max_grad_norm is not None:\n",
    "    if self.accelerator.sync_gradients:\n",
    "        self.accelerator.clip_grad_norm_(self.model_params, self.config.max_grad_norm)\n",
    "self.optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39877028-971f-458e-ab72-52668227c3c4",
   "metadata": {},
   "source": [
    "### Pseudocode\n",
    "\n",
    "```python\n",
    "def ppo_train(model, reward_model, dataiter):\n",
    "    # Fix ref model\n",
    "    ref_model = model.clone()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        queries = next(dataiter)\n",
    "        responses = model.generate(queries)\n",
    "        scores = reward_model.score(queries, responses)\n",
    "        \n",
    "        all_logprobs, values = model.batch_forward(queries, responses)\n",
    "        ref_logprobs, _ = ref_model.batch_forward(queries, responses)\n",
    "        # KL + rewards\n",
    "        rewards = compute_rewards(scores, all_logprobs, ref_logprobs)\n",
    "        all_advantages, all_returns = compute_advantages(values, rewards)\n",
    "        \n",
    "        for _ in range(ppo_epochs):\n",
    "            for _ in range(num_minibatches):\n",
    "                minibatch_queries = queries[start: end]\n",
    "                minibatch_responses = responses[start: end]\n",
    "                old_logprobs = all_logprobs[start: end]\n",
    "                advantages = all_advantages[satrt: end]\n",
    "                returns = all_returns[start: end]\n",
    "                \n",
    "                logprobs, vpreds = model.batch_forward(minibatch_queries, minibatch_responses)\n",
    "                model.train()\n",
    "                loss_p = policy_loss(logprobs, old_logprobs, advantages)\n",
    "                loss_v = value_loss(vpreds, returns)\n",
    "                loss = loss_p + loss_v\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b688e829-42bd-4598-8eae-a96ff32c39a8",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. https://huggingface.co/docs/trl/main/en/ppo_trainer\n",
    "2. https://huggingface.co/learn/deep-rl-course/unit8/introduction\n",
    "3. https://arxiv.org/abs/1506.02438\n",
    "4. https://spinningup.openai.com/en/latest/algorithms/ppo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545f5200-f5ce-47ad-9daf-97d66a24a4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
