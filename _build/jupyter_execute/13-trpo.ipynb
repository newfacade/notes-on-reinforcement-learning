{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce9f6d82-9880-476a-b4bb-9cbf53bb88ae",
   "metadata": {},
   "source": [
    "# TRPO\n",
    "\n",
    "```{note}\n",
    "TRPO updates policies by taking the largest step possible to improve performance, while satisfying a special constraint on how close the new and old policies are allowed to be.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e560a4f2-284d-4a3e-8131-c42739434971",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "Consider an infinite-horizon discounted Markov decision\n",
    "process (MDP), defined by the tuple $(\\mathcal{S}, \\mathcal{A}, P, r, \\rho_{0},\\gamma)$ where $\\mathcal{S}$ is a finite set of states, $\\mathcal{A}$ is a finite set of actions, $P: \\mathcal{S}\\times\\mathcal{A}\\times\\mathcal{A}\\to\\mathbb{R}$ is the transition probability distriarbution, $r:\\mathcal{S}\\to\\mathbb{R}$ is the reward function (difference from our previous setting), $\\rho_{0}:\\mathcal{S}\\to\\mathbb{R}$ is\n",
    "the distribution of the initial state $s_{0}$, $\\gamma\\in(0, 1)$ is the\n",
    "discount factor.\n",
    "\n",
    "Let $\\pi$ denote a stochastic policy $\\pi:\\mathcal{S}\\times\\mathcal{A}\\to[0, 1]$, and let $\\eta(\\pi)$ denote its expected discounted reward:\n",
    "\n",
    "$$\\eta(\\pi) = \\mathbb{E}_{s_0,a_0,\\dots}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}r(s_{t})\\right]$$\n",
    "\n",
    "where $s_0\\sim\\rho_0(s_0)$, $a_t\\sim\\pi(a_t|s_t)$, $s_{t+1}\\sim P(s_{t+1}|s_{t}, a_{t})$. Let $A_{\\pi}(s, a) = Q_{\\pi}(s, a) - V_{\\pi}(s)$\n",
    "\n",
    "The following useful identity expresses the expected return\n",
    "of another policy $\\tilde{\\pi}$ in terms of the advantage over $\\pi$:\n",
    "\n",
    "$$\\eta(\\tilde{\\pi}) = \\eta(\\pi) + \\mathbb{E}_{s_0,a_0,\\dots\\sim\\tilde{\\pi}}\\left[\\sum_{t=0}^{\\infty}\\gamma^{t}A_{\\pi}(s_t,a_t)\\right]$$\n",
    "\n",
    "Let $\\rho_{\\pi}$ be the discounted visitation frequencies:\n",
    "\n",
    "$$\\rho_{\\pi}(s) = P(s_0=s) + \\gamma P(s_1=s) + \\gamma^{2}P(s_2=s) + \\dots$$\n",
    "\n",
    "We can rewrite Equation with a sum over states instead\n",
    "of timesteps:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\eta(\\tilde{\\pi}) &= \\eta(\\pi) + \\sum_{t=0}^{\\infty}\\sum_{s}P(s_{t}=s|\\tilde{\\pi})\\sum_{a}\\tilde{\\pi}(a|s)\\gamma^{t}A_{\\pi}(s, a) \\\\\n",
    "&= \\eta(\\pi) + \\sum_{s}\\sum_{t=0}^{\\infty}\\gamma^{t}P(s_{t}=s|\\tilde{\\pi})\\sum_{a}\\tilde{\\pi}(a|s)A_{\\pi}(s, a) \\\\\n",
    "&= \\eta(\\pi) + \\sum_{s}\\rho_{\\tilde{\\pi}}(s)\\sum_{a}\\tilde{\\pi}(a|s)A_{\\pi}(s, a)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This equation implies that any policy update $\\pi\\to\\tilde{\\pi}$ that\n",
    "has a nonnegative expected advantage at every state $s$ is guaranteed to increase\n",
    "the policy performance $\\rho$. However, the complex dependency of $rho_{\\tilde{\\pi}}(s)$ on $\\tilde{\\pi}$ makes it difficult to optimize directly.\n",
    "Instead, we introduce the following local approximation\n",
    "to $\\eta$:\n",
    "\n",
    "$$L_{\\pi}(\\tilde{\\pi}) = \\eta{\\pi} + \\sum_{s}\\rho_{\\pi}(s)\\sum_{a}\\tilde{\\pi}(a|s)A_{\\pi}(s, a)$$\n",
    "\n",
    "Note that $L_{\\pi}$ uses the visitation frequency $\\rho_{\\pi}$ rather than $\\rho_{\\tilde{\\pi}}$. If we have a parameterized policy $\\pi_{\\theta}$, where $\\pi_{\\theta}(a|s)$ is a differentiable function\n",
    "of the parameter vector $\\theta$, then $L_{\\pi}$ matches $\\eta$ to first order:\n",
    "\n",
    "$$L_{\\pi_{\\theta_{0}}}(\\pi_{\\theta_{0}}) = \\eta(\\pi_{\\theta_{0}})$$\n",
    "\n",
    "$$\\nabla_{\\theta}L_{\\pi_{\\theta_0}}(\\pi_{\\theta_0})|_{\\theta=\\theta_0} = \\nabla_{\\theta}\\eta(\\pi_{\\theta_{0}})|_{\\theta=\\theta_0}$$\n",
    "\n",
    "It implies that a sufficiently small step that improves $L_{\\pi_{\\theta_{\\text{old}}}}$ will also improve $\\eta$, but does not give\n",
    "us any guidance on how big of a step to take."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2363d4d-8ab8-4f2f-b94a-94a84b0ed622",
   "metadata": {},
   "source": [
    "## Monotonic Improvement Guarantee\n",
    "\n",
    "To address the above issue, the author proposed\n",
    "a policy updating scheme, for which they could provide explicit lower bounds\n",
    "on the improvement of $\\eta$. Let $\\pi_{\\text{old}}$ denote the current policy, denote total variation divergence $D_{\\text{TV}}(p\\|q) = \\frac{1}{2}\\sum_{i}|p_{i} - q_{i}|$, define a distance measure between $\\pi$ and $\\tilde{\\pi}$:\n",
    "\n",
    "$$\n",
    "D_{\\text{TV}}^{\\max}(\\pi, \\tilde{\\pi}) = \\max_{s}D_{\\text{TV}}(\\pi(\\cdot|s)\\left|\\right|\\tilde{\\pi}(\\cdot|s))\n",
    "$$\n",
    "\n",
    "**Theorem 1.** Let $\\alpha=D_{\\text{TV}}^{\\max}(\\pi_{\\text{old}}, \\pi_{\\text{new}})$. Then the following\n",
    "bound holds:\n",
    "\n",
    "$$\\eta(\\pi_{\\text{new}})\\ge L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}}) - \\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}\\alpha^{2}$$\n",
    "\n",
    "where $\\epsilon=\\max_{s,a}|A_{\\pi_{\\text{old}}}(s, a)|$.\n",
    "\n",
    "Next, we note the following relationship between the total\n",
    "variation divergence and the KL divergence $D_{\\text{TV}}(p\\|q)^{2}\\le D_{\\text{KL}}(p\\|q)$. Let $D_{\\text{KL}}^{\\max}(\\pi,\\tilde{\\pi}) = \\max_{s}D_{\\text{KL}}(\\pi(\\cdot|s)\\|\\tilde{\\pi}(\\cdot|s))$, the following bounds the follows directly:\n",
    "\n",
    "$$\\eta(\\tilde{\\pi})\\ge L_{\\pi}(\\tilde{\\pi}) - CD_{\\text{KL}}^{\\max}(\\pi,\\tilde{\\pi}),$$\n",
    "\n",
    "$$\\text{  where  }\\  C=\\frac{4\\epsilon\\gamma}{(1-\\gamma)^2}.$$\n",
    "\n",
    "**Algorithm 1** describes an approximate policy iteration\n",
    "scheme based on the policy improvement bound above. Note that for now, we assume exact evaluation of\n",
    "the advantage values $A_{\\pi}$.\n",
    "\n",
    "------------\n",
    "\n",
    "Initialize $\\pi_{0}$<br>\n",
    "**for** $i=0,1,2,\\dots$ until convergence **do**<br>\n",
    "$\\quad$Compute all advantage values $A_{\\pi_{i}}(s, a)$.<br>\n",
    "$\\quad$Solve the constrained optimization problem<br>\n",
    "$\\quad \\pi_{i+1} = \\arg\\max_{\\pi}\\left[L_{\\pi_{i}}(\\pi) - CD_{\\text{KL}}^{\\max}(\\pi_{i}, \\pi)\\right]$<br>\n",
    "$\\quad$$\\quad$where $C = 4\\epsilon\\gamma/(1-\\gamma)^{2}$<br>\n",
    "$\\quad$$\\quad$and $L_{\\pi_{i}}(\\pi) = \\eta(\\pi_{i}) + \\sum_{s}\\rho_{\\pi_{i}}(s)\\sum_{a}\\pi(a|s)A_{\\pi_{i}}(s,a)$<br>\n",
    "**end for**\n",
    "\n",
    "------------\n",
    "\n",
    "This algorithm is a type of minorization-maximization (MM) algorithm, which is guaranteed\n",
    "to generate a monotonically improving sequence of policies. Trust region policy optimization, which we propose in the\n",
    "following section, is an approximation to Algorithm 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e02425-a241-46d4-9fc4-2841e4d61b33",
   "metadata": {},
   "source": [
    "## Optimization of Parameterized Policies\n",
    "\n",
    "The preceding section showed that $\\eta(\\theta) \\ge L_{\\theta_{\\text{old}}}(\\theta) - CD_{\\text{KL}}^{\\max}(\\theta_{\\text{old}}, \\theta)$, with equality at $\\theta_{\\text{old}}=\\theta$. Thus, by performing\n",
    "the following maximization, we are guaranteed to\n",
    "improve the true objective $\\eta$:\n",
    "\n",
    "$$\\underset{\\theta}{\\text{maximize}}\\left[L_{\\theta_{\\text{old}}}(\\theta) - CD_{\\text{KL}}^{\\max}(\\theta_{\\text{old}}, \\theta)\\right].$$\n",
    "\n",
    "In practice, if we used the penalty coefficient C recommended\n",
    "by the theory above, the step sizes would be very\n",
    "small. One way to take larger steps in a robust way is to use\n",
    "a constraint on the KL divergence between the new policy\n",
    "and the old policy, i.e., a trust region constraint:\n",
    "\n",
    "$$\\underset{\\theta}{\\text{maximize}}L_{\\theta_{\\text{old}}}(\\theta)$$\n",
    "\n",
    "$$\\text{subject to  }D_{\\text{KL}}^{\\max}(\\theta_{\\text{old}}, \\theta)\\le\\delta.$$\n",
    "\n",
    "This problem imposes a constraint that the KL divergence\n",
    "is bounded at every point in the state space. While it is\n",
    "motivated by the theory, this problem is impractical to solve\n",
    "due to the large number of constraints. Instead, we can use\n",
    "a heuristic approximation which considers the average KL\n",
    "divergence:\n",
    "\n",
    "$$\\bar{D}_{\\text{KL}}^{\\rho}(\\theta_1, \\theta_2) :=\\mathbb{E}_{s\\sim\\rho}\\left[D_{\\text{KL}}(\\pi_{\\theta_1}(\\cdot|s)\\left|\\right|\\pi_{\\theta_2}(\\cdot|s))\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ee06e0-44d0-414a-a71e-55bff780a664",
   "metadata": {},
   "source": [
    "## Sample-Based Estimation\n",
    "\n",
    "This section describes how the objective and constraint\n",
    "functions can be approximated using Monte Carlo\n",
    "simulation. We seek to solve the following optimization problem, obtained\n",
    "by expanding $L_{\\theta_{\\text{old}}}$:\n",
    "\n",
    "$$\\underset{\\theta}{\\text{maximize}}\\sum_{s}\\rho_{\\pi_{\\theta_{\\text{old}}}}(s)\\sum_{a}\\pi_{\\theta}(a|s)A_{\\pi_{\\theta_{\\text{old}}}}(s,a)$$\n",
    "\n",
    "$$\\text{subject to  }\\bar{D}_{\\text{KL}}^{\\rho_{\\theta_{\\text{old}}}}(\\theta_{\\text{old}}, \\theta)\\le\\delta.$$\n",
    "\n",
    "* We first replace $\\sum_{s}\\rho_{\\pi_{\\theta_{\\text{old}}}}(s)[\\dots]$ in the objective by the expectation $\\frac{1}{1-\\gamma}\\mathbb{E}_{s\\sim\\rho_{\\theta_{\\text{old}}}}[\\dots]$\n",
    "\n",
    "* Next, we\n",
    "replace the sum over the actions by an importance sampling\n",
    "estimator. Using $q$ to denote the sampling distribution, the\n",
    "contribution of a single $s_{n}$ to the loss function is\n",
    "\n",
    "    $$\\sum_{a}\\pi_{\\theta}(a|s)A_{\\pi_{\\theta_{\\text{old}}}}(s,a) = \\mathbb{E}_{a\\sim q}\\left[\\frac{\\pi_{\\theta}(a|s_n)}{q(a|s_n)}A_{\\theta_{\\text{old}}}(s_n,a)\\right]$$\n",
    "    \n",
    "Our optimization problem is exactly\n",
    "equivalent to the following one:\n",
    "\n",
    "$$\\underset{\\theta}{\\text{maximize}}\\ \\mathbb{E}_{s\\sim\\rho_{\\theta_{\\text{old}}},a\\sim q}\\left[\\frac{\\pi_{\\theta}(a|s_n)}{q(a|s_n)}A_{\\theta_{\\text{old}}}(s_n,a)\\right]$$\n",
    "\n",
    "$$\\text{subject to  }\\mathbb{E}_{s\\sim\\rho_{\\theta_{\\text{old}}}}\\left[D_{\\text{KL}}(\\pi_{\\theta_{\\text{old}}}(\\cdot|s)\\left|\\right|\\pi_{\\theta}(\\cdot|s))\\right] \\le \\delta.$$\n",
    "\n",
    "In the estimation procedure, we collect a sequence of\n",
    "states by sampling $s_0\\sim\\rho_0$ and then simulating the policy $\\pi_{\\theta_{\\text{old}}}$ for some number of timesteps to generate a trajectory $s_0,a_0,s_1,a_1,\\dots,s_T,a_T$. Hence, $q(a|s)=\\pi_{\\theta_{\\text{old}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e00f191-a5f7-4f40-ae2b-01749e9152a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}