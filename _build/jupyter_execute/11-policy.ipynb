{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ff8171-e805-4edd-aa1d-838f951631df",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n",
    "```{note}\n",
    "Since the beginning of the course, we have only studied value-based methods, where we estimate a value function as an intermediate step towards finding an optimal policy. Finding an optimal value function leads to having an optimal policy:\n",
    "\n",
    "$$\\pi^{\\ast}(s) = \\arg\\max_{a}Q^{\\ast}(s, a)$$\n",
    "\n",
    "With policy-based methods, we want to optimize the policy directly without having an intermediate step of learning a value function.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab37f54-b2c0-4c94-80e9-446caa63c646",
   "metadata": {},
   "source": [
    "## The policy-gradient methods\n",
    "\n",
    "In policy-based methods, we directly learn to approximate $\\pi^{\\ast}$ without having to learn a value function. \n",
    "\n",
    "* The idea is to parameterize the policy. For instance, using a neural network $\\pi_{\\theta}$, this policy will output a probability distribution over actions (stochastic policy).\n",
    "\n",
    "* Our objective then is to maximize the performance of the parameterized policy using gradient ascent. To do that, we define an objective function $J(\\theta)$, that is, the expected cumulative reward, and we want to find the value $\\theta$ that maximizes this objective function.\n",
    "\n",
    "![](images/policy2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1d3390-4071-48b8-b1b2-dd6f8e72bfb3",
   "metadata": {},
   "source": [
    "## The advantages and disadvantages of policy-gradient methods\n",
    "\n",
    "There are multiple advantages over value-based methods. Let’s see some of them:\n",
    "\n",
    "* Policy-gradient methods can learn a stochastic policy while value functions can’t.\n",
    "\n",
    "* Policy-gradient methods are more effective in high-dimensional action spaces and continuous actions spaces. The problem with Deep Q-learning is that their predictions assign a score for each possible action, at each time step, given the current state. Instead, with policy-gradient methods, we output a probability distribution over actions.\n",
    "    \n",
    "* Policy-gradient methods have better convergence properties. In value-based methods, we use an aggressive operator to change the value function: we take the maximum over Q-estimates. Consequently, the action probabilities may change dramatically for an arbitrarily small change in the estimated action values if that change results in a different action having the maximal value.\n",
    "    \n",
    "Naturally, policy-gradient methods also have some disadvantages:\n",
    "\n",
    "* Frequently, policy-gradient methods converges to a local maximum instead of a global optimum.\n",
    "\n",
    "* Policy-gradient goes slower, step by step: it can take longer to train.\n",
    "\n",
    "* Policy-gradient can have high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e5dd2-6312-4399-9095-2067fbd321e7",
   "metadata": {},
   "source": [
    "## The Policy Gradient Theorem\n",
    "\n",
    "The objective function outputs the expected cumulative reward:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau\\sim\\pi}[R(\\tau)]$$\n",
    "\n",
    "It can be formulated as:\n",
    "\n",
    "![](images/policy3.png)\n",
    "\n",
    "Policy-gradient is an optimization problem: we want to find the values of $\\theta$ that maximize our objective function $J(\\theta)$,  so we need to use gradient-ascent:\n",
    "\n",
    "$$\\theta\\leftarrow\\theta + \\alpha * \\nabla J(\\theta)$$\n",
    "\n",
    "However, there are two problems with computing the derivative of $J(\\theta)$:\n",
    "\n",
    "1. We can’t calculate the true gradient of the objective function since it requires calculating the probability of each possible trajectory, which is computationally super expensive. So we want to calculate a gradient estimation with a sample-based estimate.\n",
    "\n",
    "2. To differentiate this objective function, we need to differentiate the state distribution, this is attached to the environment. The problem is that we can’t differentiate it because we might not know about it.\n",
    "\n",
    "Fortunately we’re going to use a solution called the Policy Gradient Theorem that will help us to reformulate the objective function into a differentiable function that does not involve the differentiation of the state distribution.\n",
    "\n",
    "![](images/policy4.png)\n",
    "\n",
    "````{prf:proof}\n",
    "We have:\n",
    "\n",
    "```{math}\n",
    "\\begin{aligned}\n",
    "\\nabla J(\\theta) &= \\nabla_{\\theta}\\sum_{\\tau}P(\\tau;\\theta)R(\\tau)\\\\\n",
    "&=\\sum_{\\tau}\\nabla_{\\theta}P(\\tau;\\theta)R(\\tau)\\\\\n",
    "&=\\sum_{\\tau}P(\\tau;\\theta)\\frac{\\nabla_{\\theta}P(\\tau;\\theta)}{P(\\tau;\\theta)}R(\\tau)\\\\\n",
    "&=\\sum_{\\tau}P(\\tau;\\theta)\\nabla{\\log P(\\tau;\\theta)}R(\\tau)\\\\\n",
    "&=\\sum_{\\tau}P(\\tau;\\theta)\\nabla\\left[\\log \\mu(s_{0})\\Pi_{t=0}^{H}P(s_{t+1}|s_{t}, a_{t})\\pi_{\\theta}(a_{t}|s_{t})\\right]R(\\tau)\\\\\n",
    "&=\\sum_{\\tau}P(\\tau;\\theta)\\sum_{t=0}^{H}\\nabla \\pi_{\\theta}(a_{t}|s_{t})R(\\tau)\\\\\n",
    "&=\\mathbb{E}_{\\pi_{\\theta}}\\left[\\nabla_{\\theta}\\log \\pi_{\\theta}(a_{t}|s_{t})R(\\tau)\\right]\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded50b16-fe51-4e2c-8041-f26352f28dd5",
   "metadata": {},
   "source": [
    "## The Reinforce algorithm (Monte Carlo Reinforce)\n",
    "\n",
    "In a loop:\n",
    "\n",
    "* Use the policy $\\pi_{\\theta}$ to collect some episodes\n",
    "\n",
    "* Use these episodes to estimate the gradient.\n",
    "\n",
    "![](images/policy5.png)\n",
    "\n",
    "We can interpret this update as follows: $-\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}|s_{t})$ is the direction of steepest increase of the (log) probability of selecting action at from state $s_{t}$. This tells us:\n",
    "\n",
    "* If the return $R(\\tau)$ is high, it will push up the probabilities of the (state, action) combinations.\n",
    "\n",
    "* Otherwise, it will push down the probabilities of the (state, action) combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c175ce-3f3d-42b4-9602-5b52ba714720",
   "metadata": {},
   "source": [
    "## Pytorch example\n",
    "\n",
    "### Policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56baefdc-2e18-4d23-bd5d-69c402d0f69b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"MLP\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a45c45-ad51-4d9b-9e72-a3b8bc3adcc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fbb192-4376-4afb-8411-49bd9921d1f5",
   "metadata": {},
   "source": [
    "*`self.saved_log_probs` saves $\\left[\\pi_{\\theta}(a_{0}|s_{0}), \\pi_{\\theta}(a_{1}|s_{1}),\\dots,\\pi_{\\theta}(a_{T}|s_{T})\\right]$\n",
    "\n",
    "*`self.rewards` saves $\\left[r_{0},r_{1},\\dots,r_{T}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddba04-da7e-4a11-9c49-1ef6a727cbe5",
   "metadata": {},
   "source": [
    "### Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92613ed1-e496-4778-9305-cd7203de758d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00118216,  0.04504637, -0.03558404,  0.04486495], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ff5b1-b84f-4e41-8625-e6228aedda58",
   "metadata": {},
   "source": [
    "### One episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fcf4c5e-1c79-451b-bd89-6286b0e08682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7fbab79-2848-4c4e-a4a3-0f1c6ec80cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    eps = np.finfo(np.float32).eps.item()\n",
    "    policy_loss = []\n",
    "    returns = deque()\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.appendleft(R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    \n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34264a19-b89f-4ac7-afad-1e7eabd5a6c3",
   "metadata": {},
   "source": [
    "`returns` = $\\left[R_{0}, R_{1}, \\dots, R_{T}\\right]$ where $R_{t} = r_{t} + \\gamma r_{t+1} + \\gamma^{2}r_{t+2} + \\dots$\n",
    "\n",
    "`policy_loss` = $\\sum_{i=0}^{T}-\\log\\pi_{\\theta}(a_{t}|s_{t})R_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f637a886-e987-4948-bf61-f64d2eb7c0bd",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "345eea62-3dd6-4744-8df2-bb41e7810541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from collections import deque\n",
    "\n",
    "def main():\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % 10 == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c15b15e3-7d30-4241-89d4-076561a3a774",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tLast reward: 12.00\tAverage reward: 15.65\n",
      "Episode 20\tLast reward: 30.00\tAverage reward: 21.50\n",
      "Episode 30\tLast reward: 57.00\tAverage reward: 26.75\n",
      "Episode 40\tLast reward: 34.00\tAverage reward: 34.43\n",
      "Episode 50\tLast reward: 70.00\tAverage reward: 55.36\n",
      "Episode 60\tLast reward: 72.00\tAverage reward: 103.52\n",
      "Episode 70\tLast reward: 132.00\tAverage reward: 119.66\n",
      "Episode 80\tLast reward: 140.00\tAverage reward: 117.84\n",
      "Episode 90\tLast reward: 27.00\tAverage reward: 96.78\n",
      "Episode 100\tLast reward: 12.00\tAverage reward: 76.47\n",
      "Episode 110\tLast reward: 103.00\tAverage reward: 81.30\n",
      "Episode 120\tLast reward: 105.00\tAverage reward: 77.09\n",
      "Episode 130\tLast reward: 93.00\tAverage reward: 83.25\n",
      "Episode 140\tLast reward: 128.00\tAverage reward: 107.52\n",
      "Episode 150\tLast reward: 180.00\tAverage reward: 155.75\n",
      "Episode 160\tLast reward: 483.00\tAverage reward: 219.93\n",
      "Episode 170\tLast reward: 62.00\tAverage reward: 200.44\n",
      "Episode 180\tLast reward: 192.00\tAverage reward: 196.48\n",
      "Episode 190\tLast reward: 808.00\tAverage reward: 209.19\n",
      "Episode 200\tLast reward: 541.00\tAverage reward: 226.06\n",
      "Episode 210\tLast reward: 140.00\tAverage reward: 353.07\n",
      "Episode 220\tLast reward: 114.00\tAverage reward: 266.01\n",
      "Episode 230\tLast reward: 89.00\tAverage reward: 200.81\n",
      "Episode 240\tLast reward: 98.00\tAverage reward: 161.05\n",
      "Episode 250\tLast reward: 97.00\tAverage reward: 132.33\n",
      "Episode 260\tLast reward: 132.00\tAverage reward: 124.33\n",
      "Episode 270\tLast reward: 116.00\tAverage reward: 118.33\n",
      "Episode 280\tLast reward: 135.00\tAverage reward: 115.04\n",
      "Episode 290\tLast reward: 111.00\tAverage reward: 114.01\n",
      "Episode 300\tLast reward: 106.00\tAverage reward: 106.63\n",
      "Episode 310\tLast reward: 61.00\tAverage reward: 94.67\n",
      "Episode 320\tLast reward: 40.00\tAverage reward: 80.34\n",
      "Episode 330\tLast reward: 102.00\tAverage reward: 76.40\n",
      "Episode 340\tLast reward: 72.00\tAverage reward: 83.75\n",
      "Episode 350\tLast reward: 52.00\tAverage reward: 79.89\n",
      "Episode 360\tLast reward: 100.00\tAverage reward: 77.80\n",
      "Episode 370\tLast reward: 73.00\tAverage reward: 75.90\n",
      "Episode 380\tLast reward: 93.00\tAverage reward: 76.13\n",
      "Episode 390\tLast reward: 112.00\tAverage reward: 84.90\n",
      "Episode 400\tLast reward: 204.00\tAverage reward: 125.94\n",
      "Episode 410\tLast reward: 155.00\tAverage reward: 167.30\n",
      "Episode 420\tLast reward: 123.00\tAverage reward: 169.36\n",
      "Episode 430\tLast reward: 237.00\tAverage reward: 181.22\n",
      "Solved! Running reward is now 861.854364362971 and the last episode runs to 9999 time steps!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9599409-9991-40d0-ac3d-6bdc162d8346",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}