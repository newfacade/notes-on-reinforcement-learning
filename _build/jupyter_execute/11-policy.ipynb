{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ff8171-e805-4edd-aa1d-838f951631df",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n",
    "```{note}\n",
    "Since the beginning of the course, we have only studied value-based methods, where we estimate a value function as an intermediate step towards finding an optimal policy. Finding an optimal value function leads to having an optimal policy:\n",
    "\n",
    "$$\\pi^{\\ast}(s) = \\arg\\max_{a}Q^{\\ast}(s, a)$$\n",
    "\n",
    "With policy-based methods, we want to optimize the policy directly without having an intermediate step of learning a value function.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab37f54-b2c0-4c94-80e9-446caa63c646",
   "metadata": {},
   "source": [
    "## The policy-gradient methods\n",
    "\n",
    "In policy-based methods, we directly learn to approximate $\\pi^{\\ast}$ without having to learn a value function. \n",
    "\n",
    "* The idea is to parameterize the policy. For instance, using a neural network $\\pi_{\\theta}$, this policy will output a probability distribution over actions (stochastic policy).\n",
    "\n",
    "* Our objective then is to maximize the performance of the parameterized policy using gradient ascent. To do that, we define an objective function $J(\\theta)$, that is, the expected cumulative reward, and we want to find the value $\\theta$ that maximizes this objective function.\n",
    "\n",
    "![](images/policy2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1d3390-4071-48b8-b1b2-dd6f8e72bfb3",
   "metadata": {},
   "source": [
    "## The advantages and disadvantages of policy-gradient methods\n",
    "\n",
    "There are multiple advantages over value-based methods. Let’s see some of them:\n",
    "\n",
    "* Policy-gradient methods can learn a stochastic policy while value functions can’t.\n",
    "\n",
    "* Policy-gradient methods are more effective in high-dimensional action spaces and continuous actions spaces. The problem with Deep Q-learning is that their predictions assign a score for each possible action, at each time step, given the current state. Instead, with policy-gradient methods, we output a probability distribution over actions.\n",
    "    \n",
    "* Policy-gradient methods have better convergence properties. In value-based methods, we use an aggressive operator to change the value function: we take the maximum over Q-estimates. Consequently, the action probabilities may change dramatically for an arbitrarily small change in the estimated action values if that change results in a different action having the maximal value.\n",
    "    \n",
    "Naturally, policy-gradient methods also have some disadvantages:\n",
    "\n",
    "* Frequently, policy-gradient methods converges to a local maximum instead of a global optimum.\n",
    "\n",
    "* Policy-gradient goes slower, step by step: it can take longer to train.\n",
    "\n",
    "* Policy-gradient can have high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e5dd2-6312-4399-9095-2067fbd321e7",
   "metadata": {},
   "source": [
    "## The Policy Gradient Theorem\n",
    "\n",
    "The objective function outputs the expected cumulative reward:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau\\sim\\pi}[R(\\tau)]$$\n",
    "\n",
    "It can be formulated as:\n",
    "\n",
    "![](images/policy3.png)\n",
    "\n",
    "Policy-gradient is an optimization problem: we want to find the values of $\\theta$ that maximize our objective function $J(\\theta)$,  so we need to use gradient-ascent:\n",
    "\n",
    "$$\\theta\\leftarrow\\theta + \\alpha * \\nabla J(\\theta)$$\n",
    "\n",
    "However, there are two problems with computing the derivative of $J(\\theta)$:\n",
    "\n",
    "1. We can’t calculate the true gradient of the objective function since it requires calculating the probability of each possible trajectory, which is computationally super expensive. So we want to calculate a gradient estimation with a sample-based estimate.\n",
    "\n",
    "2. To differentiate this objective function, we need to differentiate the state distribution, this is attached to the environment. The problem is that we can’t differentiate it because we might not know about it.\n",
    "\n",
    "Fortunately we’re going to use a solution called the Policy Gradient Theorem that will help us to reformulate the objective function into a differentiable function that does not involve the differentiation of the state distribution.\n",
    "\n",
    "![](images/policy4.png)\n",
    "\n",
    "````{prf:proof}\n",
    "We have:\n",
    "\n",
    "```{math}\n",
    "\\begin{aligned}\n",
    "\\nabla J(\\theta) &= \\nabla_{\\theta}\\sum_{\\tau}P(\\tau;\\theta)R(\\tau)\\\\\n",
    "&=\\sum_{\\tau}\\nabla_{\\theta}P(\\tau;\\theta)R(\\tau)\\\\\n",
    "&=\\sum_{\\tau}P(\\tau;\\theta)\\frac{\\nabla_{\\theta}P(\\tau;\\theta)}{P(\\tau;\\theta)}R(\\tau)\\\\\n",
    "&=\\sum_{\\tau}P(\\tau;\\theta)\\nabla{\\log P(\\tau;\\theta)}R(\\tau)\\\\\n",
    "&=\\sum_{\\tau}P(\\tau;\\theta)\\nabla\\left[\\log \\mu(s_{0})\\Pi_{t=0}^{H}P(s_{t+1}|s_{t}, a_{t})\\pi_{\\theta}(a_{t}|s_{t})\\right]R(\\tau)\\\\\n",
    "&=\\sum_{\\tau}P(\\tau;\\theta)\\sum_{t=0}^{H}\\nabla \\pi_{\\theta}(a_{t}|s_{t})R(\\tau)\\\\\n",
    "&=\\mathbb{E}_{\\pi_{\\theta}}\\left[\\nabla_{\\theta}\\log \\pi_{\\theta}(a_{t}|s_{t})R(\\tau)\\right]\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded50b16-fe51-4e2c-8041-f26352f28dd5",
   "metadata": {},
   "source": [
    "## The Reinforce algorithm (Monte Carlo Reinforce)\n",
    "\n",
    "In a loop:\n",
    "\n",
    "* Use the policy $\\pi_{\\theta}$ to collect some episodes\n",
    "\n",
    "* Use these episodes to estimate the gradient.\n",
    "\n",
    "![](images/policy5.png)\n",
    "\n",
    "We can interpret this update as follows: $-\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}|s_{t})$ is the direction of steepest increase of the (log) probability of selecting action at from state $s_{t}$. This tells us:\n",
    "\n",
    "* If the return $R(\\tau)$ is high, it will push up the probabilities of the (state, action) combinations.\n",
    "\n",
    "* Otherwise, it will push down the probabilities of the (state, action) combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3020b9-1042-4d5d-80a9-0c3f636a5317",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}