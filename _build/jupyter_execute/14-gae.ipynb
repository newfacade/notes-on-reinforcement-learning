{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee51ea1-24ef-40a5-b4ee-9a3fb1e7da21",
   "metadata": {},
   "source": [
    "# Generalized Advantage Estimation\n",
    "\n",
    "```{note}\n",
    "The two\n",
    "main challenges of policy gradient methods are the large number of samples typically required, and the difficulty\n",
    "of obtaining stable and steady improvement despite the nonstationarity of the\n",
    "incoming data.<br>\n",
    "We address the first challenge by using value functions to substantially\n",
    "reduce the variance of policy gradient estimates at the cost of some bias. We address the second challenge by using trust region optimization\n",
    "procedure for both the policy and the value function, which are represented by\n",
    "neural networks.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35efc8b9-54b7-403c-a8da-a1ff97bab911",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "There are several different related expressions for the policy gradient, which\n",
    "have the form\n",
    "\n",
    "$$g = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty}\\Psi_{t}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_t|s_t)\\right]$$\n",
    "\n",
    "where $\\Psi_{t}$ may be one of the following:\n",
    "\n",
    "1. $\\sum_{t=0}^{\\infty}$: total reward of the trajectory.\n",
    "2. $\\sum_{t'=t}^{\\infty}r_{t'}$: reward following action $a_t$.\n",
    "3. $\\sum_{t'=t}^{\\infty}r_{t'} - b(s_t)$: baselined version of\n",
    "previous formula.\n",
    "4. $Q_{\\pi}(s_t, a_t)$: state-action value function.\n",
    "5. $A_{\\pi}(s_t, a_t)$: advantage function.\n",
    "6. $r_{t} + V_{\\pi}(s_{t+1}) - V_{\\pi}(s_t)$: TD residual.\n",
    "\n",
    "The choice $\\Psi_{t} = A_{\\pi}(s_t, a_t)$ yields almost the lowest possible variance, though in practice, the\n",
    "advantage function is not known and must be estimated.\n",
    "\n",
    "We will introduce a parameter $\\gamma$ that allows us to reduce variance by downweighting rewards corresponding\n",
    "to delayed effects, at the cost of introducing bias. This parameter corresponds to the\n",
    "discount factor used in discounted formulations of MDPs, , but we treat it as a variance reduction\n",
    "parameter in an undiscounted problem.\n",
    "\n",
    "$$V_{\\pi,\\gamma} := \\mathbb{E}_{s_{t+1}:\\infty, a_{t}:\\infty}\\left[\\sum_{l=0}^{\\infty}\\gamma^{l}r_{t+l}\\right]$$\n",
    "\n",
    "Before proceeding, we will introduce the notion of a $\\gamma-$just estimator of the advantage function,\n",
    "which is an estimator that does not introduce bias when we use it in place of $A^{\\pi,\\gamma}$.\n",
    "\n",
    "**Definition 1.** The estimator $\\hat{A}_{t}$ is $\\gamma-$just if\n",
    "\n",
    "$$\\mathbb{E}_{s_0:\\infty,a_0:\\infty}\\left[\\hat{A}_{t}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_t|s_t)\\right]=\\mathbb{E}_{s_0:\\infty,a_0:\\infty}\\left[A_{\\pi,\\gamma}(a_t, s_t)\\nabla_{\\theta}\\log\\pi_{\\theta}(a_t|s_t)\\right] = g^{\\gamma}$$\n",
    "\n",
    "**Proposition 1.** Suppose that $\\hat{A}_{t}$ can be written in the form $\\hat{A}_{t} = Q_t(s_{t:\\infty},a_{t:\\infty}) - b_t(s_{0:t,}, a_{0:t-1})$ such that for all $(s_t, a_t)$, $\\mathbb{E}Q_{t}=Q_{\\pi,\\gamma}$. Then $\\hat{A}$ is $\\gamma-$just."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da6b40a-b1ee-402e-abe3-19c2ea17d2c2",
   "metadata": {},
   "source": [
    "## Advantage function estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f6cab-5717-4a4b-b957-2848118dbe46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}