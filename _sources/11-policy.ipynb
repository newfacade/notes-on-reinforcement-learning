{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ff8171-e805-4edd-aa1d-838f951631df",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n",
    "```{note}\n",
    "Since the beginning of the course, we have only studied value-based methods, where we estimate a value function as an intermediate step towards finding an optimal policy. Finding an optimal value function leads to having an optimal policy:\n",
    "\n",
    "$$\\pi^{\\ast}(s) = \\arg\\max_{a}Q^{\\ast}(s, a)$$\n",
    "\n",
    "With policy-based methods, we want to optimize the policy directly without having an intermediate step of learning a value function.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab37f54-b2c0-4c94-80e9-446caa63c646",
   "metadata": {},
   "source": [
    "## The policy-gradient methods\n",
    "\n",
    "In policy-based methods, we directly learn to approximate $\\pi^{\\ast}$.\n",
    "\n",
    "* The idea is to parameterize the policy. For instance, using a neural network $\\pi_{\\theta}$, this policy will output a probability distribution over actions (stochastic policy).\n",
    "\n",
    "* Our objective then is to maximize the performance of the parameterized policy using gradient ascent. To do that, we define an objective function $J(\\theta)$, that is, the expected cumulative reward, and we want to find the value $\\theta$ that maximizes this objective function.\n",
    "\n",
    "![](images/policy2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1d3390-4071-48b8-b1b2-dd6f8e72bfb3",
   "metadata": {},
   "source": [
    "## The advantages and disadvantages of policy-gradient methods\n",
    "\n",
    "There are multiple advantages over value-based methods. Let’s see some of them:\n",
    "\n",
    "* Policy-gradient methods can learn a stochastic policy while value functions can’t.\n",
    "\n",
    "* Policy-gradient methods are more effective in high-dimensional action spaces and continuous actions spaces. The problem with Deep Q-learning is that their predictions assign a score for each possible action, at each time step, given the current state. Instead, with policy-gradient methods, we output a probability distribution over actions.\n",
    "    \n",
    "* Policy-gradient methods have better convergence properties. In value-based methods, we use an aggressive operator to change the value function: we take the maximum over Q-estimates. Consequently, the action probabilities may change dramatically for an arbitrarily small change in the estimated action values if that change results in a different action having the maximal value.\n",
    "    \n",
    "Naturally, policy-gradient methods also have some disadvantages:\n",
    "\n",
    "* Frequently, policy-gradient methods converges to a local maximum instead of a global optimum.\n",
    "\n",
    "* Policy-gradient goes slower, step by step: it can take longer to train.\n",
    "\n",
    "* Policy-gradient can have high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1e5dd2-6312-4399-9095-2067fbd321e7",
   "metadata": {},
   "source": [
    "## The Policy Gradient Theorem\n",
    "\n",
    "The objective function outputs the expected cumulative reward:\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}_{\\tau\\sim\\pi}[R(\\tau)]$$\n",
    "\n",
    "It can be formulated as:\n",
    "\n",
    "![](images/policy3.png)\n",
    "\n",
    "Policy-gradient is an optimization problem: we want to find the values of $\\theta$ that maximize our objective function $J(\\theta)$,  so we need to use gradient-ascent:\n",
    "\n",
    "$$\\theta\\leftarrow\\theta + \\alpha * \\nabla J(\\theta)$$\n",
    "\n",
    "However, there are two problems with computing the derivative of $J(\\theta)$:\n",
    "\n",
    "1. We can’t calculate the true gradient of the objective function since it requires calculating the probability of each possible trajectory, which is computationally super expensive. So we want to calculate a gradient estimation with a sample-based estimate.\n",
    "\n",
    "2. To differentiate this objective function, we need to differentiate the state distribution, this is attached to the environment. The problem is that we can’t differentiate it because we might not know about it.\n",
    "\n",
    "Fortunately we’re going to use a solution called the Policy Gradient Theorem that will help us to reformulate the objective function into a differentiable function that does not involve the differentiation of the state distribution.\n",
    "\n",
    "![](images/policy4.png)\n",
    "\n",
    "````{prf:proof}\n",
    "We have:\n",
    "\n",
    "```{math}\n",
    "\\begin{aligned}\n",
    "\\nabla J(\\theta) &= \\nabla_{\\theta}\\sum_{\\tau}P(\\tau;\\theta)R(\\tau)\\\\\n",
    "&=\\sum_{\\tau}\\nabla_{\\theta}P(\\tau;\\theta)R(\\tau)\\\\\n",
    "&=\\sum_{\\tau}P(\\tau;\\theta)\\frac{\\nabla_{\\theta}P(\\tau;\\theta)}{P(\\tau;\\theta)}R(\\tau)\\\\\n",
    "&=\\sum_{\\tau}P(\\tau;\\theta)\\nabla{\\log P(\\tau;\\theta)}R(\\tau)\\\\\n",
    "&=\\sum_{\\tau}P(\\tau;\\theta)\\nabla\\left[\\log \\mu(s_{0})\\Pi_{t=0}^{H}P(s_{t+1}|s_{t}, a_{t})\\pi_{\\theta}(a_{t}|s_{t})\\right]R(\\tau)\\\\\n",
    "&=\\sum_{\\tau}P(\\tau;\\theta)\\sum_{t=0}^{H}\\nabla \\pi_{\\theta}(a_{t}|s_{t})R(\\tau)\\\\\n",
    "&=\\mathbb{E}_{\\pi_{\\theta}}\\left[\\nabla_{\\theta}\\log \\pi_{\\theta}(a_{t}|s_{t})R(\\tau)\\right]\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded50b16-fe51-4e2c-8041-f26352f28dd5",
   "metadata": {},
   "source": [
    "## The Reinforce algorithm (Monte Carlo Reinforce)\n",
    "\n",
    "In a loop:\n",
    "\n",
    "* Use the policy $\\pi_{\\theta}$ to collect some episodes\n",
    "\n",
    "* Use these episodes to estimate the gradient.\n",
    "\n",
    "![](images/policy5.png)\n",
    "\n",
    "We can interpret this update as follows: $-\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}|s_{t})$ is the direction of steepest increase of the (log) probability of selecting action at from state $s_{t}$. This tells us:\n",
    "\n",
    "* If the return $R(\\tau)$ is high, it will push up the probabilities of the (state, action) combinations.\n",
    "\n",
    "* Otherwise, it will push down the probabilities of the (state, action) combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1dc286-28d1-465a-918f-df32a95e0c96",
   "metadata": {},
   "source": [
    "## Don’t Let the Past Distract You\n",
    "\n",
    "Examine our most recent expression for the policy gradient:\n",
    "\n",
    "$$\\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{\\tau\\sim\\theta}\\left[\\sum_{t=0}^{T}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}|s_{t})R(\\tau)\\right]$$\n",
    "\n",
    "Taking a step with this gradient pushes up the log-probabilities of each action in proportion to $R(\\tau)$, the sum of all rewards ever obtained. But this doesn’t make much sense.\n",
    "\n",
    "Agents should really only reinforce actions on the basis of their consequences. Rewards obtained before taking an action have no bearing on how good that action was: only rewards that come after.\n",
    "\n",
    "It turns out that this intuition shows up in the math, and we can show that the policy gradient can also be expressed by\n",
    "\n",
    "$$\\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{\\tau\\sim\\theta}\\left[\\sum_{t=0}^{T}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}|s_{t})\\sum_{t'=t}^{T}R(s_{t'},a_{t'},s_{t'+1})\\right]$$\n",
    "\n",
    "In this form, actions are only reinforced based on rewards obtained after they are taken. We'll call this form the \"reward-to-go policy gradient\", it is a lower-variance estimate than the simplest policy gradient.\n",
    "\n",
    "The proof of this claim depends on the **EGLP lemma**: Suppose that $P_{\\theta}$ is a parameterized probability distribution over a random variable, $x$. Then:\n",
    "\n",
    "$$\\mathbb{E}_{x\\sim P_{\\theta}}\\left[\\nabla_{\\theta}\\log P_{\\theta}(x)\\right] = 0$$\n",
    "\n",
    "````{prf:proof}\n",
    "Recall that all probability distributions are normalized:\n",
    "\n",
    "```{math}\n",
    "\\int_{x}P_{\\theta}(x) = 1\n",
    "```\n",
    "\n",
    "Take the gradient of both sides of the normalization condition:\n",
    "\n",
    "```{math}\n",
    "\\nabla_{\\theta}\\int_{x}P_{\\theta}(x) = \\nabla_{\\theta}1 = 0\n",
    "```\n",
    "\n",
    "Use the log derivative trick to get:\n",
    "\n",
    "```{math}\n",
    "\\begin{aligned}\n",
    "0 &= \\nabla_{\\theta}\\int_{x}P_{\\theta}(x) \\\\\n",
    " &= \\int_{x}\\nabla_{\\theta}P_{\\theta}(x) \\\\\n",
    " &= \\int_{x}P_{\\theta}(x)\\nabla_{\\theta}\\log P_{\\theta}(x)\n",
    "\\end{aligned}\n",
    "```\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde2de81-70ac-4d39-ac6c-709661d3acbd",
   "metadata": {},
   "source": [
    "## Baselines in Policy Gradients\n",
    "\n",
    "An immediate consequence of the EGLP lemma is that for any function $b$ which only depends on state,\n",
    "\n",
    "$$\\mathbb{E}_{a_{t}\\sim\\pi_{\\theta}}\\left[\\nabla_{\\theta}\\log\\pi(a_t|s_t)b(s_t)\\right]$$\n",
    "\n",
    "This allows us to add or subtract any number of terms like this from our expression for the policy gradient, without changing it in expectation:\n",
    "\n",
    "$$\\nabla_{\\theta}J(\\theta) = \\mathbb{E}_{\\tau\\sim\\theta}\\left[\\sum_{t=0}^{T}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}|s_{t})\\left(\\sum_{t'=t}^{T}R(s_{t'},a_{t'},s_{t'+1}) - b(s_{t})\\right)\\right]$$\n",
    "\n",
    "Any function $b$ used in this way is called a baseline.\n",
    "\n",
    "The most common choice of baseline is the on-policy value function $V^{\\pi}(s_t)$. Recall that this is the average return an agent gets if it starts in state $s_t$ and then acts according to policy $\\pi$ for the rest of its life. Empirically, the choice $b(s_t) = V^{\\pi}(s_t)$ has the desirable effect of reducing variance in the sample estimate for the policy gradient. This results in faster and more stable policy learning.\n",
    "\n",
    "```{note}\n",
    "In practice, $V^{\\pi}(s_t)$ cannot be computed exactly, so it has to be approximated. This is usually done with a neural network, $V_{\\phi}(s_t)$, which is updated concurrently with the policy (so that the value network always approximates the value function of the most recent policy).\n",
    "\n",
    "The simplest method for learning $V_{\\phi}$, used in most implementations of policy optimization algorithms,  is to minimize a mean-squared-error objective:\n",
    "\n",
    "$$\\phi_{k} = \\arg\\underset{\\phi}{\\min}\\underset{s_t, \\hat{R}_{t}\\sim\\pi_{k}}{\\mathbb{E}}\\left[\\left(V_{\\phi}(s_t) - \\hat{R}_{t}\\right)^{2}\\right]$$\n",
    "\n",
    "where $\\pi_k$ is the policy at epoch $k$. This is done with one or more steps of gradient descent, starting from the previous value parameters $\\phi_{k-1}$.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd4b80-5139-41dc-ab12-b88e8b8f0010",
   "metadata": {},
   "source": [
    "## Other Forms of the Policy Gradient\n",
    "\n",
    "What we have seen so far is that the policy gradient has the general form\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\pi_{\\theta}) = \\underset{{\\tau \\sim \\pi_{\\theta}}}{\\mathbb{E}}\\left[{\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) \\Phi_t}\\right],$$\n",
    "\n",
    "where $\\Phi_t$ could be any of\n",
    "\n",
    "$$\\Phi_t = R(\\tau),$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\\Phi_t = \\sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1}),$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\Phi_t = \\sum_{t'=t}^T R(s_{t'}, a_{t'}, s_{t'+1}) - b(s_t).$$\n",
    "\n",
    "All of these choices lead to the same expected value for the policy gradient, despite having different variances. It turns out that there are two more valid choices of weights $\\Phi_t$ which are important to know.\n",
    "\n",
    "1. On-Policy Action-Value Function. The choice\n",
    "\n",
    "    $$\\Phi_t = Q^{\\pi_{\\theta}}(s_t, a_t)$$\n",
    "\n",
    "    is also valid.\n",
    "\n",
    "2. The Advantage Function defined by $A^{\\pi}(s_t,a_t) = Q^{\\pi}(s_t,a_t) - V^{\\pi}(s_t)$, describes how much better or worse it is than other actions on average (relative to the current policy). This choice,\n",
    "\n",
    "    $$\\Phi_t = A^{\\pi_{\\theta}}(s_t, a_t)$$\n",
    "\n",
    "    is also valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c175ce-3f3d-42b4-9602-5b52ba714720",
   "metadata": {},
   "source": [
    "## Pytorch example\n",
    "\n",
    "### Policy network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56baefdc-2e18-4d23-bd5d-69c402d0f69b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"MLP\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "        self.dropout = nn.Dropout(p=0.6)\n",
    "        self.affine2 = nn.Linear(128, 2)\n",
    "        \n",
    "        self.saved_log_probs = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.affine1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(x)\n",
    "        action_scores = self.affine2(x)\n",
    "        return F.softmax(action_scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00a45c45-ad51-4d9b-9e72-a3b8bc3adcc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fbb192-4376-4afb-8411-49bd9921d1f5",
   "metadata": {},
   "source": [
    "*`self.saved_log_probs` saves $\\left[\\pi_{\\theta}(a_{0}|s_{0}), \\pi_{\\theta}(a_{1}|s_{1}),\\dots,\\pi_{\\theta}(a_{T}|s_{T})\\right]$\n",
    "\n",
    "*`self.rewards` saves $\\left[r_{0},r_{1},\\dots,r_{T}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ddba04-da7e-4a11-9c49-1ef6a727cbe5",
   "metadata": {},
   "source": [
    "### Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92613ed1-e496-4778-9305-cd7203de758d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.00118216,  0.04504637, -0.03558404,  0.04486495], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset(seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ff5b1-b84f-4e41-8625-e6228aedda58",
   "metadata": {},
   "source": [
    "### One episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fcf4c5e-1c79-451b-bd89-6286b0e08682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    probs = policy(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    policy.saved_log_probs.append(m.log_prob(action))\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7fbab79-2848-4c4e-a4a3-0f1c6ec80cf3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def finish_episode():\n",
    "    R = 0\n",
    "    eps = np.finfo(np.float32).eps.item()\n",
    "    policy_loss = []\n",
    "    returns = deque()\n",
    "    for r in policy.rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.appendleft(R)\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "    \n",
    "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
    "        policy_loss.append(-log_prob * R)\n",
    "    optimizer.zero_grad()\n",
    "    policy_loss = torch.cat(policy_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    optimizer.step()\n",
    "    del policy.rewards[:]\n",
    "    del policy.saved_log_probs[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34264a19-b89f-4ac7-afad-1e7eabd5a6c3",
   "metadata": {},
   "source": [
    "`returns` = $\\left[R_{0}, R_{1}, \\dots, R_{T}\\right]$ where $R_{t} = r_{t} + \\gamma r_{t+1} + \\gamma^{2}r_{t+2} + \\dots$\n",
    "\n",
    "`policy_loss` = $\\sum_{i=0}^{T}-\\log\\pi_{\\theta}(a_{t}|s_{t})R_{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f637a886-e987-4948-bf61-f64d2eb7c0bd",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "345eea62-3dd6-4744-8df2-bb41e7810541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "from collections import deque\n",
    "\n",
    "def main():\n",
    "    running_reward = 10\n",
    "    for i_episode in count(1):\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "        for t in range(1, 10000):  # Don't infinite loop while learning\n",
    "            action = select_action(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            policy.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        finish_episode()\n",
    "        if i_episode % 10 == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c15b15e3-7d30-4241-89d4-076561a3a774",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xiayunhui/anaconda3/lib/python3.11/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tLast reward: 45.00\tAverage reward: 15.06\n",
      "Episode 20\tLast reward: 35.00\tAverage reward: 27.11\n",
      "Episode 30\tLast reward: 98.00\tAverage reward: 40.73\n",
      "Episode 40\tLast reward: 30.00\tAverage reward: 42.89\n",
      "Episode 50\tLast reward: 39.00\tAverage reward: 50.75\n",
      "Episode 60\tLast reward: 92.00\tAverage reward: 66.16\n",
      "Episode 70\tLast reward: 331.00\tAverage reward: 107.18\n",
      "Episode 80\tLast reward: 103.00\tAverage reward: 154.36\n",
      "Episode 90\tLast reward: 48.00\tAverage reward: 113.81\n",
      "Episode 100\tLast reward: 96.00\tAverage reward: 99.90\n",
      "Episode 110\tLast reward: 172.00\tAverage reward: 155.10\n",
      "Episode 120\tLast reward: 239.00\tAverage reward: 206.40\n",
      "Episode 130\tLast reward: 196.00\tAverage reward: 224.19\n",
      "Episode 140\tLast reward: 194.00\tAverage reward: 251.26\n",
      "Episode 150\tLast reward: 372.00\tAverage reward: 239.45\n",
      "Episode 160\tLast reward: 305.00\tAverage reward: 362.52\n",
      "Solved! Running reward is now 649.4581415449471 and the last episode runs to 5964 time steps!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083388eb-868c-4263-ab84-7e1218caa433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
