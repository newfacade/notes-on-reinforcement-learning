{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1b862a-401f-4ffa-a0da-1fbf1489ad42",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "```{note}\n",
    "The idea that we learn by interacting with our environment is probably the first to occur to us when we think about the nature of learning. When an infant plays, waves its arms, or looks about, it has no explicit teacher, but it does have a direct sensorimotor connection to its environment. Exercising this connection produces a wealth of information about cause and effect, about the consequences of actions, and about what to do in order to achieve goals. Throughout our lives, such interactions are undoubtedly a major source of knowledge about our environment and ourselves.<br>\n",
    "In this book we explore a computational approach, called reinforcement learning, to learning from interaction. Rather than directly theorizing about how people or animals learn, we primarily explore idealized learning situations and evaluate the effectiveness of various learning methods.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6fd671-beaf-438e-a912-ded7a30cb918",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them.\n",
    "\n",
    "![](images/agent-env.svg)\n",
    "\n",
    "The main characters of RL are the **agent** (the learner) and the **environment**. The environment is the world that the agent lives in and interacts with. At every step of interaction, the agent sees a (possibly partial) observation of the state of the world, and then decides on an action to take. The environment changes when the agent acts on it, but may also change on its own.\n",
    "\n",
    "A **reward** signal defines the goal of a reinforcement learning problem. On each time step, the environment sends to the reinforcement learning agent a single number called the reward. The agent’s sole objective is to maximize the total reward it receives over the long run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc3bfbc-7988-4156-be9a-afdcc42ec5e4",
   "metadata": {},
   "source": [
    "### States and Observations\n",
    "\n",
    "A **state** $s$ is a complete description of the state of the world. There is no information about the world which is hidden from the state. An **observation** $o$ is a partial description of a state, which may omit information.\n",
    "\n",
    "When the agent is able to observe the complete state of the environment, we say that the environment is **fully observed**. When the agent can only see a partial observation, we say that the environment is **partially observed**.\n",
    "\n",
    "```{caution}\n",
    "Reinforcement learning notation sometimes puts the symbol for state, $s$, in places where it would be technically more appropriate to write the symbol for observation, $o$. Specifically, this happens when talking about how the agent decides an action: we often signal in notation that the action is conditioned on the state, when in practice, the action is conditioned on the observation because the agent does not have access to the state.<br>\n",
    "We’ll follow standard conventions for notation, but it should be clear from context which is meant.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985cc46-d250-4d12-bda0-5b933006c979",
   "metadata": {},
   "source": [
    "### Action Spaces\n",
    "\n",
    "Different environments allow different kinds of actions. The set of all valid actions in a given environment is often called the **action space**. Some environments, like Atari and Go, have **discrete action spaces**, where only a finite number of moves are available to the agent. Other environments, like where the agent controls a robot in a physical world, have **continuous action spaces**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5553d1b-1160-4db1-aa5d-d3881735c4ee",
   "metadata": {},
   "source": [
    "### Policies\n",
    "\n",
    "A **policy** defines the learning agent’s way of behaving at a given time. Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states. The policy is the core of a reinforcement learning agent in the sense that it alone is sufficient to determine behavior. In general, policies can be deterministic or stochastic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c06a2a6-ce8a-4f74-82a4-92d3fdc8eb4c",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "A good way to understand reinforcement learning is to consider some of the examples\n",
    "and possible applications that have guided its development.\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8852ce79-b4c0-4043-b4c6-3d289c307401",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
