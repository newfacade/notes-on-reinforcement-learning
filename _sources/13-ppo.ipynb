{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd97f284-a728-4e10-bb3c-3b509c8c597d",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization\n",
    "\n",
    "```{note}\n",
    "PPO is motivated by the same question as TRPO: how can we take the biggest possible improvement step on a policy using the data we currently have, without stepping so far that we accidentally cause performance collapse? Where TRPO tries to solve this problem with a complex second-order method, PPO is a family of first-order methods that use a few other tricks to keep new policies close to old. PPO methods are significantly simpler to implement, and empirically seem to perform at least as well as TRPO.\n",
    "```\n",
    "\n",
    "There are two primary variants of PPO: PPO-Penalty and PPO-Clip.\n",
    "\n",
    "**PPO-Penalty** approximately solves a KL-constrained update like TRPO, but penalizes the KL-divergence in the objective function instead of making it a hard constraint, and automatically adjusts the penalty coefficient over the course of training so that it’s scaled appropriately.\n",
    "\n",
    "**PPO-Clip** doesn’t have a KL-divergence term in the objective and doesn’t have a constraint at all. Instead relies on specialized clipping in the objective function to remove incentives for the new policy to get far from the old policy.\n",
    "\n",
    "Here, we’ll focus only on PPO-Clip (the primary variant used at OpenAI)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fa2e25-2d47-4cd8-8c3e-9015bb9670e5",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "PPO-clip updates policies via\n",
    "\n",
    "$$\n",
    "\\theta_{k+1} = \\arg \\max_{\\theta} \\underset{s,a \\sim \\pi_{\\theta_k}}{{\\mathbb{E}}}\\left[\n",
    "    L(s,a,\\theta_k, \\theta)\\right],\n",
    "$$\n",
    "\n",
    "typically taking multiple steps of (usually minibatch) SGD to maximize the objective. Here $L$ is given by\n",
    "\n",
    "$$\n",
    "L(s,a,\\theta_k,\\theta) = \\min\\left(\n",
    "\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}  A^{\\pi_{\\theta_k}}(s,a), \\;\\;\n",
    "\\text{clip}\\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}, 1 - \\epsilon, 1+\\epsilon \\right) A^{\\pi_{\\theta_k}}(s,a)\n",
    "\\right),\n",
    "$$\n",
    "\n",
    "in which $\\epsilon$ is a (small) hyperparameter which roughly says how far away the new policy is allowed to go from the old.\n",
    "\n",
    "What we have seen so far is that clipping serves as a regularizer by removing incentives for the policy to change dramatically, and the hyperparameter $\\epsilon$ corresponds to how far away the new policy can go from the old while still profiting the objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f10cc5-aeaf-4da9-9e75-3f104a81ef40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
