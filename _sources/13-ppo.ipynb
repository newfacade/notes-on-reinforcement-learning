{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "455d0db4-7031-44ec-bb4a-33d05a0c2a2d",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization\n",
    "\n",
    "```{note}\n",
    "In the previous section, we learned about Advantage Actor Critic (A2C), a hybrid architecture combining value-based and policy-based methods.<br>\n",
    "Today we’ll learn about Proximal Policy Optimization (PPO), an architecture that improves our agent’s training stability by avoiding policy updates that are too large. To do that, we use a ratio that indicates the difference between our current and old policy and clip this ratio to a specific range $[1-\\epsilon, 1+\\epsilon]$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e11409e-8f53-4ba4-9629-6cd0c5da1244",
   "metadata": {},
   "source": [
    "## Introducing the Clipped Surrogate Objective Function\n",
    "\n",
    "### Recap: The Policy Objective Function\n",
    "\n",
    "Let’s remember what the objective is to optimize in Reinforce:\n",
    "\n",
    "![](images/ppo1.jpeg)\n",
    "\n",
    "The idea was that by taking a gradient ascent step on this function, we would push our agent to take actions that lead to higher rewards and avoid harmful actions. \n",
    "\n",
    "However, the problem comes from the step size:\n",
    "\n",
    "* Too small, the training process was too slow\n",
    "\n",
    "* Too high, there was too much variability in the training\n",
    "\n",
    "With PPO, the idea is to constrain our policy update with a new objective function called the Clipped surrogate objective function that will constrain the policy change in a small range using a clip:\n",
    "\n",
    "![](images/ppo2.jpeg)\n",
    "\n",
    "Let’s study each part to understand how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdfaf20-20b4-4254-9d4a-6f836ebfbbd3",
   "metadata": {},
   "source": [
    "### The Ratio Function\n",
    "\n",
    "The ratio function is calculated as follows:\n",
    "\n",
    "![](images/ppo3.jpeg)\n",
    "\n",
    "As we can see, $r_{t}(\\theta)$ denotes the probability ratio between the current and old policy:\n",
    "\n",
    "* If $r_{t}(\\theta) > 1$, the action $a_{t}$ and state $s_{t}$ is more likely in the current policy than the old policy.\n",
    "\n",
    "* If $r_{t}(\\theta) < 1$, the action is less likely for the current policy than for the old one.\n",
    "\n",
    "So this probability ratio is an easy way to estimate the divergence between old and current policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a193b6e2-68d3-4cd7-911a-ba3b8e83fa00",
   "metadata": {},
   "source": [
    "### The unclipped part of the Clipped Surrogate Objective function\n",
    "\n",
    "![](images/ppo4.jpeg)\n",
    "\n",
    "This ratio can replace the log probability we use in the policy objective function. This gives us the left part of the new objective function: multiplying the ratio by the advantage.\n",
    "\n",
    "![](images/ppo5.jpeg)\n",
    "\n",
    "However, without a constraint, if the action taken is much more probable in our current policy than in our former, this would lead to a significant policy gradient step and, therefore, an excessive policy update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d37886-c84f-4cba-9693-a77e1771e969",
   "metadata": {},
   "source": [
    "### The clipped Part of the Clipped Surrogate Objective function\n",
    "\n",
    "Consequently, we need to constrain this objective function by penalizing changes that lead to a ratio far away from 1.\n",
    "\n",
    "By clipping the ratio, we ensure that we do not have a too large policy update because the current policy can’t be too different from the older one. To do that, PPO clip probability ratio directly in the objective function with its Clipped surrogate objective function.\n",
    "\n",
    "![](images/ppo6.jpeg)\n",
    "\n",
    "With the Clipped Surrogate Objective function, we have two probability ratios, one non-clipped and one clipped in a range between $[1-\\epsilon, 1 + \\epsilon]$.\n",
    "\n",
    "Taking the minimum of the clipped and non-clipped objective means we’ll select either the clipped or the non-clipped objective based on the ratio and advantage situation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205108ee-f3c7-42c3-948e-4eb985ba10d0",
   "metadata": {},
   "source": [
    "## PPO-Clip\n",
    "\n",
    "PPO-clip updates policies via\n",
    "\n",
    "$$\n",
    "\\theta_{k+1} = \\arg \\max_{\\theta} \\underset{s,a \\sim \\pi_{\\theta_k}}{{\\mathbb{E}}}\\left[\n",
    "    L(s,a,\\theta_k, \\theta)\\right],\n",
    "$$\n",
    "\n",
    "typically taking multiple steps of (usually minibatch) SGD to maximize the objective. Here $L$ is given by\n",
    "\n",
    "$$\n",
    "L(s,a,\\theta_k,\\theta) = \\min\\left(\n",
    "\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}  A^{\\pi_{\\theta_k}}(s,a), \\;\\;\n",
    "\\text{clip}\\left(\\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)}, 1 - \\epsilon, 1+\\epsilon \\right) A^{\\pi_{\\theta_k}}(s,a)\n",
    "\\right),\n",
    "$$\n",
    "\n",
    "in which $\\epsilon$ is a (small) hyperparameter which roughly says how far away the new policy is allowed to go from the old.\n",
    "\n",
    "What we have seen so far is that clipping serves as a regularizer by removing incentives for the policy to change dramatically, and the hyperparameter $\\epsilon$ corresponds to how far away the new policy can go from the old while still profiting the objective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea60bbfb-46a6-4964-8608-689488d541be",
   "metadata": {},
   "source": [
    "## PPO-Penalty\n",
    "\n",
    "Another approach, which can be used as an alternative to the clipped surrogate objective, or in\n",
    "addition to it, is to use a penalty on KL divergence, and to adapt the penalty coefficient so that we\n",
    "achieve some target value of the KL divergence $d_{\\text{targ}}$ each policy update.\n",
    "\n",
    "In the simplest instantiation of this algorithm, we perform the following steps in each policy\n",
    "update:\n",
    "\n",
    "* Using several epochs of minibatch SGD, optimize the KL-penalized objective\n",
    "\n",
    "$$L^{KLPEN}(\\theta) = \\hat{\\mathbb{E}}_{t}\\left[\\frac{\\pi_{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{\\text{old}}}(a_{t}|s_{t})}\\hat{A}_{t} -\\beta\\text{KL}\\left[\\pi_{\\theta_{\\text{old}}}(\\cdot|s_{t}), \\pi_{\\theta}(\\cdot|s_{t})\\right]\\right]$$\n",
    "\n",
    "* Compute $d=\\hat{\\mathbb{E}}_{t}\\left[\\text{KL}\\left[\\pi_{\\theta_{\\text{old}}}(\\cdot|s_{t}), \\pi_{\\theta}(\\cdot|s_{t})\\right]\\right]$\n",
    "\n",
    "    - If $d<d_{\\text{targ}}/1.5,\\beta\\gets\\beta/2$\n",
    "    - If $d>d_{\\text{targ}}\\times 1.5,\\beta\\gets\\beta\\times 2$\n",
    "    \n",
    "The updated $\\beta$ is used for the next policy update.\n",
    "\n",
    "```{tip}\n",
    "The Kullback–Leibler (KL) divergence between $P$ and $Q$ is:\n",
    "\n",
    "$$\\text{KL}(P, Q) = \\mathbb{E}_{x\\sim P}\\left[\\log\\frac{p(x)}{q(x)}\\right].$$\n",
    "\n",
    "It is a way to measure if two distributions are close together or not (the smaller the closer).\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2256b0-e40b-4845-8e7f-e99a3753397a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
