{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee51ea1-24ef-40a5-b4ee-9a3fb1e7da21",
   "metadata": {},
   "source": [
    "# Generalized Advantage Estimation\n",
    "\n",
    "```{note}\n",
    "The two\n",
    "main challenges of policy gradient methods are the large number of samples typically required, and the difficulty\n",
    "of obtaining stable and steady improvement despite the nonstationarity of the\n",
    "incoming data.<br>\n",
    "We address the first challenge by using value functions to substantially\n",
    "reduce the variance of policy gradient estimates at the cost of some bias. We address the second challenge by using trust region optimization\n",
    "procedure for both the policy and the value function, which are represented by\n",
    "neural networks.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35efc8b9-54b7-403c-a8da-a1ff97bab911",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "There are several different related expressions for the policy gradient, which\n",
    "have the form\n",
    "\n",
    "$$g = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty}\\Psi_{t}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_t|s_t)\\right]$$\n",
    "\n",
    "where $\\Psi_{t}$ may be one of the following:\n",
    "\n",
    "1. $\\sum_{t=0}^{\\infty}r_t$: total reward of the trajectory.\n",
    "2. $\\sum_{t'=t}^{\\infty}r_{t'}$: reward following action $a_t$.\n",
    "3. $\\sum_{t'=t}^{\\infty}r_{t'} - b(s_t)$: baselined version of\n",
    "previous formula.\n",
    "4. $Q_{\\pi}(s_t, a_t)$: state-action value function.\n",
    "5. $A_{\\pi}(s_t, a_t)$: advantage function.\n",
    "6. $r_{t} + V_{\\pi}(s_{t+1}) - V_{\\pi}(s_t)$: TD residual.\n",
    "\n",
    "The choice $\\Psi_{t} = A_{\\pi}(s_t, a_t)$ yields almost the lowest possible variance, though in practice, the\n",
    "advantage function is not known and must be estimated.\n",
    "\n",
    "We will introduce a parameter $\\gamma$ that allows us to reduce variance by downweighting rewards corresponding\n",
    "to delayed effects, at the cost of introducing bias. This parameter corresponds to the\n",
    "discount factor used in discounted formulations of MDPs, , but we treat it as a variance reduction\n",
    "parameter in an undiscounted problem.\n",
    "\n",
    "$$V_{\\pi,\\gamma}(s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t}:\\infty}\\left[\\sum_{l=0}^{\\infty}\\gamma^{l}r_{t+l}\\right]$$\n",
    "\n",
    "$$Q_{\\pi,\\gamma}(s_t) := \\mathbb{E}_{s_{t+1}:\\infty, a_{t+1}:\\infty}\\left[\\sum_{l=0}^{\\infty}\\gamma^{l}r_{t+l}\\right]$$\n",
    "\n",
    "$$A_{\\pi,\\gamma}(s_t) := Q_{\\pi,\\gamma}(s_t) -  V_{\\pi,\\gamma}(s_t)$$\n",
    "\n",
    "The colon notation $a: b$ refers to the inclusive range $(a, a+1,\\dots, b)$. The discounted approximation to the policy gradient is defined as follows:\n",
    "\n",
    "$$g^{\\gamma} := \\mathbb{E}_{s_0:\\infty,a_0:\\infty}\\left[\\sum_{t=0}^{\\infty}A_{\\pi,\\gamma}(a_t, s_t)\\nabla_{\\theta}\\log\\pi_{\\theta}(a_t|s_t)\\right]$$\n",
    "\n",
    "Before proceeding, we will introduce the notion of a $\\gamma-$just estimator of the advantage function,\n",
    "which is an estimator that does not introduce bias when we use it in place of $A^{\\pi,\\gamma}$.\n",
    "\n",
    "**Definition 1.** The estimator $\\hat{A}_{t}$ is $\\gamma-$just if\n",
    "\n",
    "$$\\mathbb{E}_{s_0:\\infty,a_0:\\infty}\\left[\\hat{A}_{t}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_t|s_t)\\right]=\\mathbb{E}_{s_0:\\infty,a_0:\\infty}\\left[A_{\\pi,\\gamma}(a_t, s_t)\\nabla_{\\theta}\\log\\pi_{\\theta}(a_t|s_t)\\right] = g^{\\gamma}$$\n",
    "\n",
    "We can verify that the following expressions are \n",
    "-just\n",
    "advantage estimators for $\\hat{A}_t$:\n",
    "\n",
    "* $\\sum_{l=0}^{\\infty}\\gamma^{l}r_{t+l}$\n",
    "* $Q_{\\pi,\\gamma}(s_t, a_t)$\n",
    "* $A_{\\pi,\\gamma}(s_t, a_t)$\n",
    "* $r_t + \\gamma V_{\\pi,\\gamma}(s_{t+1}) - V_{\\pi,\\gamma}(s_{t})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da6b40a-b1ee-402e-abe3-19c2ea17d2c2",
   "metadata": {},
   "source": [
    "## Advantage function estimation\n",
    "\n",
    "This section will be concerned with producing an accurate estimate $\\hat{A}_{t}$ which will then be used to construct a policy gradient estimator of the\n",
    "following form:\n",
    "\n",
    "$$\\hat{g} = \\frac{1}{N}\\sum_{n=1}^{N}\\sum_{t=0}^{T}\\hat{A}_{t}^{n}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}^{n}|s_{t}^{n})$$\n",
    "\n",
    "where $n$ indexes over a batch of episodes.\n",
    "\n",
    "Let $V$ be an approximate value function. Define \n",
    "\n",
    "$$\\delta_{t}^{V} = r_t + \\gamma V(s_{t+1}) - V(s_t)$$\n",
    "\n",
    "i.e., the TD residual. $\\delta_{t}^{V}$ can be considered as an estimate of the\n",
    "advantage of the action $a_t$. In fact, if we have the correct value function $V=V_{\\pi,\\gamma}$, then it is a $\\gamma$-just\n",
    "advantage estimator. However, this estimator is only $\\gamma$-just for $V=V_{\\pi,\\gamma}$, otherwise it will yield biased policy gradient\n",
    "estimates.\n",
    "\n",
    "Next, let:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{A}_{t}^{(1)} &:= \\delta_{t}^{V} &&=-V(s_t) + r_t + \\gamma V(s_{t+1})\\\\\n",
    "\\hat{A}_{t}^{(2)} &:= \\delta_{t}^{V} + \\gamma\\delta_{t+1}^{V} &&=-V(s_t) + r_t + \\gamma r_{t+1} + \\gamma^{2} V(s_{t+2})\\\\\n",
    "\\hat{A}_{t}^{(k)} &:= \\sum_{l=0}^{k-1}\\gamma^{l}\\delta_{t+l}^{V} &&=-V(s_t) + r_t + \\gamma r_{t+1} + \\dots + \\gamma^{k-1}r_{t+k-1} + \\gamma^{k} V(s_{t+k})\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We can consider $\\hat{A}_{t}^{(k)}$ to be an estimator of the advantage function, which is only $\\gamma$-just when $V=V_{\\pi,\\gamma}$. However,\n",
    "note that the bias generally becomes smaller as $k\\to\\infty$, since the term $\\gamma^{k}V(s_{t+k})$ becomes more\n",
    "heavily discounted, and the term $-V(s_t)$ does not affect the bias.\n",
    "\n",
    "The generalized advantage estimator $\\text{GAE}(\\gamma, \\lambda)$ is defined as the exponentially-weighted average\n",
    "of these $k$-step estimators:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{A}_{t}^{\\text{GAE}(\\gamma, \\lambda)} &:= (1 - \\lambda)\\left(\\hat{A}_{t}^{(1)} + \\lambda\\hat{A}_{t}^{(2)} + \\lambda^{2}\\hat{A}_{t}^{(3)} + \\dots\\right) \\\\\n",
    "&= \\sum_{l=0}^{\\infty}(\\gamma\\lambda)^{l}\\delta_{t+l}^{V}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The construction we used above is closely\n",
    "analogous to the one used to define $\\text{TD}(\\lambda)$, however $\\text{TD}(\\lambda)$ is an estimator\n",
    "of the value function, whereas here we are estimating the advantage function.\n",
    "\n",
    "There are two notable special cases of this formula, obtained by setting $\\lambda=0$ and $\\lambda=1$.\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{GAE}(\\gamma, 0):\\hat{A}_{t} = \\delta_{t} &&= r_t + \\gamma V(s_{t+1}) - V(s_t) \\\\\n",
    "&\\text{GAE}(\\gamma, 1):\\hat{A}_{t} = \\sum_{l=0}\\gamma^{l}\\delta_{t+l} &&= \\sum_{l=0}^{\\infty}\\gamma^{l}r_{t+l} - V(s_t)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "```{tip}\n",
    "$\\text{GAE}(\\gamma, 1)$ is $\\gamma-$just regardless of the accuracy of $V$, but it has high variance due to the sum of\n",
    "terms. $\\text{GAE}(\\gamma, 0)$ is $\\gamma-$just for $V=V_{\\pi,\\gamma}$ and otherwise induces bias, but it typically has much\n",
    "lower variance. The generalized advantage estimator for $0<\\lambda<1$ makes a compromise between\n",
    "bias and variance, controlled by parameter $\\lambda$.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f6cab-5717-4a4b-b957-2848118dbe46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
