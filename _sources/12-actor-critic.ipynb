{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd36b119-19a6-4615-ad6a-92fb09c3d003",
   "metadata": {},
   "source": [
    "# Actor-Critic Methods\n",
    "\n",
    "```{note}\n",
    "In Policy-Based methods, we aim to optimize the policy directly without using a value function. We saw that Reinforce worked well. However, because we use Monte-Carlo sampling to estimate return, we have significant variance in policy gradient estimation.<br>\n",
    "So today we’ll study Actor-Critic methods, a hybrid architecture combining value-based and Policy-Based methods that helps to stabilize the training by reducing the variance using:<br>\n",
    "* An Actor that controls how our agent behaves (Policy-Based method)<br>\n",
    "* A Critic that measures how good the taken action is (Value-Based method)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9e04e6-9bf1-452a-9829-dbe34e11ff03",
   "metadata": {},
   "source": [
    "## The Problem of Variance in Reinforce\n",
    "\n",
    "In Reinforce, we want to increase the probability of actions in a trajectory proportionally to how high the return is.\n",
    "\n",
    "![](images/actor1.jpeg)\n",
    "\n",
    "This return $R(\\tau)$ is calculated using a Monte-Carlo sampling. We collect a trajectory and calculate the discounted return, and use this score to increase or decrease the probability of every action taken in that trajectory.\n",
    "\n",
    "The advantage of this method is that it’s unbiased. Since we’re not estimating the return, we use only the true return we obtain.\n",
    "\n",
    "Given the stochasticity of the environment and stochasticity of the policy, trajectories can lead to different returns, which can lead to high variance. Consequently, the same starting state can lead to very different returns. Because of this, the return starting at the same state can vary significantly across episodes.\n",
    "\n",
    "![](images/actor2.jpeg)\n",
    "\n",
    "The solution is to mitigate the variance by using a large number of trajectories. However, increasing the batch size significantly reduces sample efficiency. So we need to find additional mechanisms to reduce the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdebf808-564f-4f87-97f9-ede4c6d1fe2b",
   "metadata": {},
   "source": [
    "## Advantage Actor-Critic (A2C)\n",
    "\n",
    "### Reducing variance with Actor-Critic methods\n",
    "\n",
    "The solution to reducing the variance of the Reinforce algorithm and training our agent faster and better is to use a combination of Policy-Based and Value-Based methods: the Actor-Critic method.\n",
    "\n",
    "To understand the Actor-Critic, imagine you’re playing a video game. You can play with a friend that will provide you with some feedback. You’re the Actor and your friend is the Critic.\n",
    "\n",
    "![](images/ac3.jpeg)\n",
    "\n",
    "You don’t know how to play at the beginning, so you try some actions randomly. The Critic observes your action and provides feedback. Learning from this feedback, you’ll update your policy and be better at playing that game. On the other hand, your friend (Critic) will also update their way to provide feedback so it can be better next time.\n",
    "\n",
    "This is the idea behind Actor-Critic. We learn two function approximations:\n",
    "\n",
    "* A policy that controls how our agent acts: $\\pi_{\\theta}(s)$\n",
    "\n",
    "* A value function to assist the policy update by measuring how good the action taken is: $\\hat{q}_{w}(s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6807afc3-2c7b-41bf-9ef8-6a2a0d47bfd5",
   "metadata": {},
   "source": [
    "### The Actor-Critic Process\n",
    "\n",
    "Let’s see the training process to understand how the Actor and Critic are optimized:\n",
    "\n",
    "* At each timestep $t$, we get the current state $S_{t}$ from the environment and pass it as input through our Actor and Critic.\n",
    "\n",
    "* Our Policy takes the state and outputs an action $A_{t}$.\n",
    "\n",
    "* The Critic takes that action also as input and, using $S_{t}$ and $A_{t}$, computes the value of taking that action at that state: the Q-value.\n",
    "\n",
    "* The action $A_{t}$ performed in the environment outputs a new state $S_{t+1}$ and a reward $R_{t+1}$.\n",
    "\n",
    "* The Actor updates its policy parameters using the Q value.\n",
    "\n",
    "![](images/ac4.jpeg)\n",
    "\n",
    "* Thanks to its updated parameters, the Actor produces the next action to take at $A_{t+1}$ given the new state $S_{t+1}$.\n",
    "\n",
    "* The Critic then updates its value parameters.\n",
    "\n",
    "$$\\Delta{w} = \\beta(r_{t+1} + \\gamma\\hat{q}_{w}(s_{t+1}, a_{t+1}) - \\hat{q}_{w}(s_{t}, a_{t}))\\nabla_{w}\\hat{q}_{w}(s_{t}, a_{t})$$\n",
    "\n",
    "```{tip}\n",
    "If we fix the TD target.\n",
    "\n",
    "$$\\Delta{w} = \\frac{\\beta}{2}\\nabla_{w}(r_{t+1} + \\gamma\\hat{q}_{w}(s_{t+1}, a_{t+1}) - \\hat{q}_{w}(s_{t}, a_{t}))^{2}$$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d32b19c-d9eb-4898-af02-400a594947e1",
   "metadata": {},
   "source": [
    "### Adding Advantage in Actor-Critic (A2C)\n",
    "\n",
    "We can stabilize learning further by using the Advantage function as Critic instead of the Action value function.\n",
    "\n",
    "The idea is that the Advantage function calculates the relative advantage of an action compared to the others possible at a state: how taking that action at a state is better compared to the average value of the state. It’s subtracting the mean value of the state from the state action pair:\n",
    "\n",
    "![](images/ac5.jpeg)\n",
    "\n",
    "The extra reward is what’s beyond the expected value of that state.\n",
    "\n",
    "* If $A(s, a) > 0$, our gradient is pushed in that direction.\n",
    "\n",
    "* If $A(s, a) < 0$, our gradient is pushed in the opposite direction.\n",
    "\n",
    "The problem with implementing this advantage function is that it requires two value functions - $Q(s, a)$ and $V(s)$, Fortunately, we can use the TD error as a good estimator of the advantage function.\n",
    "\n",
    "![](images/ac6.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d046a50b-a023-40bd-bc10-7c8ad750dd03",
   "metadata": {},
   "source": [
    "## Pytorch example\n",
    "\n",
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4398a796-50c9-43f8-9d09-903d8f4bbe02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    implements both actor and critic in one model\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(4, 128)\n",
    "\n",
    "        # actor's layer\n",
    "        self.action_head = nn.Linear(128, 2)\n",
    "\n",
    "        # critic's layer\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "        # action & reward buffer\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward of both actor and critic\n",
    "        \"\"\"\n",
    "        x = F.relu(self.affine1(x))\n",
    "\n",
    "        # actor: choses action to take from state s_t\n",
    "        # by returning probability of each action\n",
    "        action_prob = F.softmax(self.action_head(x), dim=-1)\n",
    "\n",
    "        # critic: evaluates being in the state s_t\n",
    "        state_values = self.value_head(x)\n",
    "\n",
    "        # return values for both actor and critic as a tuple of 2 values:\n",
    "        # 1. a list with the probability of each action over the action space\n",
    "        # 2. the value from state s_t\n",
    "        return action_prob, state_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a752b3b5-08af-4559-8b47-634d8e520c00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Policy()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3038d384-f2c8-4cf0-96e5-a2031640c88d",
   "metadata": {},
   "source": [
    "### Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5f50c71-f7bd-40f8-8ffa-95620143fec9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.02383879, -0.02015088,  0.03142257, -0.04080841], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset(seed=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cae704c-00e5-4013-bc0e-d5565a2bdc71",
   "metadata": {},
   "source": [
    "### One episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "288f9df9-7f9f-4673-9427-9214c38a1ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs, state_value = model(state)\n",
    "\n",
    "    # create a categorical distribution over the list of probabilities of actions\n",
    "    m = Categorical(probs)\n",
    "\n",
    "    # and sample an action using the distribution\n",
    "    action = m.sample()\n",
    "\n",
    "    # save to action buffer\n",
    "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "\n",
    "    # the action to take (left or right)\n",
    "    return action.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d416e45c-5d61-42ae-9eaa-7a0d458b7941",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    \"\"\"\n",
    "    Training code. Calculates actor and critic loss and performs backprop.\n",
    "    \"\"\"\n",
    "    R = 0\n",
    "    eps = np.finfo(np.float32).eps.item()\n",
    "    saved_actions = model.saved_actions\n",
    "    policy_losses = [] # list to save actor (policy) loss\n",
    "    value_losses = [] # list to save critic (value) loss\n",
    "    returns = [] # list to save the true values\n",
    "\n",
    "    # calculate the true value using rewards returned from the environment\n",
    "    for r in model.rewards[::-1]:\n",
    "        # calculate the discounted value\n",
    "        R = r + 0.99 * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    for (log_prob, value), R in zip(saved_actions, returns):\n",
    "        advantage = R - value.item()\n",
    "\n",
    "        # calculate actor (policy) loss\n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "\n",
    "        # calculate critic (value) loss using L1 smooth loss\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # sum up all the values of policy_losses and value_losses\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "\n",
    "    # perform backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # reset rewards and action buffer\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc5cc65-101e-4014-932a-e8d6aeadda6c",
   "metadata": {},
   "source": [
    "`returns` = $\\left[R_{0}, R_{1}, \\dots, R_{T}\\right]$ where $R_{t} = r_{t} + \\gamma r_{t+1} + \\gamma^{2}r_{t+2} + \\dots$\n",
    "\n",
    "`policy_loss` = $\\sum_{i=0}^{T}-\\log\\pi_{\\theta}(a_{t}|s_{t})(R_{t} - \\hat{V}_{w}(s_{t}))$\n",
    "\n",
    "`value_loss` = $\\sum_{i=0}^{T}|R_{t} - \\hat{V}_{w}(s_{t})|$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76ff07e-96b8-440a-8db2-d5606dce08f5",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e965227-3b8c-4b68-ba63-3afb75a2e20c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "\n",
    "def main():\n",
    "    running_reward = 10\n",
    "\n",
    "    # run infinitely many episodes\n",
    "    for i_episode in count(1):\n",
    "\n",
    "        # reset environment and episode reward\n",
    "        state, _ = env.reset()\n",
    "        ep_reward = 0\n",
    "\n",
    "        # for each episode, only run 9999 steps so that we don't\n",
    "        # infinite loop while learning\n",
    "        for t in range(1, 10000):\n",
    "\n",
    "            # select action from policy\n",
    "            action = select_action(state)\n",
    "\n",
    "            # take the action\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "            model.rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # update cumulative reward\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # perform backprop\n",
    "        finish_episode()\n",
    "\n",
    "        # log results\n",
    "        if i_episode % 10 == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "\n",
    "        # check if we have \"solved\" the cart pole problem\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d33cd64-b50a-4c97-9e54-bb75ad88e502",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10\tLast reward: 12.00\tAverage reward: 13.41\n",
      "Episode 20\tLast reward: 10.00\tAverage reward: 11.82\n",
      "Episode 30\tLast reward: 10.00\tAverage reward: 11.44\n",
      "Episode 40\tLast reward: 10.00\tAverage reward: 12.48\n",
      "Episode 50\tLast reward: 29.00\tAverage reward: 13.10\n",
      "Episode 60\tLast reward: 22.00\tAverage reward: 14.30\n",
      "Episode 70\tLast reward: 42.00\tAverage reward: 20.13\n",
      "Episode 80\tLast reward: 87.00\tAverage reward: 64.19\n",
      "Episode 90\tLast reward: 130.00\tAverage reward: 71.48\n",
      "Episode 100\tLast reward: 1857.00\tAverage reward: 195.87\n",
      "Episode 110\tLast reward: 26.00\tAverage reward: 173.51\n",
      "Episode 120\tLast reward: 179.00\tAverage reward: 164.31\n",
      "Episode 130\tLast reward: 239.00\tAverage reward: 178.51\n",
      "Solved! Running reward is now 815.4286146729273 and the last episode runs to 9999 time steps!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
