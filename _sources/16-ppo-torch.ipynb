{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40bd4847-c88a-4174-9585-e5c53980dcb7",
   "metadata": {},
   "source": [
    "# PPO with CleanRL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c91af55-e629-41ce-a0cf-f75b0e940335",
   "metadata": {},
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6df39364-e864-4b2d-bc5a-96857b443a49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import gymnasium as gym\n",
    "\n",
    "def make_env(env_id):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "    \n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d285c532-937d-4ed6-95bd-08ba031c6ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncVectorEnv(3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "num_envs = 3\n",
    "seed = 1\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv([make_env(env_id) for _ in range(num_envs)])\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "envs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57c2a64-6fd7-44ae-a438-def1a95ecabd",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "956c3d19-2495-4b7f-937f-781fea7cf8dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dc7f87f-c5c9-46c3-9e02-2bb03bee55f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(envs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39b10fef-2df4-46f4-96c6-9003ed0c48f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 2.5e-4\n",
    "optimizer = optim.Adam(agent.parameters(), lr=learning_rate, eps=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3bece4-1127-4992-95a8-a4e260ad95ff",
   "metadata": {},
   "source": [
    "## Start the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9c73e57-97f2-48ca-97ec-9c62caeac693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the number of steps to run in each environment per policy rollout\n",
    "num_steps = 128\n",
    "\n",
    "# ALGO Logic: Storage setup\n",
    "obs = torch.zeros((num_steps, num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((num_steps, num_envs) + envs.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((num_steps, num_envs)).to(device)\n",
    "rewards = torch.zeros((num_steps, num_envs)).to(device)\n",
    "dones = torch.zeros((num_steps, num_envs)).to(device)\n",
    "values = torch.zeros((num_steps, num_envs)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81723ec7-8317-431e-816f-74143ba66369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start the game\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = envs.reset(seed=seed) # shape (num_envs, single_observation_space.shape)\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(num_envs).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae686aa-2379-406a-b09d-9a1f1035bd3c",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37587f13-7c07-4825-a95f-fc086f782af2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_timesteps = 20000\n",
    "batch_size = int(num_envs * num_steps)\n",
    "minibatch_size = int(batch_size // 4)\n",
    "num_iterations = total_timesteps // batch_size\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95  # Use GAE for advantage computation\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d59bd3d-ed17-4e98-8758-7aa7d58b59d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fill obs, actions, logprobs, rewards, dones, values\n",
    "\"\"\"\n",
    "for step in range(num_steps):\n",
    "    global_step += num_envs\n",
    "    # s_t\n",
    "    obs[step] = next_obs\n",
    "    dones[step] = next_done\n",
    "\n",
    "    # a_t, v_t\n",
    "    with torch.no_grad():\n",
    "        action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "        values[step] = value.flatten()\n",
    "    actions[step] = action\n",
    "    logprobs[step] = logprob\n",
    "\n",
    "    # s_{t+1}, r_t\n",
    "    next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "    next_done = np.logical_or(terminations, truncations)\n",
    "    rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "    next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bcdefba-dcab-413c-88f2-e83a87aa8ba4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "advantage computation\n",
    "\"\"\"\n",
    "with torch.no_grad():\n",
    "    # v_{T+1}\n",
    "    next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "    advantages = torch.zeros_like(rewards).to(device)\n",
    "    lastgaelam = 0\n",
    "    for t in reversed(range(num_steps)):\n",
    "        if t == num_steps - 1:\n",
    "            nextnonterminal = 1.0 - next_done\n",
    "            nextvalues = next_value\n",
    "        else:\n",
    "            nextnonterminal = 1.0 - dones[t + 1]\n",
    "            nextvalues = values[t + 1]\n",
    "        # r_t + gamma * v_{t+1} - v{t}\n",
    "        delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
    "        # update lastgaelam & compute advantages\n",
    "        advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "    # Q values\n",
    "    returns = advantages + values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3aa603e2-7cc3-4ce1-82ab-840961581971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# flatten the batch\n",
    "b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "b_logprobs = logprobs.reshape(-1)\n",
    "b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "b_advantages = advantages.reshape(-1)\n",
    "b_returns = returns.reshape(-1)\n",
    "b_values = values.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1038af9-7fba-401c-b495-afd173bdeb6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimizing the policy and value network\n",
    "update_epochs = 4\n",
    "clip_coef = 0.2\n",
    "ent_coef = 0.01\n",
    "vf_coef = 0.5\n",
    "max_grad_norm = 0.5\n",
    "b_inds = np.arange(batch_size)\n",
    "clipfracs = []\n",
    "for epoch in range(update_epochs):\n",
    "    np.random.shuffle(b_inds)\n",
    "    for start in range(0, batch_size, minibatch_size):\n",
    "        end = start + minibatch_size\n",
    "        mb_inds = b_inds[start:end]  # random timesteps\n",
    "\n",
    "        _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "        logratio = newlogprob - b_logprobs[mb_inds]\n",
    "        ratio = logratio.exp()\n",
    "        \n",
    "        mb_advantages = b_advantages[mb_inds]\n",
    "        \n",
    "        # Policy loss\n",
    "        pg_loss1 = -mb_advantages * ratio\n",
    "        pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "        pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "        \n",
    "        # Value loss\n",
    "        v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "        \n",
    "        entropy_loss = entropy.mean()\n",
    "        loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
