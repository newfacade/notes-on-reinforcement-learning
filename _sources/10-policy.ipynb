{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b38793c6-0a64-4049-8d15-1d4bd06a2e8b",
   "metadata": {},
   "source": [
    "# Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c994cc13-73e8-4f81-9d5c-bc5407e7ce24",
   "metadata": {},
   "source": [
    "## The Simplest Policy Gradient\n",
    "\n",
    "Here, we consider the case of a stochastic, parameterized policy, $\\pi_{\\theta}$. We aim to maximize the expected return $J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}[R(\\tau)]$. For the purposes of this derivation, we’ll take $R(\\tau)$ to give the finite-horizon undiscounted return, but the derivation for the infinite-horizon discounted return setting is almost identical.\n",
    "\n",
    "We would like to optimize the policy by gradient ascent:\n",
    "\n",
    "$$\\theta_{k+1} = \\theta_{k} + \\alpha\\nabla_{\\theta}J(\\pi_{\\theta})|_{\\theta_{k}}.$$\n",
    "\n",
    "The gradient of policy performance, $\\nabla_{\\theta}J(\\pi_{\\theta})$, is called the policy gradient, and algorithms that optimize the policy this way are called policy gradient algorithms.\n",
    "\n",
    "To actually use this algorithm, we need an expression for the policy gradient which we can numerically compute. This involves two steps:\n",
    "1. deriving the analytical gradient of policy performance, which turns out to have the form of an expected value.\n",
    "2. forming a sample estimate of that expected value, which can be computed with data from a finite number of agent-environment interaction steps.\n",
    "\n",
    "In this subsection, we’ll find the simplest form of that expression. In later subsections, we’ll show how to improve on the simplest form to get the version we actually use in standard policy gradient implementations.\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\nabla_{\\theta}J(\\pi_{\\theta}) &= \\nabla_{\\theta}\\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}[R(\\tau)]\\\\\n",
    "&= \\nabla_{\\theta}\\int_{\\tau}P(\\tau|\\theta)R(\\tau) \\\\\n",
    "&= \\int_{\\tau}\\nabla_{\\theta}P(\\tau|\\theta)R(\\tau) \\\\\n",
    "&= \\int_{\\tau}P(\\tau|\\theta)\\nabla_{\\theta}\\log P(\\tau|\\theta)R(\\tau)\\\\\n",
    "&= \\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}\\left[\\log P(\\tau|\\theta)R(\\tau)\\right]\\\\\n",
    "&= \\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}\\left[\\sum_{t=0}^{T}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}|s_{t})R(\\tau)\\right]\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "This is an expectation, which means that we can estimate it with a sample mean. If we collect a set of trajectories $\\mathcal{D} = \\{\\tau_{i}\\}_{i=1,\\dots,N}$ where each trajectory is obtained by letting the agent act in the environment using the policy $\\pi_{\\theta}$, the policy gradient can be estimated with\n",
    "\n",
    "$$\\hat{g} = \\frac{1}{|\\mathcal{D}|}\\sum_{\\tau\\in\\mathcal{D}}\\sum_{t=0}^{T}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}|s_{t})R(\\tau)$$\n",
    "\n",
    "This last expression is the simplest version of the computable expression we desired. Assuming that we have represented our policy in a way which allows us to calculate $\\log\\pi_{\\theta}(a_{t}|s_{t})$, and if we are able to run the policy in the environment to collect the trajectory dataset, we can compute the policy gradient and take an update step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbace7c-5164-417f-9016-9a3580fc632b",
   "metadata": {},
   "source": [
    "## Expected Grad-Log-Prob Lemma\n",
    "\n",
    "In this subsection, we will derive an intermediate result which is extensively used throughout the theory of policy gradients. We will call it the Expected Grad-Log-Prob (EGLP) lemma.\n",
    "\n",
    "**EGLP Lemma.** Suppose that $P_{\\theta}$ is a parameterized probability distribution over a random variable, $x$. Then:\n",
    "\n",
    "$$\\underset{x\\sim P_{\\theta}}{\\mathbb{E}}[\\nabla_{\\theta}\\log P_{\\theta}(x)] = 0$$\n",
    "\n",
    "**Proof.** Recall that all probability distributions are normalized:\n",
    "\n",
    "$$\\int_{x}P_{\\theta}(x) = 1$$\n",
    "\n",
    "Take the gradient of both sides of the normalization condition and use the log derivative trick to get:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "0 &= \\nabla_{\\theta}1 \\\\\n",
    "&= \\nabla_{\\theta}\\int_{x}P_{\\theta}(x) \\\\\n",
    "&= \\int_{x}\\nabla_{\\theta}P_{\\theta}(x) \\\\\n",
    "&= \\int_{x}P_{\\theta}(x)\\nabla_{\\theta}\\log P_{\\theta}(x)\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cd3547-a33d-4919-8e02-6e5bdbc305ec",
   "metadata": {},
   "source": [
    "## Don’t Let the Past Distract You\n",
    "\n",
    "Examine our most recent expression for the policy gradient:\n",
    "\n",
    "$$\\nabla_{\\theta}J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}\\left[\\sum_{t=0}^{T}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}|s_{t})R(\\tau)\\right]$$\n",
    "\n",
    "Taking a step with this gradient pushes up the log-probabilities of each action in proportion to $R(\\tau)$, the sum of all rewards ever obtained. But this doesn’t make much sense.\n",
    "\n",
    "Agents should really only reinforce actions on the basis of their consequences. Rewards obtained before taking an action have no bearing on how good that action was: only rewards that come after.\n",
    "\n",
    "It turns out that this intuition shows up in the math, and we can show that the policy gradient can also be expressed by\n",
    "\n",
    "$$\\nabla_{\\theta}J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}\\left[\\sum_{t=0}^{T}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}|s_{t})\\sum_{t'=t}^{T}R(s_{t'}, a_{t'}, s_{t'+1})\\right]$$\n",
    "\n",
    "In this form, actions are only reinforced based on rewards obtained after they are taken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eb02da-04c2-4490-bd6b-20055c60600f",
   "metadata": {},
   "source": [
    "## Baselines in Policy Gradients\n",
    "\n",
    "An immediate consequence of the EGLP lemma is that for any function $b$ which only depends on state,\n",
    "\n",
    "$$\\underset{a_{t}\\sim\\pi_{\\theta}}{\\mathbb{E}}\\left[\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}|s_{t})b(s_{t})\\right] = 0$$\n",
    "\n",
    "This allows us to add or subtract any number of terms like this from our expression for the policy gradient, without changing it in expectation:\n",
    "\n",
    "$$\\nabla_{\\theta}J(\\pi_{\\theta}) = \\mathbb{E}_{\\tau\\sim\\pi_{\\theta}}\\left[\\sum_{t=0}^{T}\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}|s_{t})\\left(\\sum_{t'=t}^{T}R(s_{t'}, a_{t'}, s_{t'+1}) - b(s_{t})\\right)\\right]$$\n",
    "\n",
    "Any function $b$ used in this way is called a **baseline.**\n",
    "\n",
    "The most common choice of baseline is the on-policy value function $V^{\\pi}(s_{t})$. Recall that this is the average return an agent gets if it starts in state $s_t$ and then acts according to policy $\\pi$ for the rest of its life.\n",
    "\n",
    "Empirically, the choice $b(s_t) = V^{\\pi}(s_t)$ has the desirable effect of reducing variance in the sample estimate for the policy gradient. This results in faster and more stable policy learning. It is also appealing from a conceptual angle: it encodes the intuition that if an agent gets what it expected, it should “feel” neutral about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b76c4ab-9605-45c0-a4c2-ea34614cfea5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
