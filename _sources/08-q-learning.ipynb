{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1faa1f6a-130c-4b6d-8ea2-8b518f75b32f",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "```{note}\n",
    "Recall that Sarsa can only estimate the action values of a given policy, and it must be combined with a policy improvement step to find optimal policies.<br>\n",
    "By contrast, Q-learning can directly estimate optimal action values and find\n",
    "optimal policies.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d68fc-9d7d-4b90-94ff-20acea5b0d89",
   "metadata": {},
   "source": [
    "## The Q-Learning algorithm\n",
    "\n",
    "**Step 1: We initialize the Q-table**\n",
    "\n",
    "![](images/Q-learning-1.jpeg)\n",
    "\n",
    "**Step 2: Choose an action using the epsilon-greedy strategy**\n",
    "\n",
    "![](images/Q-learning-2.jpeg)\n",
    "\n",
    "The epsilon-greedy strategy is a policy that handles the exploration/exploitation trade-off. The idea is that:\n",
    "\n",
    "* With probability $1-\\epsilon$ : we do exploitation (our agent selects the action with the highest state-action pair value).\n",
    "\n",
    "* With probability $\\epsilon$ : we do exploration (trying random action).\n",
    "\n",
    "At the beginning of the training, the probability of doing exploration will be huge since $\\epsilon$ is very high, so most of the time, weâ€™ll explore. But as the training goes on, and consequently our Q-table gets better and better in its estimations, we progressively reduce the epsilon value since we will need less and less exploration and more exploitation.\n",
    "\n",
    "**Step 3: Perform action $A_{t}$, get reward $R_{t+1}$ and next state $S_{t+1}$**\n",
    "\n",
    "**Step 4: Update $Q(S_t, A_t)$**\n",
    "\n",
    "Remember that in TD Learning, we update our value function after one step of the interaction.\n",
    "\n",
    "In Q-learning, to produce our TD target, we used the immediate reward $R_{t+1}$ plus the discounted value of the next state, computed by finding the action that maximizes the current Q-function at the next state.\n",
    "\n",
    "![](images/Q-learning-4.jpeg)\n",
    "\n",
    "```{tip}\n",
    "Q-learning is a stochastic approximation algorithm for solving the Bellman optimality equation expressed in terms of action values:\n",
    "\n",
    "$$Q(s, a) = \\mathbb{E}\\left[R_{t+1} + \\gamma\\max_{a}Q(S_{t+1}, a)|S_{t}=s, A_{t}=a\\right]$$\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef7fff4-f2d1-4bf3-af5a-aedf386f849c",
   "metadata": {},
   "source": [
    "## Off-policy vs On-policy\n",
    "\n",
    "```{note}\n",
    "What makes Q-learning special compared to the other TD algorithms is that\n",
    "Q-learning is off-policy while the others are on-policy.\n",
    "```\n",
    "\n",
    "Two policies exist in any reinforcement learning task: a behavior policy and a target\n",
    "policy. The behavior policy is the one used to generate experience samples. The target\n",
    "policy is the one that is constantly updated to converge to an optimal policy. When the\n",
    "behavior policy is the same as the target policy, such a learning process is called on-policy.\n",
    "Otherwise, when they are different, the learning process is called off-policy.\n",
    "\n",
    "* Sarsa is on-policy. The samples required by Sarsa in every iteration include $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$. How these samples are generated is illustrated below:\n",
    "\n",
    "    $$S_{t},A_{t}\\overset{\\text{enviroment}}{\\longrightarrow}R_{t+1},S_{t+1}\\overset{\\pi}{\\longrightarrow}A_{t+1}$$\n",
    "\n",
    "    $A_{t+1}$ is dependent on the target policy $\\pi$.\n",
    "* Q-learning is off-policy. The samples required by Q-learning in every iteration is $(S_t, A_t, R_{t+1}, S_{t+1})$. How these samples are generated is illustrated below:\n",
    "\n",
    "    $$S_{t},A_{t}\\overset{\\text{enviroment}}{\\longrightarrow}R_{t+1},S_{t+1}$$\n",
    "\n",
    "    the estimation of the optimal action value of $(S_t, A_t)$ does not involve $\\pi$ and we can use any policy to generate samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f150b26-5624-4b34-a7a4-b5674ccf6441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
