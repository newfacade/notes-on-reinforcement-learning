{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1faa1f6a-130c-4b6d-8ea2-8b518f75b32f",
   "metadata": {},
   "source": [
    "# Q-learning\n",
    "\n",
    "```{note}\n",
    "Recall that Sarsa can only estimate the action values of a given policy, and it must be combined with a policy improvement step to  nd optimal policies.<br>\n",
    "In contrast, Q-learning can directly estimate optimal action values and  nd\n",
    "optimal policies.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d68fc-9d7d-4b90-94ff-20acea5b0d89",
   "metadata": {},
   "source": [
    "## The Q-Learning algorithm\n",
    "\n",
    "The Q-learning algorithm is:\n",
    "\n",
    "$$\n",
    "q(s_{t},a_{t}) \\gets q(s_{t},a_{t}) + \\alpha\\left[(r_{t+1} + \\gamma \\max_{a}q(s_{t+1},a)) - q(s_{t},a_{t})\\right]\n",
    "$$\n",
    "\n",
    "The expression of Q-learning is similar to that of Sarsa. They are different only\n",
    "in terms of their TD targets: the TD target of Q-learning is $r_{t+1} + \\gamma \\max_{a}q(s_{t+1},a)$\n",
    "whereas that of Sarsa is $r_{t+1} + \\gamma q(s_{t+1},a_{t+1})$.\n",
    "\n",
    "Q-learning is a stochastic approximation algorithm for solving the following\n",
    "equation:\n",
    "\n",
    "$$q(s, a) = \\mathbb{E}\\left[R_{t+1} + \\gamma\\max_{a}q(S_{t+1}, a)|S_{t}=s, A_{t}=a\\right]$$\n",
    "\n",
    "This is the Bellman optimality equation expressed in terms of action values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef7fff4-f2d1-4bf3-af5a-aedf386f849c",
   "metadata": {},
   "source": [
    "## Off-policy vs On-policy\n",
    "\n",
    "```{tip}\n",
    "What makes Q-learning special compared to the other TD algorithms is that\n",
    "Q-learning is off-policy while the others are on-policy.\n",
    "```\n",
    "\n",
    "Two policies exist in any reinforcement learning task: a behavior policy and a target\n",
    "policy. The behavior policy is the one used to generate experience samples. The target\n",
    "policy is the one that is constantly updated to converge to an optimal policy. When the\n",
    "behavior policy is the same as the target policy, such a learning process is called on-policy.\n",
    "Otherwise, when they are di erent, the learning process is called off-policy.\n",
    "\n",
    "* Sarsa is on-policy. The samples required by Sarsa in every iteration include $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$. How\n",
    "these samples are generated is illustrated below:\n",
    "$$s_{t},a_{t}\\overset{\\text{model}}{\\longrightarrow}r_{t+1},s_{t+1}\\overset{\\pi}{\\longrightarrow}a_{t+1}$$\n",
    "$a_{t+1}$ is dependent on the target policy $\\pi$.\n",
    "* Q-learning is off-policy. The samples required by Q-learning in every iteration is $(s_t, a_t, r_{t+1}, s_{t+1})$.\n",
    "How these samples are generated is illustrated below:\n",
    "$$s_{t},a_{t}\\overset{\\text{model}}{\\longrightarrow}r_{t+1},s_{t+1}$$\n",
    "the estimation of the optimal action value of $(s_t, a_t)$ does\n",
    "not involve $\\pi$ and we can use any policy to generate samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f150b26-5624-4b34-a7a4-b5674ccf6441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
