{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f1b862a-401f-4ffa-a0da-1fbf1489ad42",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "```{note}\n",
    "Reinforcement learning (RL) is a machine learning approach for teaching agents how to solve tasks by trial and error. Deep RL refers to the combination of RL with deep learning.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68cdd8f-c5cf-4849-be03-5e2e2385b7c5",
   "metadata": {},
   "source": [
    "## Key Concepts and Terminology\n",
    "\n",
    "![](images/agent-env.png)\n",
    "\n",
    "The main characters of RL are the **agent** and the **environment**. The environment is the world that the agent lives in and interacts with. At every step of interaction, the agent sees a (possibly partial) observation of the state of the world, and then decides on an action to take. The environment changes when the agent acts on it, but may also change on its own.\n",
    "\n",
    "The agent also perceives a **reward** signal from the environment, a number that tells it how good or bad the current world state is. The goal of the agent is to maximize its cumulative reward, called **return**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc3bfbc-7988-4156-be9a-afdcc42ec5e4",
   "metadata": {},
   "source": [
    "### States and Observations\n",
    "\n",
    "A **state** $s$ is a complete description of the state of the world. There is no information about the world which is hidden from the state. An **observation** $o$ is a partial description of a state, which may omit information.\n",
    "\n",
    "In deep RL, we almost always represent states and observations by a real-valued vector, matrix, or higher-order tensor. For instance, a visual observation could be represented by the RGB matrix of its pixel values.\n",
    "\n",
    "When the agent is able to observe the complete state of the environment, we say that the environment is **fully observed**. When the agent can only see a partial observation, we say that the environment is **partially observed**.\n",
    "\n",
    "```{caution}\n",
    "Reinforcement learning notation sometimes puts the symbol for state, $s$, in places where it would be technically more appropriate to write the symbol for observation, $o$. Specifically, this happens when talking about how the agent decides an action: we often signal in notation that the action is conditioned on the state, when in practice, the action is conditioned on the observation because the agent does not have access to the state.<br>\n",
    "Weâ€™ll follow standard conventions for notation, but it should be clear from context which is meant.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985cc46-d250-4d12-bda0-5b933006c979",
   "metadata": {},
   "source": [
    "### Action Spaces\n",
    "\n",
    "Different environments allow different kinds of actions. The set of all valid actions in a given environment is often called the **action space**. Some environments, like Atari and Go, have **discrete action spaces**, where only a finite number of moves are available to the agent. Other environments, like where the agent controls a robot in a physical world, have **continuous action spaces**. In continuous spaces, actions are real-valued vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5553d1b-1160-4db1-aa5d-d3881735c4ee",
   "metadata": {},
   "source": [
    "## Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345a426-5efb-4b77-a8d1-a41809bb316f",
   "metadata": {},
   "source": [
    "![reinforce](images/reinforce.png)\n",
    "\n",
    "That is:\n",
    "\n",
    "$$s_{t} \\xrightarrow[]{Policy} a_{t} \\xrightarrow[]{Enviroment} r_{t+1},s_{t+1}$$\n",
    "\n",
    "Policy: $\\pi(a|s)$\n",
    "\n",
    "Enviroment controls the state transition and reward process, they have the Markov property:\n",
    "\n",
    "$$p(s_{t+1}|s_{t},a_{t},\\dots,s_{0},a_{0})=p(s_{t+1}|s_{t},a_{t})$$\n",
    "$$p(r_{t+1}|s_{t},a_{t},\\dots,s_{0},a_{0})=p(r_{t+1}|s_{t},a_{t})$$\n",
    "\n",
    "Obtain trajectory $(s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},s_{2},a_{2},\\dots)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2fbc2c-92b9-4194-86dd-5a1cb59c4080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
