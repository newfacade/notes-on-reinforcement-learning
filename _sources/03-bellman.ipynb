{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c69dcf-37bb-4b2c-8f6a-fbd86e786f1d",
   "metadata": {},
   "source": [
    "# State values and Bellman equation\n",
    "\n",
    "```{note}\n",
    "This section introduces a core concept and an important tool. The core concept\n",
    "is the state value, which is defined as the average reward that an agent can obtain if\n",
    "it follows a given policy. The greater the state value is, the better the corresponding\n",
    "policy is. While state values are important, how can we analyze them? The answer is the\n",
    "Bellman equation, which describes the relationships between the values of all states.\n",
    "```\n",
    "\n",
    "## State values\n",
    "\n",
    "Starting from $t$, we can obtain a state-action-reward trajectory:\n",
    "\n",
    "$$S_{t}\\overset{A_{t}}{\\longrightarrow}S_{t+1},R_{t+1} \\overset{A_{t+1}}{\\longrightarrow}S_{t+2},R_{t+2} \\overset{A_{t+2}}{\\longrightarrow}S_{t+3},R_{t+3}\\dots$$\n",
    "\n",
    "By definition, the discounted return along the trajectory is\n",
    "\n",
    "$$G_{t} = R_{t+1} + \\gamma R_{t+2} + \\gamma^{2}R_{t+3} + \\dots$$\n",
    "\n",
    "where $\\gamma\\in(0,1)$ is the discount rate. Note that $G_{t}$ is a random variable since $R_{t+1},R_{t+2},\\dots$\n",
    "are all random variables. Since $G_{t}$ is a random variable, we can calculate its expected value:\n",
    "\n",
    "$$v_{\\pi}(s) := \\mathbb{E}[G_{t}|S_{t}=s]$$\n",
    "\n",
    "Here, $v_{\\pi}(s)$ is called the state-value function or simply the state value of $s$. Some important\n",
    "remarks are given below.\n",
    "\n",
    "* $v_{\\pi}(s)$ depends on $s$.\n",
    "\n",
    "* $v_{\\pi}(s)$ depends on $\\pi$.\n",
    "\n",
    "* $v_{\\pi}(s)$ does not depends on $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f1dc9-e631-4132-9093-31542d6099da",
   "metadata": {},
   "source": [
    "## Bellman equation\n",
    "\n",
    "We now introduce the Bellman equation, a set of linear equations that describe the\n",
    "relationships between the values of all the states. First, note that Gt can be rewritten as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G_{t} &= R_{t+1} + \\gamma R_{t+2} + \\gamma^{2}R_{t+3} + \\dots\\\\\n",
    "&=R_{t+1} + \\gamma(R_{t+2} + \\gamma R_{t+3} + \\dots)\\\\\n",
    "&=R_{t+1} + \\gamma G_{t+1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that state value can be written as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &= \\mathbb{E}[G_{t}|S_{t}=s]\\\\\n",
    "&=\\mathbb{E}[R_{t+1} + \\gamma G_{t+1}|S_{t}=s]\\\\\n",
    "&=\\mathbb{E}[R_{t+1}|S_{t}=s] + \\gamma\\mathbb{E}[G_{t+1}|S_{t}=s]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Calculate the first term by using the law of total expectation:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[R_{t+1}|S_{t}=s] &= \\sum_{a\\in\\mathcal{A}}\\pi(a|s)r(s,a)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The second term can be calculated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[G_{t+1}|S_{t}=s] &= \\sum_{s'\\in\\mathcal{S}}\\mathbb{E}[G_{t+1}|S_{t}=s,S_{t+1}=s']p(s'|s)\\\\\n",
    "&= \\sum_{s'\\in\\mathcal{S}}\\mathbb{E}[G_{t+1}|S_{t+1}=s']p(s'|s)\\\\\n",
    "&= \\sum_{s'\\in\\mathcal{S}}v_{\\pi}(s')p(s'|s)\\\\\n",
    "&= \\sum_{s'\\in\\mathcal{S}}v_{\\pi}(s')\\sum_{a\\in\\mathcal{A}}p(s'|s,a)\\pi(a|s)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This leads to the Bellman equation:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &= \\mathbb{E}[R_{t+1} + \\gamma v_{\\pi}(S_{t+1})|S_{t}=s] \\\\\n",
    "&=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\left[r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)v_{\\pi}(s')\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Bellman equation can be written in the matrix form:\n",
    "\n",
    "$$v_{\\pi} = r_{\\pi} + \\gamma P_{\\pi}v_{\\pi}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea734b66-b1a5-4747-810c-a9fa00e8e900",
   "metadata": {},
   "source": [
    "## Action values\n",
    "\n",
    "The action value of a state-action pair $(s,a)$ is defined as\n",
    "\n",
    "$$q_{\\pi}(s,a) := \\mathbb{E}[G_{t}|S_{t}=s,A_{t}=a]$$\n",
    "\n",
    "As can be seen, the action value is defined as the expected return that can be obtained\n",
    "after taking an action at a state.\n",
    "\n",
    "The relationship between action values and state values:\n",
    "\n",
    "$$v_{\\pi}(s) = \\sum_{a\\in\\mathcal{A}}\\pi(a|s)q_{\\pi}(s, a)$$\n",
    "\n",
    "The Bellman equation that we previously introduced was defined based on state values.\n",
    "In fact, it can also be expressed in terms of action values:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{\\pi}(s,a) &= \\mathbb{E}[R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1})|S_{t}=s,A_{t}=a]\\\\\n",
    "&= r(s,a) + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)\\sum_{a'\\in\\mathcal{A}}\\pi(a'|s')q_{\\pi}(s',a')\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3446bca-7049-4bb4-86e8-48696fc6b4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
