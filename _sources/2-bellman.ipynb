{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68c69dcf-37bb-4b2c-8f6a-fbd86e786f1d",
   "metadata": {},
   "source": [
    "# State values and Bellman equation\n",
    "\n",
    "```{note}\n",
    "This section introduces a core concept and an important tool. The core concept\n",
    "is the state value, which is defined as the average reward that an agent can obtain if\n",
    "it follows a given policy. The greater the state value is, the better the corresponding\n",
    "policy is. While state values are important, how can we analyze them? The answer is the\n",
    "Bellman equation, which describes the relationships between the values of all states.\n",
    "```\n",
    "\n",
    "## State values\n",
    "\n",
    "Starting from $t$, we can obtain a state-action-reward trajectory:\n",
    "\n",
    "$$S_{t}\\overset{A_{t}}{\\longrightarrow}S_{t+1},R_{t+1} \\overset{A_{t+1}}{\\longrightarrow}S_{t+2},R_{t+2} \\overset{A_{t+2}}{\\longrightarrow}S_{t+3},R_{t+3}\\dots$$\n",
    "\n",
    "By definition, the discounted return along the trajectory is\n",
    "\n",
    "$$G_{t} = R_{t+1} + \\gamma R_{t+2} + \\gamma^{2}R_{t+3} + \\dots$$\n",
    "\n",
    "where $\\gamma\\in(0,1)$ is the discount rate. Note that $G_{t}$ is a random variable since $R_{t+1},R_{t+2},\\dots$\n",
    "are all random variables. Since Gt is a random variable, we can calculate its expected value:\n",
    "\n",
    "$$v_{\\pi}(s) := \\mathbb{E}[G_{t}|S_{t}=s]$$\n",
    "\n",
    "Here, $v_{\\pi}(s)$ is called the state-value function or simply the state value of $s$. Some important\n",
    "remarks are given below.\n",
    "\n",
    "* $v_{\\pi}(s)$ depends on $s$.\n",
    "\n",
    "* $v_{\\pi}(s)$ depends on $\\pi$.\n",
    "\n",
    "* $v_{\\pi}(s)$ does not depends on $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f1dc9-e631-4132-9093-31542d6099da",
   "metadata": {},
   "source": [
    "## Bellman equation\n",
    "\n",
    "We now introduce the Bellman equation, a set of linear equations that describe the\n",
    "relationships between the values of all the states. Note that state value can be written as\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &= \\mathbb{E}[G_{t}|S_{t}=s]\\\\\n",
    "&=\\mathbb{E}[R_{t+1} + \\gamma G_{t+1}|S_{t}=s]\\\\\n",
    "&=\\mathbb{E}[R_{t+1}|S_{t}=s] + \\gamma\\mathbb{E}[G_{t+1}|S_{t}=s]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The first term by using the law of total expectation:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[R_{t+1}|S_{t}=s] &= \\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\mathbb{E}[R_{t+1}|S_{t}=s,A_{t}=a]\\\\\n",
    "&= \\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\sum_{r\\in\\mathcal{R}}p(r|s,a)r\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The second term can be calculated as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}[G_{t+1}|S_{t}=s] &= \\sum_{s'\\in\\mathcal{S}}\\mathbb{E}[G_{t+1}|S_{t}=s,S_{t+1}=s']p(s'|s)\\\\\n",
    "&= \\sum_{s'\\in\\mathcal{S}}\\mathbb{E}[G_{t+1}|S_{t+1}=s']p(s'|s)\\\\\n",
    "&= \\sum_{s'\\in\\mathcal{S}}v_{\\pi}(s')p(s'|s)\\\\\n",
    "&= \\sum_{s'\\in\\mathcal{S}}v_{\\pi}(s')\\sum_{a\\in\\mathcal{A}}p(s'|s,a)\\pi(a|s)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This leads:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\sum_{r\\in\\mathcal{R}}p(r|s,a)r + \\gamma\\sum_{s'\\in\\mathcal{S}}v_{\\pi}(s')\\sum_{a\\in\\mathcal{A}}p(s'|s,a)\\pi(a|s)\\\\\n",
    "&=\\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\left[\\sum_{r\\in\\mathcal{R}}p(r|s,a)r + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)v_{\\pi}(s')\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Bellman equation can be written in the matrix form:\n",
    "\n",
    "$$v_{\\pi} = r_{\\pi} + \\gamma P_{\\pi}v_{\\pi}$$\n",
    "\n",
    "In terms of the action values:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{\\pi}(s, a) &= \\sum_{r\\in\\mathcal{R}}p(r|s,a)r + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)v_{\\pi}(s')\\\\\n",
    "&= \\sum_{r\\in\\mathcal{R}}p(r|s,a)r + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)\\sum_{a'\\in\\mathcal{A}}\\pi(a'|s')q_{\\pi}(s',a')\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd69c37-6206-40d6-941f-623c4bfae653",
   "metadata": {},
   "source": [
    "## Bellman optimality equation\n",
    "\n",
    "While the ultimate goal of reinforcement learning is to obtain optimal policies, it is necessary to first define what an optimal policy is. The definition is based on the state values.\n",
    "\n",
    "A policy $\\pi^{\\ast}$ is optimal if $v_{\\pi^{\\ast}}(s) \\ge v_{\\pi}(s)$ for all $s\\in\\mathcal{S}$ and for all policy $\\pi$.\n",
    "\n",
    "* Exists?\n",
    "* Unique?\n",
    "* If exists, how the obtain?\n",
    "\n",
    "Bellman optimal equation:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{\\pi}(s) &= \\underset{\\pi\\in\\Pi}{\\max} \\sum_{a\\in\\mathcal{A}}\\pi(a|s)\\left[\\sum_{r\\in\\mathcal{R}}p(r|s,a)r + \\gamma\\sum_{s'\\in\\mathcal{S}}p(s'|s,a)v_{\\pi}(s')\\right]\\\\\n",
    "&= \\underset{\\pi\\in\\Pi}{\\max} \\sum_{a\\in\\mathcal{A}}\\pi(a|s)q_{\\pi}(s, a)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "matrix form:\n",
    "\n",
    "$$v=\\underset{\\pi\\in\\Pi}{\\max}(r_{\\pi} + \\gamma P_{\\pi}v) = f(v)$$\n",
    "\n",
    "We can show that $\\left \\| f(v_{1}) - f(v_{2}) \\right \\|_{\\infty} \\le \\gamma\\left \\| v_{1} - v_{2} \\right \\|_{\\infty}$, then by using the contraction mapping theorem, we conclude that there exits one unique solution to the Bellman optimal equation.\n",
    "\n",
    "Next, we assert that the solution $v^{\\ast}$ of the Bellman optimal equation is the optimal state value, and the corresponding policy\n",
    "\n",
    "$$\\pi^{\\ast}(a|s)=\\underset{a\\in\\mathcal{A}}{\\text{argmax}}\\ q^{\\ast}(s,a)$$\n",
    "\n",
    "is an optimal policy.<br/>\n",
    "**Proof**: For any policy $\\pi$, it holds that\n",
    "\n",
    "$$v_{\\pi} = r_{\\pi} + \\gamma P_{\\pi}v_{\\pi}$$\n",
    "\n",
    "Since\n",
    "\n",
    "$$v^{\\ast} = \\underset{\\pi}{\\max}(r_{\\pi} + \\gamma P_{\\pi}v^{\\ast}) = r_{\\pi^{\\ast}} + \\gamma P_{\\pi^{\\ast}}v^{\\ast} \\ge r_{\\pi} + \\gamma P_{\\pi}v^{\\ast}$$\n",
    "\n",
    "we have\n",
    "\n",
    "$$v^{\\ast} - v_{\\pi} \\ge (r_{\\pi} + \\gamma P_{\\pi}v^{\\ast}) - (r_{\\pi} + \\gamma P_{\\pi}v_{\\pi}) = \\gamma P_{\\pi}(v^{\\ast} - v_{\\pi})$$\n",
    "\n",
    "Repeated applying the above inequality gives $v^{\\ast} - v_{\\pi} \\ge \\gamma P_{\\pi}(v^{\\ast} - v_{\\pi}) \\ge \\gamma^{2} P_{\\pi}^{2}(v^{\\ast} - v_{\\pi})\\ge \\dots$. It follows that\n",
    "\n",
    "$$v^{\\ast} - v_{\\pi} \\ge \\lim_{n\\to\\infty}\\gamma^{n} P_{\\pi}^{n}(v^{\\ast} - v_{\\pi})=0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cceaf41-670c-4bfc-af12-01dbb0723347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
