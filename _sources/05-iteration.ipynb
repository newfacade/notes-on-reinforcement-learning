{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "360306cb-0341-49b9-9fd8-9d1d63e68910",
   "metadata": {},
   "source": [
    "# Value iteration and policy iteration\n",
    "\n",
    "```{note}\n",
    "We now describe two efficient algorithms for solving finite-state MDPs, we will assume that we know the state\n",
    "transition probabilities and the reward function.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a62f9bd-493a-4356-a0f3-870a6ff85689",
   "metadata": {},
   "source": [
    "## Value iteration\n",
    "\n",
    "How to solve the Bellman optimal equation?\n",
    "\n",
    "$$v=\\underset{\\pi\\in\\Pi}{\\max}(r_{\\pi} + \\gamma P_{\\pi}v)$$\n",
    "\n",
    "Value Iteration solve this problem iteratively:\n",
    "\n",
    "$$v_{k+1}=\\underset{\\pi\\in\\Pi}{\\max}(r_{\\pi} + \\gamma P_{\\pi}v_{k})$$\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "* Initial optimal state value guess $v_{0}$\n",
    "\n",
    "* Policy update: $\\pi_{k+1} = \\underset{\\pi}{\\text{argmax}}(r_{\\pi} + \\gamma P_{\\pi}v_{k})$. Using $v_{k}(s)$ to calculate $q_{k}(s, a)$, set\n",
    "\n",
    "$$\\pi_{k+1}(s,a) = \\begin{cases}\n",
    "1, &\\text{If }a=\\underset{a\\in\\mathcal{A}}{\\text{argmax}}\\ q_{k}(s,a)\\\\\n",
    "0, &\\text{Otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "* Value update: $v_{k+1}(s) = \\underset{a\\in\\mathcal{A}}{\\max}q_{k}(s,a)$\n",
    "\n",
    "It is the process of finding the fixed point of a contraction mapping:\n",
    "\n",
    "$$v_{0}\\to f(v_{0})\\to f(f(v_{0}))\\to f(f(f(v_{0})))\\to\\dots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820940ab-5d92-4180-9d86-30dcbb5994c8",
   "metadata": {},
   "source": [
    "## Policy iteration\n",
    "\n",
    "Policy iteration is also an iterative algorithm. Each iteration has two steps:\n",
    "\n",
    "* Policy evaluation. As the name suggests, this step evaluates a given policy by calculating the corresponding state value:\n",
    "\n",
    "$$v_{\\pi_{k}} = r_{\\pi_{k}} + \\gamma P_{\\pi_{k}}v_{\\pi_{k}}$$\n",
    "\n",
    "* Policy improvement (= policy update):\n",
    "\n",
    "$$\\pi_{k+1} = \\underset{\\pi}{\\text{argmax}}(r_{\\pi} + \\gamma P_{\\pi}v_{\\pi_{k}})$$\n",
    "\n",
    "In the policy evaluation step, we can obtain the solution iteratively:\n",
    "\n",
    "$$v_{\\pi_{k}}^{(j+1)} = r_{\\pi_{k}} + \\gamma P_{\\pi_{k}}v_{\\pi_{k}}^{(j)},\\quad j=0,1,\\dots$$\n",
    "\n",
    "Policy iteration using true state value: $\\pi_{0}\\to v_{\\pi_{0}}\\to \\pi_{1}\\to v_{\\pi_{1}}\\to \\pi_{2}\\to\\dots$\n",
    "\n",
    "Natural process of improving policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e84845-244a-4a27-a55b-5b4b2a5013f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
