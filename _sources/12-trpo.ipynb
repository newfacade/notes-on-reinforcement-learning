{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ed2c497-614c-4415-8afc-051c61036b2a",
   "metadata": {},
   "source": [
    "# Trust Region Policy Optimization\n",
    "\n",
    "```{note}\n",
    "TRPO updates policies by taking the largest step possible to improve performance, while satisfying a special constraint on how close the new and old policies are allowed to be. The constraint is expressed in terms of KL-Divergence, a measure of distance between probability distributions.<br>\n",
    "This is different from normal policy gradient, which keeps new and old policies close in parameter space. But even seemingly small differences in parameter space can have very large differences in performance—so a single bad step can collapse the policy performance. This makes it dangerous to use large step sizes with vanilla policy gradients, thus hurting its sample efficiency. TRPO nicely avoids this kind of collapse, and tends to quickly and monotonically improve performance.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078d6367-4559-4c5b-a966-9af5bda3452c",
   "metadata": {},
   "source": [
    "## Quick Facts\n",
    "\n",
    "* TRPO is an on-policy algorithm.\n",
    "* TRPO can be used for environments with either discrete or continuous action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfabe844-5bbf-41d2-a227-d827bb20887c",
   "metadata": {},
   "source": [
    "## Key Equations\n",
    "\n",
    "Let $\\pi_{\\theta}$ denote a policy with parameters $\\theta$. The theoretical TRPO update is:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\theta_{k+1} = \\arg \\max_{\\theta} \\; & {\\mathcal L}(\\theta_k, \\theta) \\\\\n",
    "\\text{s.t.} \\; & \\bar{D}_{KL}(\\theta || \\theta_k) \\leq \\delta\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where ${\\mathcal L}(\\theta_k, \\theta)$ is the surrogate advantage, a measure of how policy $\\pi_{\\theta}$ performs relative to the old policy $\\pi_{\\theta_k}$ using data from the old policy:\n",
    "\n",
    "$$\n",
    "{\\mathcal L}(\\theta_k, \\theta) = \\underset{s,a \\sim \\pi_{\\theta_k}}{\\mathbb{E}}\\left[{\n",
    "    \\frac{\\pi_{\\theta}(a|s)}{\\pi_{\\theta_k}(a|s)} A^{\\pi_{\\theta_k}}(s,a)\n",
    "    }\\right],\n",
    "$$\n",
    "\n",
    "and $\\bar{D}_{KL}(\\theta || \\theta_k)$ is an average KL-divergence between policies across states visited by the old policy:\n",
    "\n",
    "$$\n",
    "\\bar{D}_{KL}(\\theta || \\theta_k) = \\underset{s \\sim \\pi_{\\theta_k}}{\\mathbb{E}}\\left[{\n",
    "    D_{KL}\\left(\\pi_{\\theta}(\\cdot|s) || \\pi_{\\theta_k} (\\cdot|s) \\right)\n",
    "}\\right].\n",
    "$$\n",
    "\n",
    "```{note}\n",
    "The objective and constraint are both zero when $\\theta = \\theta_k$. Furthermore, the gradient of the constraint with respect to $\\theta$ is zero when $\\theta = \\theta_k$.\n",
    "```\n",
    "\n",
    "The theoretical TRPO update isn’t the easiest to work with, so TRPO makes some approximations to get an answer quickly. We Taylor expand the objective and constraint to leading order around $\\theta_k$:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "{\\mathcal L}(\\theta_k, \\theta) &\\approx g^T (\\theta - \\theta_k) \\\\\n",
    "\\bar{D}_{KL}(\\theta || \\theta_k) & \\approx \\frac{1}{2} (\\theta - \\theta_k)^T H (\\theta - \\theta_k)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "resulting in an approximate optimization problem,\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\theta_{k+1} = \\arg \\max_{\\theta} \\; & g^T (\\theta - \\theta_k) \\\\\n",
    "\\text{s.t.} \\; & \\frac{1}{2} (\\theta - \\theta_k)^T H (\\theta - \\theta_k) \\leq \\delta.\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "```{note}\n",
    "By happy coincidence, the gradient g of the surrogate advantage function with respect to $\\theta$, evaluated at $\\theta = \\theta_k$, is exactly equal to the policy gradient, $\\nabla_{\\theta} J(\\pi_{\\theta})$! \n",
    "```\n",
    "\n",
    "This approximate problem can be analytically solved by the methods of Lagrangian duality, yielding the solution:\n",
    "\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k + \\sqrt{\\frac{2 \\delta}{g^T H^{-1} g}} H^{-1} g.\n",
    "$$\n",
    "\n",
    "If we were to stop here, and just use this final result, the algorithm would be exactly calculating the Natural Policy Gradient. A problem is that, due to the approximation errors introduced by the Taylor expansion, this may not satisfy the KL constraint, or actually improve the surrogate advantage. TRPO adds a modification to this update rule: a backtracking line search,\n",
    "\n",
    "$$\n",
    "\\theta_{k+1} = \\theta_k + \\alpha^j \\sqrt{\\frac{2 \\delta}{g^T H^{-1} g}} H^{-1} g,\n",
    "$$\n",
    "\n",
    "where $\\alpha \\in (0,1)$ is the backtracking coefficient, and $j$ is the smallest nonnegative integer such that $\\pi_{\\theta_{k+1}}$ satisfies the KL constraint and produces a positive surrogate advantage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e3ccbe-14cd-4320-8b99-12bea41a0100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
