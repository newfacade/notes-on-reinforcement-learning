{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d1980ba-ed0c-474f-8d89-1dab3787efd0",
   "metadata": {},
   "source": [
    "# Deep Q-learning\n",
    "\n",
    "```{note}\n",
    "The main problem with Q-Learning is that it does not scale well to large MDPs with many states and actions. The solution is to find a function $Q_{\\theta}(s,a)$ that approximates the Q-Value of any state-action\n",
    "pair $(s,a)$ using a manageable number of parameters. A\n",
    "DNN used to estimate Q-Values is called a Deep Q-Network (DQN), and using a\n",
    "DQN for Approximate Q-Learning is called Deep Q-Learning.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a5f985-a6be-42af-907d-d70eb75265f7",
   "metadata": {},
   "source": [
    "## Basic Deep Q-learning\n",
    "\n",
    "Now, how can we train a DQN? Well, consider the approximate Q-Value computed\n",
    "by the DQN for a given state-action pair $(s, a)$. Thanks to Bellman, we know we want\n",
    "this approximate Q-Value to be as close as possible to the reward $r$ that we actually\n",
    "observe after playing action $a$ in state $s$, plus the discounted value of playing optimally from then on.\n",
    "\n",
    "To estimate this sum of future discounted rewards, we can simply execute\n",
    "the DQN on the next state $s'$ and for all possible actions $a'$. We get an approximate\n",
    "future Q-Value for each possible action. We then pick the highest and discount it, and this gives us an estimate of\n",
    "the sum of future discounted rewards. By summing the reward $r$ and the future discounted\n",
    "value estimate, we get a target Q-Value $y(s, a)$ for the state-action pair $(s, a)$:\n",
    "\n",
    "$$Q_{\\text{target}}(s, a) = r + \\gamma\\max_{a'}Q_{\\theta}(s', a')$$\n",
    "\n",
    "With this target Q-Value, we can run a training step using any Gradient Descent algorithm. Specifically, we generally try to minimize the squared error between the estimated\n",
    "Q-Value $Q(s, a)$ and the target Q-Value. And that’s all for the basic Deep Q-Learning\n",
    "algorithm!\n",
    "\n",
    "The first thing we need is a Deep Q-Network. In theory, you need a neural net that\n",
    "takes a state-action pair and outputs an approximate Q-Value, but in practice it’s\n",
    "much more efficient to use a neural net that takes a state and outputs one approximate\n",
    "Q-Value for each possible action.\n",
    "\n",
    "To select an action using this DQN, we pick the action with the largest predicted QValue.\n",
    "To ensure that the agent explores the environment, we will use an $\\epsilon$-greedy\n",
    "policy.\n",
    "\n",
    "```{tip}\n",
    "Instead of training the DQN based only on the latest experiences, we will store all\n",
    "experiences in a replay buffer, and we will sample a random training\n",
    "batch from it at each training iteration. This helps reduce the correlations\n",
    "between the experiences in a training batch, which tremendously helps training.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f196d5a9-e081-4f7c-9d21-76053af262ab",
   "metadata": {},
   "source": [
    "## Fixed Q-Value Targets\n",
    "\n",
    "In the basic Deep Q-Learning algorithm, the model is used both to make predictions\n",
    "and to set its own targets. This can lead to a situation analogous to a dog chasing its\n",
    "own tail. This feedback loop can make the network unstable.\n",
    "\n",
    "To solve this problem, in their 2013 paper the DeepMind researchers\n",
    "used two DQNs instead of one: the first is the online model, which learns at each\n",
    "step and is used to move the agent around, and the other is the target model used only\n",
    "to define the targets. The target model is just a clone of the online model. \n",
    "\n",
    "Since the target model is updated much less often than the online model, the Q-Value\n",
    "targets are more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2662ef6d-8f45-488e-9883-8a17e9e6475a",
   "metadata": {},
   "source": [
    "## Double DQN\n",
    "\n",
    "In a 2015 paper, DeepMind researchers tweaked their DQN algorithm, increasing\n",
    "its performance and somewhat stabilizing training. They called this variant Double\n",
    "DQN. The update was based on the observation that the target network is prone to\n",
    "overestimating Q-Values. \n",
    "\n",
    "To fix this, they proposed\n",
    "using the online model instead of the target model when selecting the best\n",
    "actions for the next states, and using the target model only to estimate the Q-Values\n",
    "for these best actions.\n",
    "\n",
    "paper: https://arxiv.org/pdf/1509.06461.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb68dd7-e40e-4c27-bbc8-cb1df4d4e00f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
