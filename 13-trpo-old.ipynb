{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e8845d3-3110-4a75-9b41-57a8b4a3438e",
   "metadata": {},
   "source": [
    "# TRPO\n",
    "\n",
    "```{note}\n",
    "Policy Gradient methods (PG) are popular in reinforcement learning (RL). The basic principle uses gradient ascent to follow policies with the steepest increase in rewards. However, the first-order optimizer is not very accurate for curved areas. We can get overconfidence and make bad moves that ruin the progress of the training. TRPO is one of the most quoted papers in addressing this issue.\n",
    "```\n",
    "\n",
    "To understand TRPO, it will be better off to discuss three key concepts first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96d00d8-1cf5-456a-b472-f627eb84ab93",
   "metadata": {},
   "source": [
    "## Minorize-Maximization algorithm\n",
    "\n",
    "Can we guarantee that any policy updates always improve the expected rewards? The MM algorithm achieves this iteratively by maximizing a lower bound function (the blue line below) approximating the expected reward locally.\n",
    "\n",
    "![](images/trpo1.webp)\n",
    "\n",
    "We start with an initial policy guess. We find a lower bound $M$ that approximate the expected reward $\\eta$ locally at the current guess. We locate the optimal point for $M$ and use it as the next guess. We approximate a lower bound again and repeat the iteration. Eventually, our guess will converge to the optimal policy. To make this work, $M$ should be easier to optimize than $\\eta$. As a preview, $M$ is a quadratic equation\n",
    "\n",
    "$$ax^{2} + bx + c$$\n",
    "\n",
    "but in the vector form:\n",
    "\n",
    "$$g\\cdot(\\theta - \\theta_{\\text{old}}) - \\frac{\\beta}{2}(\\theta - \\theta_{\\text{old}})^{T}F(\\theta - \\theta_{\\text{old}})$$\n",
    "\n",
    "It is a convex function and well studied on how to optimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f90bf7-11f6-45f1-8962-8357618bff7f",
   "metadata": {},
   "source": [
    "## Trust region\n",
    "\n",
    "There are two major optimization methods: line search and trust region. Gradient descent is a line search. We determine the descending direction first and then take a step towards that direction.\n",
    "\n",
    "![](images/trpo2.webp)\n",
    "\n",
    "In the trust region, we determine the maximum step size that we want to explore and then we locate the optimal point within this trust region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ebe9b4-1540-4381-8592-f17de78e0194",
   "metadata": {},
   "source": [
    "## Importance sampling\n",
    "\n",
    "Importance sampling calculates the expected value of $f(x)$ where $x$ has a data distribution $p$.\n",
    "\n",
    "$$\\mathbb{E}_{x\\sim p}\\left[f(x)\\right]$$\n",
    "\n",
    "In importance sampling, we sample data from $q$ instead of $p$ and use the probability ratio between $p$ and $q$ to recalibrate the result.\n",
    "\n",
    "$$\\mathbb{E}_{x\\sim q}\\left[\\frac{f(x)p(x)}{q(x)}\\right]$$\n",
    "\n",
    "Letâ€™s go into details of applying the importance sampling concept in PG:\n",
    "\n",
    "$$L^{PG}(\\theta) = \\hat{\\mathbb{E}}_{t}\\left[\\log\\pi_{\\theta}(a_{t}|s_{t})\\hat{A}_{t}\\right]$$\n",
    "\n",
    "This can be expressed as importance sampling (IS) also:\n",
    "\n",
    "$$L_{\\theta_{\\text{old}}}^{IS} = \\hat{\\mathbb{E}}_{t}\\left[\\frac{\\pi_{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{\\text{old}}}(a_{t}|s_{t})}\\hat{A}_{t}\\right]$$\n",
    "\n",
    "The derivatives for both objective functions are the same. i.e. they have the same optimal solution.\n",
    "\n",
    "$$\\nabla_{\\theta}\\log\\pi_{\\theta}(a_{t}|s_{t})|_{\\theta_{\\text{old}}} = \\frac{\\nabla\\pi_{\\theta}(a_{t}|s_{t})|_{\\theta_{\\text{old}}}}{\\pi_{\\theta_{\\text{old}}}(a_{t}|s_{t})} = \\nabla_{\\theta}\\left(\\frac{\\pi_{\\theta}(a_{t}|s_{t})}{\\pi_{\\theta_{\\text{old}}}(a_{t}|s_{t})}\\right)|_{\\theta_{\\text{old}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9aafec3-fe2c-40d2-b595-bc2799ba133c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
