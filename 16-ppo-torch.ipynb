{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40bd4847-c88a-4174-9585-e5c53980dcb7",
   "metadata": {},
   "source": [
    "# PPO with CleanRL\n",
    "\n",
    "```{note}\n",
    "Now that we studied the theory behind PPO, the best way to understand how it works is to implement it from scratch.<br>\n",
    "Implementing an architecture from scratch is the best way to understand it, and itâ€™s a good habit. We have already done it for a value-based method with Q-Learning and a Policy-based method with Reinforce.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c91af55-e629-41ce-a0cf-f75b0e940335",
   "metadata": {},
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6df39364-e864-4b2d-bc5a-96857b443a49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import gymnasium as gym\n",
    "\n",
    "def make_env(env_id):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        return env\n",
    "    \n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d285c532-937d-4ed6-95bd-08ba031c6ae6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncVectorEnv(3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "num_envs = 3\n",
    "seed = 1\n",
    "\n",
    "envs = gym.vector.SyncVectorEnv([make_env(env_id) for _ in range(num_envs)])\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "envs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57c2a64-6fd7-44ae-a438-def1a95ecabd",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "956c3d19-2495-4b7f-937f-781fea7cf8dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super().__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "        )\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.array(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01),\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dc7f87f-c5c9-46c3-9e02-2bb03bee55f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "agent = Agent(envs).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39b10fef-2df4-46f4-96c6-9003ed0c48f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 2.5e-4\n",
    "optimizer = optim.Adam(agent.parameters(), lr=learning_rate, eps=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3bece4-1127-4992-95a8-a4e260ad95ff",
   "metadata": {},
   "source": [
    "## Start the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9c73e57-97f2-48ca-97ec-9c62caeac693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the number of steps to run in each environment per policy rollout\n",
    "num_steps = 128\n",
    "\n",
    "# ALGO Logic: Storage setup\n",
    "obs = torch.zeros((num_steps, num_envs) + envs.single_observation_space.shape).to(device)\n",
    "actions = torch.zeros((num_steps, num_envs) + envs.single_action_space.shape).to(device)\n",
    "logprobs = torch.zeros((num_steps, num_envs)).to(device)\n",
    "rewards = torch.zeros((num_steps, num_envs)).to(device)\n",
    "dones = torch.zeros((num_steps, num_envs)).to(device)\n",
    "values = torch.zeros((num_steps, num_envs)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81723ec7-8317-431e-816f-74143ba66369",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# start the game\n",
    "global_step = 0\n",
    "start_time = time.time()\n",
    "next_obs, _ = envs.reset(seed=seed) # shape (num_envs, single_observation_space.shape)\n",
    "next_obs = torch.Tensor(next_obs).to(device)\n",
    "next_done = torch.zeros(num_envs).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae686aa-2379-406a-b09d-9a1f1035bd3c",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37587f13-7c07-4825-a95f-fc086f782af2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_timesteps = 5000\n",
    "num_minibatches = 4\n",
    "batch_size = int(num_envs * num_steps)\n",
    "minibatch_size = int(batch_size // num_minibatches)\n",
    "num_iterations = total_timesteps // batch_size\n",
    "\n",
    "gamma = 0.99\n",
    "gae_lambda = 0.95  # Use GAE for advantage computation\n",
    "global_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d59bd3d-ed17-4e98-8758-7aa7d58b59d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 15.195906639099121\n",
      "1 1 14.586302757263184\n",
      "1 2 15.095952987670898\n",
      "1 3 16.335002899169922\n",
      "2 0 10.235828399658203\n",
      "2 1 8.145291328430176\n",
      "2 2 9.919469833374023\n",
      "2 3 8.310291290283203\n",
      "3 0 14.384485244750977\n",
      "3 1 9.557671546936035\n",
      "3 2 11.550650596618652\n",
      "3 3 13.196294784545898\n",
      "4 0 15.475090980529785\n",
      "4 1 11.329744338989258\n",
      "4 2 11.575265884399414\n",
      "4 3 12.216898918151855\n",
      "5 0 17.111989974975586\n",
      "5 1 20.04167366027832\n",
      "5 2 18.0026912689209\n",
      "5 3 17.139039993286133\n",
      "6 0 17.672100067138672\n",
      "6 1 17.039413452148438\n",
      "6 2 20.281112670898438\n",
      "6 3 17.588523864746094\n",
      "7 0 13.127963066101074\n",
      "7 1 13.810617446899414\n",
      "7 2 11.982034683227539\n",
      "7 3 14.697120666503906\n",
      "8 0 19.245304107666016\n",
      "8 1 18.96194839477539\n",
      "8 2 18.185470581054688\n",
      "8 3 17.695661544799805\n",
      "9 0 22.96338653564453\n",
      "9 1 23.906734466552734\n",
      "9 2 21.928604125976562\n",
      "9 3 22.954378128051758\n",
      "10 0 16.186859130859375\n",
      "10 1 17.45125961303711\n",
      "10 2 19.65501594543457\n",
      "10 3 15.642524719238281\n",
      "11 0 16.93822479248047\n",
      "11 1 15.163423538208008\n",
      "11 2 13.592549324035645\n",
      "11 3 13.926717758178711\n",
      "12 0 21.17538833618164\n",
      "12 1 21.22687530517578\n",
      "12 2 18.34033966064453\n",
      "12 3 18.781917572021484\n",
      "13 0 20.065887451171875\n",
      "13 1 17.755939483642578\n",
      "13 2 17.562976837158203\n",
      "13 3 16.334026336669922\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(1, num_iterations + 1):\n",
    "    \n",
    "    # fill obs, actions, logprobs, rewards, dones, values\n",
    "    for step in range(num_steps):\n",
    "        global_step += num_envs\n",
    "        # s_t\n",
    "        obs[step] = next_obs\n",
    "        dones[step] = next_done\n",
    "        \n",
    "        # a_t, v_t\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "            values[step] = value.flatten()\n",
    "        actions[step] = action\n",
    "        logprobs[step] = logprob\n",
    "\n",
    "        # s_{t+1}, r_t\n",
    "        next_obs, reward, terminations, truncations, infos = envs.step(action.cpu().numpy())\n",
    "        next_done = np.logical_or(terminations, truncations)\n",
    "        rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n",
    "        \n",
    "        \n",
    "    # advantage computation\n",
    "    with torch.no_grad():\n",
    "        # v_{T+1}\n",
    "        next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "        advantages = torch.zeros_like(rewards).to(device)\n",
    "        lastgaelam = 0\n",
    "        for t in reversed(range(num_steps)):\n",
    "            if t == num_steps - 1:\n",
    "                nextnonterminal = 1.0 - next_done\n",
    "                nextvalues = next_value\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - dones[t + 1]\n",
    "                nextvalues = values[t + 1]\n",
    "            # r_t + gamma * v_{t+1} - v{t}\n",
    "            delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
    "            # update lastgaelam & compute advantages\n",
    "            advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "        # Q values\n",
    "        returns = advantages + values\n",
    "\n",
    "        \n",
    "    # flatten the batch\n",
    "    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "    b_logprobs = logprobs.reshape(-1)\n",
    "    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "    b_advantages = advantages.reshape(-1)\n",
    "    b_returns = returns.reshape(-1)\n",
    "    b_values = values.reshape(-1)\n",
    "    \n",
    "    # Optimizing the policy and value network\n",
    "    update_epochs = 4\n",
    "    clip_coef = 0.2\n",
    "    ent_coef = 0.01\n",
    "    vf_coef = 0.5\n",
    "    max_grad_norm = 0.5\n",
    "    \n",
    "    b_inds = np.arange(batch_size)\n",
    "    clipfracs = []\n",
    "    for epoch in range(update_epochs):\n",
    "        np.random.shuffle(b_inds)\n",
    "        for start in range(0, batch_size, minibatch_size):\n",
    "            end = start + minibatch_size\n",
    "            mb_inds = b_inds[start:end]\n",
    "\n",
    "            _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "            logratio = newlogprob - b_logprobs[mb_inds]\n",
    "            ratio = logratio.exp()\n",
    "\n",
    "            mb_advantages = b_advantages[mb_inds]\n",
    "\n",
    "            # Policy loss\n",
    "            pg_loss1 = -mb_advantages * ratio\n",
    "            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "            # Value loss\n",
    "            v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "            entropy_loss = entropy.mean()\n",
    "            loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if start == 0:\n",
    "                print(iteration, epoch, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc468d3-e9ba-4ab1-8593-ec1c67ac2a4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
