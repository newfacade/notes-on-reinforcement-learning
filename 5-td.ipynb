{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb50ec0e-c775-4ec5-be18-8ab3586e2539",
   "metadata": {},
   "source": [
    "# Temporal-Difference methods\n",
    "\n",
    "An incremental model-free method.\n",
    "\n",
    "TD target = current policy + move one step further.\n",
    "\n",
    "## TD learning of state values\n",
    "\n",
    "Our goal is to estimate $v_{\\pi}(s)$ for all $s\\in\\mathcal{S}$. Suppose we have some experience samples $(s_{0},r_{1},s_{1},\\dots,s_{t},r_{t+1},s_{t+1},\\dots)$ generated following $\\pi$. The following TD algorithm can estimate state values using these samples:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{t+1}(s_{t}) &= v_{t}(s_{t}) - \\alpha_{t}(s_{t})\\left[v_{t}(s_{t}) - (r_{t+1} + \\gamma v_{t}(s_{t+1}))\\right]\\\\\n",
    "v_{t+1}(s) &= v_{t}(s),\\quad\\text{for all }s\\ne s_{t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "At time $t$, only the value of the visited state $s_{t}$ is updated. This algorithm can be viewd as a special stochastic approximation algorithm for solving the Bellman equation:\n",
    "\n",
    "$$v_\\pi(s) = \\mathbb{E}\\left[R + \\gamma v_{\\pi}(S)|s\\right]$$\n",
    "\n",
    "Define $g_{s_{t}}(w) = w - \\mathbb{E}[R + \\gamma v_{\\pi}(S)|s_{t}]$, $r_{t+1}$ and $s_{t+1}$ are samples of $R$ and $S$, so:\n",
    "\n",
    "$$\\tilde{g}_{s_{t}}(w,\\eta) = w - (r_{t+1} + \\gamma v_{\\pi}(s_{t+1}))$$\n",
    "\n",
    "The only difference between TD algorithm and the Robbins-Monro algorithm is that we use $v_{t}(s_{t+1})$ to replace $v_{\\pi}(s_{t+1})$.\n",
    "\n",
    "TD target move one step forward:\n",
    "\n",
    "$$r_{t+1} + \\gamma v_{t}(s_{t+1})$$\n",
    "\n",
    "TD error:\n",
    "\n",
    "$$v_{t}(s_{t}) - (r_{t+1} + \\gamma v_{t}(s_{t+1}))$$\n",
    "\n",
    "TD learning is online, it can update the state/action values immediately after receiving an experience sample, while MC learning is offline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e709b5-f674-4ba6-b26a-e67345651815",
   "metadata": {},
   "source": [
    "## TD learning of action values: Sarsa\n",
    "\n",
    "Suppose we have some experience samples generated following $\\pi$: $s_{0},a_{0},r_{1},s_{1},a_{1},\\dots,s_{t},a_{t},r_{t+1},s_{t+1},a_{t+1},\\dots$, We can use the following Sarsa algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{t+1}(s_{t},a_{t}) &= q_{t}(s_{t},a_{t}) - \\alpha_{t}(s_{t},a_{t})\\left[q_{t}(s_{t},a_{t}) - (r_{t+1} + \\gamma q_{t}(s_{t+1},a_{t+1}))\\right]\\\\\n",
    "q_{t+1}(s,a) &= q_{t}(s,a),\\quad\\text{for all }(s,a)\\ne (s_{t},a_{t})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Why is this algorithm called \"Sarsa\"? This is because each iteration requires $(s_{t},a_{t},r_{t+1},s_{t+1},a_{t+1})$\n",
    "\n",
    "Sarsa is a stochastic approximation algorithm for solving the Bellman equation:\n",
    "\n",
    "$$q_{\\pi}(s, a) = \\mathbb{E}\\left[R_{t+1} + \\gamma q_{\\pi}(S_{t+1},A_{t+1})|S_{t}=s,A_{t}=a\\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bd4e31-4013-40a8-9a2b-5c8c8ac36861",
   "metadata": {},
   "source": [
    "## n-step Sarsa\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Sarsa}\\leftarrow G_{t}^{(1)} &= R_{t+1} + \\gamma q_{\\pi}(S_{t+1}, A_{t+1})\\\\\n",
    "G_{t}^{(2)} &= R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} q_{\\pi}(S_{t+2}, A_{t+2})\\\\\n",
    "&\\vdots\\\\\n",
    "\\text{n-step Sarsa}\\leftarrow G_{t}^{(n)} &= R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{n} q_{\\pi}(S_{t+n}, A_{t+n})\\\\\n",
    "&\\vdots\\\\\n",
    "\\text{MC}\\leftarrow G_{t}^{(\\infty)} &= R_{t+1} + \\gamma R_{t+2} + \\gamma^{2} R_{t+3} \\dots\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Sarsa has fewer random vairiables, it has small variance but big bias; MC learning has big variance but small bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a599c65-da20-47fd-99d2-ce7f4de5605a",
   "metadata": {},
   "source": [
    "## TD learning of optimal action values: Q-learning\n",
    "\n",
    "The Q-learning algorithm is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{t+1}(s_{t},a_{t}) &= q_{t}(s_{t},a_{t}) - \\alpha_{t}(s_{t},a_{t})\\left[q_{t}(s_{t},a_{t}) - (r_{t+1} + \\gamma \\underset{a\\in\\mathcal{A}}{\\max}q_{t}(s_{t+1},a))\\right]\\\\\n",
    "q_{t+1}(s,a) &= q_{t}(s,a),\\quad\\text{for all }(s,a)\\ne (s_{t},a_{t})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Q-learning is a stochastic approximation algorithm for solving the Bellman optimality equation:\n",
    "\n",
    "$$q(s,a) = \\mathbb{E}\\left[R_{t+1} + \\gamma\\underset{a}{\\max}q(S_{t+1},a)|S_{t}=s,A_{t}=a\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b2dfaa-1f76-4c3a-99ed-4e89121865c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
